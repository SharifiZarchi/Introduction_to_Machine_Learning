{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a6f48c7",
   "metadata": {},
   "source": [
    "<div><font face=\"Times New Roman\" size=7><br><br>\n",
    "<center>\n",
    "Spectral Clustering\n",
    "<center><br></div>\n",
    " \n",
    "\n",
    "### Machine Learning for Bioinformatics: Homework 1 (Practical)\n",
    "*Refer to (preferably)Quera or Alireza Gargoori for any questions you have or other inconveniences.*  <br>\n",
    "*Telegram ID: @alregamo*  <br>\n",
    "*Email: alireza.agm@gmail.com* <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119e75a1",
   "metadata": {},
   "source": [
    "<font face=\"Arial\" size=4><br>\n",
    "    \n",
    "One type of common methods for clustering are the <b>Spectral Clustering</b> methods, which we would learn in this execise.\n",
    "<br><br>\n",
    "Spectral Clustering is a type of clustering algorithm in machine learning that <b>uses eigenvectors of a similarity matrix</b> to divide a set of data points into clusters. The basic idea behind spectral clustering is to use the eigenvectors of the Laplacian matrix of a graph to represent the data points and find clusters by applying k-means or another clustering algorithm to the eigenvectors. <br><br>\n",
    "    \n",
    "<div> <br>\n",
    "<center>\n",
    "<img src=\"https://images.squarespace-cdn.com/content/v1/5d782753c70af105c29a9b14/1608653466446-YC3DJUQR7FDU35XM90AE/shutterstock_1410280415.jpg?format=1000w\" width=\"700\">\n",
    "</center>\n",
    "</div><br>\n",
    "      \n",
    "Consider the matrix <b>$W$</b> representing the similarity of data points, where $(i,j)$ entry is non-zero if samples $(x^{(i)}, x^{(j)})$ are in the mutual KNN (k-nearest neighbors) of each other.<br><br>\n",
    "    $$W_{ij} = \\begin{cases}\n",
    "    1, & x^{(i)} \\in KNN \\{x^{(j)}\\} \\hspace{3mm} AND \\hspace{3mm} x^{(j)} \\in KNN \\{x^{(i)}\\} \\\\\n",
    "    0, & otherwise\n",
    "    \\end{cases}\n",
    "    $$ <br><br>\n",
    "    Another method to define <b>$W$</b> is to use a fully connected graph, where the value of its entries are obtained from a function which measures the similarity of these samples.<br><br>\n",
    "    $$W_{ij} =  k(x^{(i)}, x^{(j)})$$ <br><br>\n",
    "    where $k(x^{(i)}, x^{(j)})$ is a function to measure the similarity of two samples. An example of this function could be the <b>Radial Basis Function (RBF)</b>, defined as:\n",
    "    $$k(x^{(i)}, x^{(j)}) = exp(-\\gamma \\lVert x^{(i)} - x^{(j)} \\rVert)$$\n",
    "   <br> We call matrix <b>$W$</b> the adjacency matrix. (There are other types of definitions for this matrix, like the $\\epsilon$-neighborhood graph, simple KNN graph, etc.) <br><br>\n",
    "    The degree of each vertex in the graph is defined as $g_i = \\sum_{j}^{} w_{ij}$.\n",
    "    Also, consider the diagonal matrix <b>$G$</b>, defined as: <br><br>\n",
    "    $$G_{ij} = \\begin{cases}\n",
    "    g_i, & i=j \\\\\n",
    "    0, & otherwise\n",
    "    \\end{cases}\n",
    "    $$ <br><br>\n",
    "    Now we are ready to define the <b>Laplacian</b> matrix <b>$L$</b>:\n",
    "    $$L = G - W $$<br><br><br>\n",
    "    It can be shown the eigenvectors of <b>$L$</b> corresponding to the ùëö smallest eigenvalues are appropriate for clustering. In other words, we first compute the ùëö smallest eigenvalues $\\lambda_i$ and their corresponding eigenvectors $\\phi_i$. Let $\\Phi \\in \\mathbb{R} ^{p\\times m}$ be a matrix consisting of $\\{\\phi_i\\}_{i=1}^{m}$, i.e. the first(smallest)  eigenvectors of <b>$L$</b>:<br><br>\n",
    "    $$\\Phi(x^{(i)}) = [\\phi_1(x^{(i)}), \\phi_2(x^{(i)}), ..., \\phi_m(x^{(i)})]^T \\in \\mathbb{R}^m$$ <br>\n",
    "    In other words, we transform the original data $x^{(i)}$ from $\\mathbb{R}^{p}$ to $\\mathbb{R}^{m}$ through the first m eigenvectors of <b>$L$</b>: it is a nonlinear transformation. The $\\it{i}$th row of $\\Phi$ represents the $\\it{i}$th data point in the new feature space. This step is also called Laplacian eigenmap, which is the key step in spectral clustering.\n",
    "    <br><br> Now, in the final step, we need to apply K-means clustering to the rows of $\\Phi$ to group the data into m clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c693f8",
   "metadata": {},
   "source": [
    "<font face=\"Arial\" size=4>\n",
    "<br>In summary, the procedure consists of 4 steps: <br><br>\n",
    "    <b>1.</b> Constuct the adjacency matrix <b>$W$</b>. <br> <br>\n",
    "    <b>2.</b> Find the corresponding laplacian matrix <b>$L$</b>. <br><br>\n",
    "    <b>3.</b> Find the m smallest eigenvalues and their corresponding eigenvectors $\\{\\phi_i\\}_{i=1}^{m}$. Transform the samples in the original feature space into the new one, using the matrix $\\Phi$. <br><br>\n",
    "    <b>4.</b> Apply K-Means in this new feature space. <br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f54e69",
   "metadata": {},
   "source": [
    "## Part (A) (Bonus)\n",
    "<font face=\"Arial\" size=4>\n",
    "<br>Prove that Laplacian Matrix is positive semi-definite. (Therefore, all the eigenvalues of <b>$L$</b> are $\\geq 0$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74a0d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68581469",
   "metadata": {},
   "source": [
    "## Part (B)\n",
    "<font face=\"Arial\" size=4> <br>\n",
    "We stated that the transformation must be done using the eigenvectos corresponding to the smallest eigenvalues of laplacian matrix. This can be proved mathematically; however, we want to illustrate it with a simple example. <br>\n",
    "\n",
    "It can be shown that as the eigenvalues of the laplacian matrix gets closer to zero, the graph is more disconnected! <br>\n",
    "Explain the above statement in the following gif. In other words, explain the effect of adding edges to the graph on the eigenvalues of the laplacian matrix:\n",
    "<div> <br>\n",
    "<center>\n",
    "<img src=\"spectral.gif\" width=\"700\">\n",
    "</center>\n",
    "</div><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90303dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370984ae",
   "metadata": {},
   "source": [
    "<font face=\"Arial\" size=4>\n",
    "Why do you think the second eigenvalue is close to zero in the following graph?\n",
    "<div> <br>\n",
    "<center>\n",
    "<img src=\"spectral_final.png\" width=\"700\">\n",
    "</center>\n",
    "</div><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db3a5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5aa127",
   "metadata": {},
   "source": [
    "## Part (C)\n",
    "<font face=\"Arial\" size=4> <br>\n",
    "Here we want to compare the results of spectral clustering to the typical K-Means clustering. Consider the following dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec14c46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_circles\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "X, labels = make_circles(500, factor=0.2, noise=0.1, random_state=1)\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "scatter = plt.scatter(X[:, 0], X[:, 1], marker='o', c=labels, s=100, cmap='coolwarm', alpha=0.75)\n",
    "plt.legend(handles=scatter.legend_elements()[0], labels=['Class 1','Class 0'], fontsize='x-large', markerscale=1.5)\n",
    "plt.xlabel(\"$x_1$\")\n",
    "plt.ylabel(\"$x_2$\")\n",
    "plt.title(\"Scatter Plot of the Dataset\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fdfa7af",
   "metadata": {},
   "source": [
    "<font face=\"Arial\" size=4>\n",
    "Assume that the true labels are unknown, and we need to cluster the samples into two clusters. Apply K-Means algorithm with $K=2$ on the samples and plot the resulting clusters. <br> What do you think about the clusters from K-Means? Do the results match with the true labels?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea26c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "## Your Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e459f1",
   "metadata": {},
   "source": [
    "<font face=\"Arial\" size=4> <br>\n",
    "Now apply spectral clustering method on this dataset. You can use <code>sklearn.cluster.SpectralClustering</code> in this part. Also, use the KNN adjacency matrix for <b>$W$</b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3134a076",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import SpectralClustering\n",
    "\n",
    "## Your Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74d5245",
   "metadata": {},
   "source": [
    "<font face=\"Arial\" size=4> <br>\n",
    "Change the adjacency matrix as the fully connected graph with rbf as the similarity measurement function. (Note that you can set this through the <code>affinity</code> parameter of the spectral clustering model.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e5ff57",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af1e124",
   "metadata": {},
   "source": [
    "<font face=\"Arial\" size=4> <br>\n",
    "\n",
    "Compare the results of K-Means model vs. Spectral Clustering methods. Also, mention the effect of <code>rbf</code> adjacency matrix on the result of spectral clustering model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b38c893",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "356d4166",
   "metadata": {},
   "source": [
    "# Part (D)\n",
    "\n",
    "<font face=\"Arial\" size=4><br>\n",
    "Explain the pros and cons of spectral clustering methods. (Feel free to search more about its advantages and disadvantages through internet, but mention your sources.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e065db0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25355292",
   "metadata": {},
   "source": [
    "# Part (E): Clustering on Cancer Gene Expression RNA-seq dataset\n",
    "\n",
    "### Part (E.1): Clustering\n",
    "<font face=\"Arial\" size=4><br>\n",
    "This dataset is from the UCI Machine Learning repository. This collection of data is part of the RNA-Seq (HiSeq) PANCAN data set. It is a random extraction of gene expressions of patients having different types of tumor: \n",
    "    \n",
    "BRCA (breast invasive carcinoma) <br>\n",
    "KIRC (kidney renal clear cell carcinoma) <br>\n",
    "COAD (colon adenocarcinoma) <br>\n",
    "LUAD (lung adenocarcinoma) <br>\n",
    "PRAD (prostate adenocarcinoma) <br> <br>\n",
    "    \n",
    "    \n",
    "There are 801 instances with 20531 attributes, which are the gene expressions among different patients with each of this tumors. The data can be downloaded here: https://archive.ics.uci.edu/ml/datasets/gene+expression+cancer+RNA-Seq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52fc70d4",
   "metadata": {},
   "source": [
    "<font face=\"Arial\" size=4>\n",
    "Assume that the true labels are unknown and we cannot use classification methods to distinguish cancer types. Use Spectral Clustering method and try to cluster the data as best as you can. You are allowed to use <code>sklearn.cluster.SpectralClustering</code> for this purpose. The choice of hyperparameters are on your own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c784d936",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db10850b",
   "metadata": {},
   "source": [
    "### Part (E.2): Evaluation of Clustering Algorithm\n",
    "\n",
    "<font face=\"Arial\" size=4><br>\n",
    "Search about the metrics that we can use for clustering purposes, given that we have the true labels of the data and we want to assess the performance of our clustering method. Give a short explanation about the <code>adjusted rand index(ARI)</code> and <code>normalized mutual information(NMI)</code> metrics and evaluate your model with these metrics. You are allowed to use <code>sklearn.metrics</code> in this part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2024ca5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f988e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c623a64f",
   "metadata": {},
   "source": [
    "### Part (E.3): Visualization\n",
    "\n",
    "<font face=\"Arial\" size=4><br>\n",
    "Visualize the ground truth labels and the predictions of your model in 2 figures. You can use <code>PCA</code> for dimensionality reduction before visualization. Also, you can use <code>t-SNE</code> from <code>sklearn.manifold.TSNE</code> instead. Other methods such as <code>UMAP</code> are accepted as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ebecd89",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your Code"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

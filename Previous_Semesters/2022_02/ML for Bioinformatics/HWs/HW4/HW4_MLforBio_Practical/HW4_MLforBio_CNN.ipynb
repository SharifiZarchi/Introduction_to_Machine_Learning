{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CvjBRQx-9O6p"
   },
   "source": [
    "# **Brain Abnormality Classification**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J-k4XvYkBsZl"
   },
   "source": [
    "**Full name:**\n",
    "\n",
    "**Student Number:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fg75p2VB9gn-"
   },
   "source": [
    "In this exercise we are going to train a CNN-based model for a multi-class classification task, brain abnormality classification. The dataset we are going to use is a small imbalanced one that contains MRIs of both normal and abnormal brains.\n",
    "We will learn how to use and refine models that have been already trained on some other rich datasets to deal with situations in which we don't have access to good datasets or powerful hardware.\n",
    "This exercise has been broken into some smaller sections and each section has its score. What you are supposed to do is shown in `\"======== TODO ========\"` format. Any part which is showed by `...` should be completed. with There may be some **Questions** in the notes below. It is not necessary to answer these questions, but they will help you to do the next sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e9zZRUWW5qzb"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ShaLktTH9MlG"
   },
   "source": [
    "First of all we need to import required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6uAiPJ4k5tVY",
    "outputId": "04bdbd1c-9b08-4e6f-b3eb-57bfe442f20f"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "import imgaug\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchsummary import summary\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import shutil\n",
    "import os\n",
    "import random\n",
    "import copy\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "print(\"PyTorch Version: \",torch.__version__)\n",
    "print(\"Torchvision Version: \",torchvision.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ivQaXRlFOZ7J"
   },
   "source": [
    "# 1) Data Preparation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GVaaZcHe-6Hj"
   },
   "source": [
    "## 1.1) Download Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zJBckbi8CczD"
   },
   "source": [
    "The following pieces of code will download dataset to your colab storage and remove corrupted or useless files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1cvRWWsOp0V7"
   },
   "outputs": [],
   "source": [
    "!wget --content-disposition https://figshare.com/ndownloader/files/28399209\n",
    "!mkdir dataset\n",
    "!tar xvf NINS_Dataset.tar --directory dataset/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_wbTOgpZmtly"
   },
   "outputs": [],
   "source": [
    "# Set the directory containing the images\n",
    "data_dir = 'dataset/'\n",
    "\n",
    "classes = os.listdir(data_dir)\n",
    "shutil.rmtree(os.path.join(data_dir, 'models'))\n",
    "classes = os.listdir(data_dir)\n",
    "\n",
    "for c in classes:\n",
    "    images = os.listdir(os.path.join(data_dir, c))\n",
    "    for image in images:\n",
    "      if image[-4:] != '.jpg':\n",
    "        corrupted_img = os.path.join(data_dir, c, image)\n",
    "        try:\n",
    "          os.remove(corrupted_img)\n",
    "        except:\n",
    "          shutil.rmtree(corrupted_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qt4Q7OPh7lNc"
   },
   "source": [
    "## 1.2) Splitting Dataset: 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nM-0MIi1DCdP"
   },
   "source": [
    "The data should be spilitted into three groups: train, validation, and test. The groups' size are your choice and they should be reasonable, but it is recommended to use 0.8, 0.1, and 0.1 for train, validation, and test, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OmiNokOhYzsa",
    "outputId": "c66462e2-0f0d-443e-98a8-7a64655e570b"
   },
   "outputs": [],
   "source": [
    "# Set up directories and data splits\n",
    "root_dir = 'dataset'\n",
    "classes = os.listdir(root_dir)\n",
    "\n",
    "############## To Do ##############\n",
    "train_split = ...\n",
    "val_split = ...\n",
    "test_split = ...\n",
    "###################################\n",
    "\n",
    "# Create a dictionary with images and labels\n",
    "data_dict = {}\n",
    "for idx, class_name in enumerate(classes):\n",
    "    class_dir = os.path.join(root_dir, class_name)\n",
    "    data_dict[class_name] = [os.path.join(class_dir, img) for img in os.listdir(class_dir)]\n",
    "\n",
    "############## To Do ##############\n",
    "# Create train, validation and test splits\n",
    "# Also print the number of samples in each class\n",
    "train_images = []\n",
    "val_images = []\n",
    "test_images = []\n",
    "for cls_name, images in data_dict.items():\n",
    "    # your code ...\n",
    "###################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VIEyalb4EUL9"
   },
   "source": [
    "**Question:** How many classes are there in your dataset?\n",
    "\n",
    "**Question:** Did you notice the imbalance between classes?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8PgvLrJDFEA8"
   },
   "source": [
    "We put samples of each group in a separate folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BX_J02vfT7lP"
   },
   "outputs": [],
   "source": [
    "!mkdir train val test\n",
    "\n",
    "n_img = 0\n",
    "for image in train_images:\n",
    "  n_img += 1\n",
    "  os.makedirs(os.path.join('train', image.split('/')[1]), exist_ok=True)\n",
    "  shutil.copy(image, os.path.join('train', *image.split('/')[1:]))\n",
    "\n",
    "for image in val_images:\n",
    "  os.makedirs(os.path.join('val', image.split('/')[1]), exist_ok=True)\n",
    "  shutil.copy(image, os.path.join('val', *image.split('/')[1:]))\n",
    "\n",
    "for image in test_images:\n",
    "  os.makedirs(os.path.join('test', image.split('/')[1]), exist_ok=True)\n",
    "  shutil.copy(image, os.path.join('test', *image.split('/')[1:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mOp_0MGU8x4G"
   },
   "source": [
    "## 1.3) Calculate Weights of Calsses: 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sNfKjbYSFTnE"
   },
   "source": [
    "As you saw there exists an imbalance between classes. The dataset contains a class with more than a thousand samples and classes with fewer than 50 samples.\n",
    "\n",
    "Here we want to calculate coefficients (weights) that can compensate this gap. So you are supposed to calculate these coefficients for each class such that if you multiply a class size by its corresponding coefficient the result will be equal for all the other classes. The minimum value of a coefficient should be 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GTSu-MXlgq8f",
    "outputId": "1b0b0f09-9eaa-45f8-a625-239d00c23b83"
   },
   "outputs": [],
   "source": [
    "############## To Do ##############\n",
    "wgts = []\n",
    "\n",
    "for c in classes:\n",
    "  print(c)\n",
    "\n",
    "  # Your code ...\n",
    "\n",
    "  print('Size of Class: ' + ... )\n",
    "  print('Weight of Class: ' + ... )\n",
    "  print()\n",
    "\n",
    "wgts = torch.Tensor(wgts)\n",
    "###################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NmGZmfvC9aI8"
   },
   "source": [
    "## 1.4) Create DataLoaders: 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X8Pn_XzmH6N5"
   },
   "source": [
    "We read and put the data of each group in its corresponding list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MDg1aJ7fUMhT"
   },
   "outputs": [],
   "source": [
    "train_images = []\n",
    "train_aug_images = []\n",
    "val_images = []\n",
    "test_images = []\n",
    "\n",
    "for c in classes:\n",
    "    class_dir = os.path.join('train', c)\n",
    "    train_images += [os.path.join(class_dir, img) for img in os.listdir(class_dir)]\n",
    "\n",
    "    class_dir = os.path.join('val', c)\n",
    "    val_images += [os.path.join(class_dir, img) for img in os.listdir(class_dir)]\n",
    "\n",
    "    class_dir = os.path.join('test', c)\n",
    "    test_images += [os.path.join(class_dir, img) for img in os.listdir(class_dir)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tWyj_xTbJfJP"
   },
   "source": [
    "Define required transforms and create Dataloaders.\n",
    "\n",
    "**Question:** What is the approperiate input size of models like ResNet, VGG, etc.? Becarefull about the size of your images as well.\n",
    "\n",
    "**Notice:** Select a suitable batch size.\n",
    "\n",
    "**Notice:** Becarefull about the range of input values. They should be in [0, 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fWRmYPIBun7j"
   },
   "outputs": [],
   "source": [
    "# Define a custom dataset class\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, image_list, classes=None, transform=None):\n",
    "        self.image_list = image_list\n",
    "        self.transform = transform\n",
    "        self.label_encoder = {c: i for (i, c) in enumerate(classes)}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_list)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        image = Image.open(self.image_list[index])\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        label = self.image_list[index].split('/')[-2]\n",
    "        label = self.label_encoder[label]\n",
    "\n",
    "        return image, label\n",
    "\n",
    "\n",
    "############## To Do ##############\n",
    "\n",
    "# Define the transformations to be applied on the train and validation sets\n",
    "transforms = transforms.Compose([\n",
    "    ...\n",
    "])\n",
    "\n",
    "# Create datasets using defined CustomDataset class\n",
    "train_dataset = ...\n",
    "val_dataset = ...\n",
    "test_dataset = ...\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = ...\n",
    "val_loader = ...\n",
    "test_loader = ...\n",
    "\n",
    "###################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RKOPPzL_76CN"
   },
   "outputs": [],
   "source": [
    "data_loaders = {'train': train_loader, 'val': val_loader}\n",
    "num_cls = ... # Number of classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CNSIXOnCRGsz"
   },
   "source": [
    "## 1.2) Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 527
    },
    "id": "bnL0BmtT9qNi",
    "outputId": "e8145160-6b23-4114-ac03-5fea8cf6f1f6"
   },
   "outputs": [],
   "source": [
    "fig, axis = plt.subplots(2, 3, figsize=(12, 6))\n",
    "for i, batch in enumerate(train_loader):\n",
    "  axis[0, 0].imshow(np.array(batch[0][0]).transpose(1, 2, 0))\n",
    "  axis[0, 1].imshow(np.array(batch[0][1]).transpose(1, 2, 0))\n",
    "  axis[0, 2].imshow(np.array(batch[0][2]).transpose(1, 2, 0))\n",
    "  axis[1, 0].imshow(np.array(batch[0][-3]).transpose(1, 2, 0),)\n",
    "  axis[1, 1].imshow(np.array(batch[0][-2]).transpose(1, 2, 0),)\n",
    "  axis[1, 2].imshow(np.array(batch[0][-1]).transpose(1, 2, 0),)\n",
    "  break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RKcN2b7aOjwK"
   },
   "source": [
    "# 2) Model Initialization and Structure Modification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eu-OBQvwBCT0"
   },
   "source": [
    "## 2.1) ResNet50: 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "onT68nXn5r4-"
   },
   "source": [
    "Load ResNet50 model with its pretrained weights. (Suggestion: IMAGENET1K_V2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gItCb_fl5obd"
   },
   "outputs": [],
   "source": [
    "############## To Do ##############\n",
    "resnet_model = ...\n",
    "###################################\n",
    "\n",
    "# Print the structure of the loaded model\n",
    "print(resnet_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FpBXcrAJ58W-"
   },
   "source": [
    "**Question:** Which part of this model is mainly used for classification? What kind of layer(s) are in this part? How many outputs does this model have? Why"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wh4q_xlw6LQK"
   },
   "source": [
    "Modify the classification part of the model so that it become suitable for your classification task, i.e. the number of outputs of the model becomes equal to number of classes in your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u2J4A1d1Bb8M"
   },
   "outputs": [],
   "source": [
    "############## To Do ##############\n",
    "# Your code ...\n",
    "###################################\n",
    "\n",
    "# Print the structure of the modified model and notice the difference\n",
    "print(resnet_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z-1Wo6uhBrMh"
   },
   "outputs": [],
   "source": [
    "# Specify the device (Suggestion: GPU is by far better than CPU)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "resnet_model = resnet_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cJ5LQueHBJgf"
   },
   "source": [
    "## 2.2) VGG16: 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D4d9xF9j7VIj"
   },
   "source": [
    "Load VGG16 model with its pretrained parameters. (Suggestion: IMAGENET1K_V1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p7FAJewdBMUe"
   },
   "outputs": [],
   "source": [
    "############## To Do ##############\n",
    "vgg_model = ...\n",
    "###################################\n",
    "\n",
    "# Print the structure of the loaded model\n",
    "print(vgg_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-lJJVpp-7oaB"
   },
   "source": [
    "**Question:** Which part of this model is mainly used for classification? What kind of layer(s) are in this part? How many outputs does this model have? Why"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oybSCNeM7oaG"
   },
   "source": [
    "Modify the classification part of the model so that it become suitable for your classification task, i.e. the number of outputs of the model becomes equal to number of classes in your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vK5FqiCsCIK-"
   },
   "outputs": [],
   "source": [
    "############## To Do ##############\n",
    "# Your code ...\n",
    "###################################\n",
    "\n",
    "# Print the structure of the modified model and notice the difference\n",
    "print(vgg_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P5HRGlqjCJ1F"
   },
   "outputs": [],
   "source": [
    "# Specify the device (Suggestion: GPU is by far better than CPU)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "vgg_model = vgg_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gDVlW3xbOjyg"
   },
   "source": [
    "# 3) Train Function: 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-0iUdIFz7B4g"
   },
   "source": [
    "Define your loss function for multi class classification. Don't forget to add class weights to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pqG2ZO-tBtgg"
   },
   "outputs": [],
   "source": [
    "wgts = wgts.to(device)\n",
    "\n",
    "############## To Do ##############\n",
    "criterion = ...\n",
    "###################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NiO_BDEU8KWZ"
   },
   "source": [
    "Here you should define your main train function.\n",
    "\n",
    "**Note1:** Since you are going to change the data during the training procedure, your train function have to get dataloaders as input.\n",
    "\n",
    "**Note2:** Also you don't know how many epochs you should train.\n",
    "  So save the best weights according to validation loss at the end of each epoch,\n",
    "  and after the end of training, load the best model.\n",
    "\n",
    "**Note3:** Save loss and accuracy of train and validation in each epoch to plot them later.\n",
    "\n",
    "**Note4:** The following code is an template that can help you, but any other functions that you define with the desired properties is acceptable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VzC8G6BU81aN"
   },
   "outputs": [],
   "source": [
    "############## To Do ##############\n",
    "def train_model(model, dataloaders, criterion, optimizer, scheduler=None, num_epochs=10):\n",
    "    since = time.time()\n",
    "\n",
    "    acc_hist = { 'train': [], 'val': [] }\n",
    "    loss_hist = { 'train': [], 'val': [] }\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch}/{num_epochs - 1}')\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                ... # Set model to training mode\n",
    "            else:\n",
    "                ... # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                ... # zero the parameter gradients\n",
    "                \n",
    "                # forward\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    # Calculate outputs, predictions, and losses\n",
    "                    # Your code ...\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        # Your code ...\n",
    "\n",
    "                # statistics\n",
    "                running_loss += ... # Summation of losses\n",
    "                running_corrects += ... # Summation of true classifications\n",
    "\n",
    "            epoch_loss = ... # Average of losses. Use running_loss\n",
    "            epoch_acc = ... # Accuracy. Use running_corrects\n",
    "\n",
    "            if phase == 'val' and not(scheduler is None):\n",
    "                ... # Set scheduler\n",
    "\n",
    "            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "            acc_hist[phase].append(epoch_acc)\n",
    "            loss_hist[phase].append(epoch_loss)\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "    print(f'Best val Acc: {best_acc:4f}')\n",
    "\n",
    "    # load best model weights with the stored state dictionary\n",
    "    ...\n",
    "\n",
    "    return model, acc_hist, loss_hist\n",
    "###################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q9Uw9Lz9Oj05"
   },
   "source": [
    "# 4) Feature Extraction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YVV1G1fACWAu"
   },
   "source": [
    "In this section with just update parameters of the recently modified layer(s)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5bai9m9UE9R4"
   },
   "source": [
    "### 4.1) ResNet50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iLpb3aAwFjR9"
   },
   "source": [
    "### 4.1.1) Freezing: 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TRAFEwUNAFzY"
   },
   "source": [
    "You should freeze all the parameters of the model except those of the new layer(s) you have just added.\n",
    "\n",
    "**Help:** You can use `requires_grad` feature in Pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xh-6a4-pE8vn"
   },
   "outputs": [],
   "source": [
    "############## To Do ##############\n",
    "# Your code ...\n",
    "###################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pN1jK5dlFq6d"
   },
   "source": [
    "### 4.1.2) Training: 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aJVUmLuxAv9o"
   },
   "source": [
    "Set optimizer such that just the parameters of the new layer(s) be updateded.\n",
    "\n",
    "**Becareful:** Learning rate is a critical hyper parameter in this training task.\n",
    "\n",
    "Scheduler is optional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iPYkcp9nFRHt"
   },
   "outputs": [],
   "source": [
    "############## To Do ##############\n",
    "resnet_optimizer = ...\n",
    "###################################\n",
    "\n",
    "lr_scheduler = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0q40Hu56-Fsu"
   },
   "outputs": [],
   "source": [
    "############## To Do ##############\n",
    "resnet_fe, resnet_acc_fe, resnet_loss_fe = train_model(...)\n",
    "###################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2mVxOxh0GD8q"
   },
   "source": [
    "### 4.2) VGG16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "imZAcY1MGD80"
   },
   "source": [
    "### 4.2.1) Freezing: 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bv3Prw-FB4Jh"
   },
   "source": [
    "You should freeze all the parameters of the model except those of the new layer(s) you have just added.\n",
    "\n",
    "**Help:** You can use `requires_grad` feature in Pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zDVXMNyE9oZu"
   },
   "outputs": [],
   "source": [
    "############## To Do ##############\n",
    "# Your code ...\n",
    "###################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6kHShVqFGD80"
   },
   "source": [
    "### 4.2.2) Training: 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pe68DzhuCCdM"
   },
   "source": [
    "Set optimizer such that just the parameters of the new layer(s) be updateded.\n",
    "\n",
    "**Becareful:** Learning rate is a critical hyper parameter in this training task.\n",
    "\n",
    "Scheduler is optional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0oelQ809GZeH"
   },
   "outputs": [],
   "source": [
    "############## To Do ##############\n",
    "vgg_optimizer = ...\n",
    "###################################\n",
    "\n",
    "lr_scheduler = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rTWv5ePtGjJR"
   },
   "outputs": [],
   "source": [
    "############## To Do ##############\n",
    "vgg_fe, vgg_acc_fe, vgg_loss_fe = train_model(...)\n",
    "###################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EUSv9xitP5aZ"
   },
   "source": [
    "# 5) Fine-Tuning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TqM-ieKFClk2"
   },
   "source": [
    "In this section the whole parameters of the model are going to be updated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eIUF5dwZLMOL"
   },
   "source": [
    "## 5.1) ResNet50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KyGIUXxMJfOh"
   },
   "source": [
    "### 5.1.1) Unfreezing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CP4GMshrCs7f"
   },
   "source": [
    "Unfreeze all the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "95z3JPk--gzO"
   },
   "outputs": [],
   "source": [
    "############## To Do ##############\n",
    "# Your code ...\n",
    "###################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z7fLHO87AZyn"
   },
   "source": [
    "### 5.1.2) Train the Model: 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hHu3r0ZuC38P"
   },
   "source": [
    "Set optimizer such that all the parameters of the model be updateded.\n",
    "\n",
    "**Becareful:** Since we are fine-tuning the model, we expect slight changes in the parameters, so learning rate should be very small in this section. You can also define different learning rates for newly modified layer(s) and the rest of the model.\n",
    "\n",
    "Scheduler is optional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FV32p8vP-3tG"
   },
   "outputs": [],
   "source": [
    "############## To Do ##############\n",
    "resnet_optimizer = ...\n",
    "\n",
    "lr_scheduler = None\n",
    "\n",
    "resnet_ft, resnet_acc_ft, resnet_loss_ft = train_model(...)\n",
    "###################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ExLt-l1uLYcI"
   },
   "source": [
    "## 5.2) VGG16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hldo6QmWLYcQ"
   },
   "source": [
    "### 5.2.1) Unfreezing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cOXTOdY2DvdI"
   },
   "source": [
    "Unfreeze all the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nr6A8CnuLYcQ"
   },
   "outputs": [],
   "source": [
    "############## To Do ##############\n",
    "# Your code ...\n",
    "###################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6I_yUm42LYcR"
   },
   "source": [
    "### 5.2.2) Train the Model: 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U7tDdLC1D1GL"
   },
   "source": [
    "Set optimizer such that all the parameters of the model be updateded.\n",
    "\n",
    "**Becareful:** Since we are fine-tuning the model, we expect slight changes in the parameters, so learning rate should be very small in this section. You can also define different learning rates for newly modified layer(s) and the rest of the model.\n",
    "\n",
    "Scheduler is optional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JVjHLpOULYcR"
   },
   "outputs": [],
   "source": [
    "############## To Do ##############\n",
    "vgg_optimizer = ....\n",
    "\n",
    "lr_schedular = None\n",
    "\n",
    "vgg_ft, vgg_acc_ft, vgg_loss_ft = train_model(...)\n",
    "###################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TB8Sn5s-Obbd"
   },
   "source": [
    "## 6) Plot Learning Curves: 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "be0UuMdcEuhM"
   },
   "source": [
    "Put the results of feature extraction and fine-tuning together and plot 2 figures for each of the models: accuracy and loss against epochs, i.e. 4 figures at all. Plot validation and train in one figure. Plot a vertical line that distinguish between feature extraction and fine-tuning epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 657
    },
    "id": "VN9NuGYyI-nz",
    "outputId": "bcfe0576-20be-40f4-944a-f4f71dcb657a"
   },
   "outputs": [],
   "source": [
    "############## To Do ##############\n",
    "# Your code ...\n",
    "###################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hM4jItRYzgCr"
   },
   "source": [
    "## 7) Confusion Matrix: 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_B13VCOTGEDt"
   },
   "source": [
    "Plot confusion matrix for each of the models. It should be a `num_of_classes * num_of_classes` square."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "6Gk0DLxdObbj",
    "outputId": "8ea06a06-b813-4f0b-95e8-62e7aecc737a"
   },
   "outputs": [],
   "source": [
    "############## To Do ##############\n",
    "# Your code ...\n",
    "###################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lkujdGJnP9VS"
   },
   "source": [
    "# 9) Evaluate on Test Split: 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QDPuYInIGlt8"
   },
   "source": [
    "Calculate the accuracy of each model on test data and also plot confusion matrix for both of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "9IPkeVrn9rMJ",
    "outputId": "f1cf3be3-0316-4022-c73c-c1b2c5ca6170"
   },
   "outputs": [],
   "source": [
    "############## To Do ##############\n",
    "# Your code ...\n",
    "###################################"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

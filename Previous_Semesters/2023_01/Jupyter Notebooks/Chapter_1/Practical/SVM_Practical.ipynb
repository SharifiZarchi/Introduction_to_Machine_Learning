{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BspwgQKL3Na8"
   },
   "source": [
    "<font face=\"Times New Roman\" size=5><div dir=rtl align=center>\n",
    "<font face=\"Times New Roman\" size=5>\n",
    "In The Name of God\n",
    "</font>\n",
    "<br> <br>\n",
    "<img src=\"https://logoyar.com/content/wp-content/uploads/2021/04/sharif-university-logo.png\" alt=\"University Logo\" width=\"150\" height=\"150\">\n",
    "<br>\n",
    "<font face=\"Times New Roman\" size=4 align=center>\n",
    "Sharif University of Technology - Department of Computer\n",
    "</font>\n",
    "<br> <br>\n",
    "<font color=\"#008080\" size=5>\n",
    "Introduction to Machine Learning\n",
    "</font>\n",
    "\n",
    "<hr/> <br>\n",
    "<font color=\"#800080\" size=6>\n",
    "Chapter 3: SVM (Support Vector Machine)\n",
    "<br>\n",
    "</font>\n",
    "<br>\n",
    "<font face=\"Times New Roman\" size=4>\n",
    ":authors <br>\n",
    "<b>Alireza Gregory Motlaq - Peyman Naseri - Alireza Heydari - Mohammad Mahdi Vahedi</b>\n",
    "</font>\n",
    "<hr>\n",
    "</div></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i6NrAXONPtcR"
   },
   "source": [
    "<font  size=5><div dir=ltr>\n",
    "<font face=\"Times New Roman\" color=\"#008080\" size=5>**Table of Contents**</font>\n",
    "<br>\n",
    "\n",
    "<font size=4>\n",
    "<ul>\n",
    "    <li>\n",
    "        <a href=\"#1\">\n",
    "            1. Support Vector Machine\n",
    "        </a>\n",
    "    </li> <br>\n",
    "    <ul>\n",
    "        <li>\n",
    "            <a href=\"#1-1\">\n",
    "                1-1. Hard Margin SVM\n",
    "            </a>\n",
    "        </li>\n",
    "        <ul>\n",
    "            <li>\n",
    "                <a href=\"#1-1-1\">\n",
    "                    1-1-1. From Scratch\n",
    "                </a>\n",
    "            </li>\n",
    "            <li>\n",
    "                <a href=\"#1-1-2\">\n",
    "                    1-1-2. Using Scikit Learn Library\n",
    "                </a>\n",
    "            </li>\n",
    "        </ul> <br>\n",
    "        <li>\n",
    "            <a href=\"#1-2\">\n",
    "                1-2. Soft Margin SVM\n",
    "            </a>\n",
    "        </li>\n",
    "        <ul>\n",
    "            <li>\n",
    "                <a href=\"#1-1-1\">\n",
    "                    1-2-1. What is the Challange?\n",
    "                </a>\n",
    "            </li>\n",
    "            <li>\n",
    "                <a href=\"#1-1-2\">\n",
    "                    1-2-2. Soft Margin Classifier\n",
    "                </a>\n",
    "            </li>\n",
    "        </ul> <br>\n",
    "        <li>\n",
    "            <a href=\"#1-3\">\n",
    "                1-3. Kernel SVM\n",
    "            </a>\n",
    "        </li>\n",
    "        <ul>\n",
    "            <li>\n",
    "                <a href=\"#1-3-1\">\n",
    "                    1-3-1. What is the Challange?\n",
    "                </a>\n",
    "            </li>\n",
    "            <li>\n",
    "                <a href=\"#1-3-2\">\n",
    "                    1-3-2. Introduction to Kernel\n",
    "                </a>\n",
    "            </li>\n",
    "            <li>\n",
    "                <a href=\"#1-3-3\">\n",
    "                    1-3-3. Polynomial Kernel\n",
    "                </a>\n",
    "            </li>\n",
    "            <li>\n",
    "                <a href=\"#1-3-4\">\n",
    "                    1-3-4. Radial Basis Function\n",
    "                </a>\n",
    "            </li>\n",
    "        </ul> <br>\n",
    "        <li>\n",
    "            <a href=\"#1-4\">\n",
    "                1-4. Multi-Class SVM\n",
    "            </a>\n",
    "        </li>\n",
    "    </ul> <br>\n",
    "    <li>\n",
    "        <a href=\"#-2\">\n",
    "            2. Support Vector Regression\n",
    "        </a>\n",
    "    </li> <br>\n",
    "    <li>\n",
    "        <a href=\"#-4\">\n",
    "            References\n",
    "        </a>\n",
    "    </li>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sBzocXtIiTbu"
   },
   "source": [
    "<font face=\"Times New Roman\"><div id=\"1\">\n",
    "# <font color=\"#800080\" size=6>**1. Support Vector Machine (SVM)**</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HwKKT790WAqS"
   },
   "source": [
    "<font face=\"Times New Roman\" size=3>\n",
    "In the first section, we will implement a simplified model of SVM and then introduce the Scikit-Learn library, which is one of the most important and powerful libraries in the field of machine learning. In the following, we will implement more complex models using the modules of this library.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rYaiPL1plrDm"
   },
   "source": [
    "<font face=\"Times New Roman\"><div id=\"1-1\">\n",
    "## <font color=\"#800080\" size=6>**1-1. Hard Margin SVM** </font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3o7yaX9IywIP"
   },
   "source": [
    "<font face=\"Times New Roman\" size=3>\n",
    "Also called: Hard Margin Classifier and Maximal Margin Classifier.<br>\n",
    "In this chapter, we are looking for the best separating boundary, but what is this best boundary and how is it found? If we call the distance of the closest point to the border as margin, the best line separating two classes is the line that maximizes this distance.To find this boundy, we implement our model in two ways. First from scratch and then using the scikit-learn library. We use the same dataset in both parts.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "swwrxuSMAESj"
   },
   "source": [
    "#### <font color=\"#008080\" face=\"Times New Roman\" size=4>**Dataset**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ju9aCx5yAESj"
   },
   "source": [
    "##### <font color=\"#008080\" face=\"Times New Roman\" size=4> - Introduction to Dataset </font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gvnnlce9zBQJ"
   },
   "source": [
    "<font face=\"Times New Roman\" size=3>\n",
    "For simplicity and presentation, we generate an artifical linearly-separable dataset with 2 features and 250 samples using make_blobs function from scikit-learn.datasets.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gYuiA7fFAESk"
   },
   "source": [
    "##### <font color=\"#008080\" face=\"Times New Roman\" size=4> - Loading Dataset </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FybchF26AESk"
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "N = 250\n",
    "p = 2\n",
    "X, y = datasets.make_blobs(\n",
    "    n_samples=N, n_features=p, centers=2, cluster_std=1.05, random_state=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "helJaWTOAESk"
   },
   "source": [
    "##### <font color=\"#008080\" face=\"Times New Roman\" size=4> - Visualizing Dataset </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 565
    },
    "id": "RzkrUZYuAESk",
    "outputId": "9254a3e4-d70b-452b-bfd1-c690238a1f4e"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "scatter = plt.scatter(X[:, 0], X[:, 1], marker='o', cmap='coolwarm', c=y, s=100, alpha=0.75);\n",
    "plt.xlabel(\"$x_1$\")\n",
    "plt.ylabel(\"$x_2$\")\n",
    "plt.title(\"Scatter Plot of the Dataset\")\n",
    "plt.legend(handles=scatter.legend_elements()[0], labels=['Class 1','Class 0'], fontsize='x-large', markerscale=1.5)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R3FtkfQ4RsL2"
   },
   "source": [
    "<font face=\"Times New Roman\" size=3>\n",
    "According to the results, our dataset is fully linearly separable; So we expect the accuracy of our algorithms on these data to be 100%.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YhLhSsSfYKbR"
   },
   "source": [
    "##### <font color=\"#008080\" face=\"Times New Roman\" size=4> - Some Exmaple Of Boundaries With Thier Margins </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HXyIzaJbzWW2"
   },
   "source": [
    "<font face=\"Times New Roman\" size=3>\n",
    "We plot some examples of boundaries with their margins.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 368
    },
    "id": "A44qICfUZlRk",
    "outputId": "7c7a832e-adc6-4a0f-ad1a-b0118ca9c7c9"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "plt.figure(figsize=(16, 4))\n",
    "\n",
    "# draw separators and margins\n",
    "x_values = np.linspace(-14, 2)\n",
    "for i, (w, b, m) in enumerate([(-1, -3, 2.2), (-0.75, -4, 4), (-0.5, -5, 1.2)]):\n",
    "    plt.subplot(1, 3, i + 1)\n",
    "\n",
    "    # draw data\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, s=50, edgecolors='black', cmap='coolwarm', alpha=0.6)\n",
    "    y_values = w * x_values + b\n",
    "\n",
    "    # draw separator and margin\n",
    "    plt.plot(x_values, y_values, '-k')\n",
    "    plt.fill_between(x_values, y_values - m, y_values + m, edgecolor='none', color='#7D3C4A', alpha=0.3)\n",
    "    plt.xlim(-14, 2)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0BrjWqO-AESk"
   },
   "source": [
    "##### <font color=\"#008080\" face=\"Times New Roman\" size=4> - Pre-Processing Dataset</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NYrvF3-hzcIV"
   },
   "source": [
    "<font face=\"Times New Roman\" size=3>\n",
    "To build the model, we first fit our model on the training data and then check the accuracy of its prediction on the test data; Here, 80% of the data is used for training and the other 20% for testing the model.\n",
    "Since the generated data was random, we don't need to shuffle before partitioning.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-mpV4390AESl"
   },
   "outputs": [],
   "source": [
    "frac = 0.8\n",
    "train_size = int(frac*N)\n",
    "X_train = X[:train_size, :]\n",
    "y_train = y[:train_size]\n",
    "\n",
    "X_test = X[train_size:, :]\n",
    "y_test = y[train_size:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iDTgWkM5AESi"
   },
   "source": [
    "<font face=\"Times New Roman\"><div id=\"1-1-1\">\n",
    "### <font color=\"#800080\" size=5>**1-1-1. From Scratch**</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xc8mIZNlzkpY"
   },
   "source": [
    "<font face=\"Times New Roman\" size=3>\n",
    "For this purpose, we define a class called <code>maxMargin_classifier</code>, which contains the methods required to implement this algorithm. In the following, an initial definition and the generality of our intended class to implement the algorithm is given as a map along the way. According to the description of each method, we will try to complete it. The only library we need for this implementation is the numpy library.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E5LZv8WxAESl"
   },
   "source": [
    "#### <font color=\"#008080\" face=\"Times New Roman\" size=4>**Model**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LpEmhZsmAESl"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class maxMargin_classifier:\n",
    "\n",
    "    def __init__(self, learning_rate=1e-3, C=10, n_iters=1000):\n",
    "        \"\"\"\n",
    "        Class initializer: Initialize class parameters including:\n",
    "            lr: learning rate of gradient descent algorithm, default=0.001\n",
    "            C: C hyperparameter of hinge loss in cost function, default=10\n",
    "            n_iters: number of iterations in gradient_descent, default=1000\n",
    "            w: weights(normal vector) of maximal margin hyperplane (parameters of the algorithm)\n",
    "            b: intercept(bias) of maximal margin hyperplane (parameter of the algorithm)\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    #-------------------------------------------#\n",
    "\n",
    "    def init_params(self, X):\n",
    "        \"\"\"\n",
    "        Initializer of the algorithm's parameters (w & b)\n",
    "        Inputs:\n",
    "            X: N*p matrix including N samples with p features\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    #-------------------------------------------#\n",
    "\n",
    "    def get_class_map(self, y):\n",
    "        \"\"\"\n",
    "        Mapping the targets' classes from {0,1} to {-1,1}\n",
    "        Inputs:\n",
    "            y: N-vector of class labels\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    #-------------------------------------------#\n",
    "\n",
    "    def satisfy_constraint(self, x, idx):\n",
    "        \"\"\"\n",
    "        Checks wether the constraint of hinge loss is satisfied, i.e. which loss function(J1 or J2)\n",
    "        and corresponding gradients are needed to be calculated.\n",
    "        Inputs:\n",
    "            x: a training sample\n",
    "            idx: index of x\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    #-------------------------------------------#\n",
    "\n",
    "    def get_gradients(self, constraint, x, idx):\n",
    "        \"\"\"\n",
    "        Get the gradient for a training sample\n",
    "        Inputs:\n",
    "            constraint: wether or not the \"satisfy_constraint\" method returns true\n",
    "            x: a training sample\n",
    "            idx: index of x\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    #-------------------------------------------#\n",
    "\n",
    "    def update_params(self, dw, db):\n",
    "        \"\"\"\n",
    "        Update the parameters, weights(w) and bias(b)\n",
    "        Inputs:\n",
    "            dw: partial derivative of cost function w.r.t. weights\n",
    "            db: partial derivative of cost function w.r.t. bias\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    #-------------------------------------------#\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit the model on the training set\n",
    "        Inputs:\n",
    "            X: N*p matrix of training set, N samples with p features\n",
    "            y: target values of binary classes\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    #-------------------------------------------#\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict the class of new samples.\n",
    "        Inputs:\n",
    "            X: k*p matrix of new samples, k samples with p features\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VdAaSH95JaIq"
   },
   "source": [
    "<font face=\"Times New Roman\" size=3>\n",
    "Now we complete the parts of this class; At first, we implement the basic methods of this class and initialize the parameters of the class. For simplicity, we consider the initial value of weights and bias equal to zero.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vYem4ThqAESl",
    "outputId": "f2aa1394-91c4-42f8-cddc-338856c8905a"
   },
   "outputs": [],
   "source": [
    "...\n",
    "def __init__(self, learning_rate=1e-3, C=10, n_iters=1000):\n",
    "    self.lr = learning_rate\n",
    "    self.C = C\n",
    "    self.n_iters = n_iters\n",
    "    self.w = None\n",
    "    self.b = None\n",
    "\n",
    "def init_params(self, X):\n",
    "    p = X.shape[1]\n",
    "    self.w = np.zeros(p)\n",
    "    self.b = 0\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ost7V86mJzsD"
   },
   "source": [
    "<font face=\"Times New Roman\" size=3>\n",
    "As mentioned, in this algorithm we need our data label to be {−1,1} instead of {0,1}. Therefore, since in most cases, the initial labels are 0 and 1, we need a method to do this conversion. With this in mind, try to complete the <code>get_class_map</code> method. The input to this function is a binary vector of data labels, and you must perform this mapping and return the new $𝑦$ vector.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "529XZD6BAESl",
    "outputId": "68df522e-8e32-4dc8-8d74-21af2a1f68f9"
   },
   "outputs": [],
   "source": [
    "...\n",
    "def get_class_map(self, y):\n",
    "    return #YOUR CODE\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oBGNNVjpL0MR"
   },
   "source": [
    "<font face=\"Times New Roman\" size=3>\n",
    "We also need methods that can obtain and update their corresponding gradients depending on the current value of the model parameters. For this purpose, 3 methods have been written to first check in which cost function mode we are; As it is clear from the defined cost function, this condition is as follows:<br><br>\n",
    "<center> $ y^{(i)}(w^T x^{(i)} + b) \\geq 1 $ </center>\n",
    "<br>Then, according to the result of this condition, we get the gradients with respect to the model parameters, $𝑤$ and $𝑏$ and update them. Of course, you can combine all 3 methods together and implement them in one method.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_hIAs0E7AESl",
    "outputId": "27109426-20fb-4219-e8b7-7c5a382a1e6b"
   },
   "outputs": [],
   "source": [
    "...\n",
    "def satisfy_constraint(self, x, idx):\n",
    "    ##Checks wether   y(w^T x + b) >= 1\n",
    "    linear_model = np.dot(x, self.w) + self.b\n",
    "    return self.encoded_y[idx] * linear_model >= 1\n",
    "\n",
    "def get_gradients(self, constraint, x, idx):\n",
    "    #Case1: J1\n",
    "    if constraint:\n",
    "        dw = self.w\n",
    "        db = 0\n",
    "        return dw, db\n",
    "\n",
    "    #Case2: J2\n",
    "    ### YOUR CODE\n",
    "    return dw, db\n",
    "\n",
    "def update_params(self, dw, db):\n",
    "    #YOUR CODE\n",
    "    pass\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "85yWtzMUNOH7"
   },
   "source": [
    "<font face=\"Times New Roman\" size=3>\n",
    "Now we have to fit our model on the training samples with the help of the functions that we have implemented in the previous section. So first we initialize the parameters of the model and then by mapping the data labels, we find the optimal parameters with the help of the gradient descent algorithm.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VTljDB6AAESm",
    "outputId": "46e55848-40fc-4685-9f3f-e86e55332fed"
   },
   "outputs": [],
   "source": [
    "...\n",
    "def fit(self, X, y):\n",
    "    self.init_params(X) #Initialization of parameters\n",
    "    self.encoded_y = self.get_class_map(y) #Encode y using the \"get_class_map\" method\n",
    "\n",
    "    #Iterate and update your parameters\n",
    "    for _ in range(self.n_iters):\n",
    "        for idx, x in enumerate(X):\n",
    "            constraint = self.satisfy_constraint(x, idx) #Check the constraint for cost function\n",
    "            dw, db = self.get_gradients(constraint, x, idx) #get the gradient using the constraint\n",
    "            self.update_params(dw, db) #Update the parameters\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uAEfxQEVO2NC"
   },
   "source": [
    "<font face=\"Times New Roman\" size=3>\n",
    "Finally, we should be able to predict the class of new examples using our model. For this purpose, implement the predict method.<br>\n",
    "Hint: This can be found using the position of the sample relative to the fitted hyperplane as shown below:<br><br>\n",
    "<center>  $f(x) = w^T x + b$ <br><br>\n",
    "\\begin{cases}\n",
    "    \\hat{y} = +1 \\qquad if \\quad f(x) > 0\\\\\n",
    "    \\\\\n",
    "    \\hat{y} = -1 \\qquad if \\quad f(x) < 0\n",
    "\\end{cases}\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BGDjZo46AESm"
   },
   "outputs": [],
   "source": [
    "...\n",
    "def predict(self, X):\n",
    "    #YOUR CODE\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sOedwEpjQdXP"
   },
   "source": [
    "\n",
    "<font face=\"Times New Roman\" size=3>\n",
    "Now we have all the puzzle pieces we need to complete our class and just put them together.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7KOJMOH2AESm"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class maxMargin_classifier:\n",
    "    def __init__(self, learning_rate=1e-3, C=10, n_iters=1000):\n",
    "        self.lr = learning_rate\n",
    "        self.C = C\n",
    "        self.n_iters = n_iters\n",
    "        self.w = None\n",
    "        self.b = None\n",
    "    #_____________________________________________________#\n",
    "\n",
    "    def init_params(self, X):\n",
    "        p = X.shape[1]\n",
    "        self.w = np.zeros(p)\n",
    "        self.b = 0\n",
    "    #_____________________________________________________#\n",
    "\n",
    "    def get_class_map(self, y):\n",
    "        #YOUR CODE\n",
    "        return np.where(y <= 0, -1, 1)\n",
    "        #pass\n",
    "\n",
    "    #_____________________________________________________#\n",
    "\n",
    "\n",
    "    def satisfy_constraint(self, x, idx):\n",
    "        ##Checks wether   y(w^T x + b) >= 1\n",
    "        linear_model = np.dot(x, self.w) + self.b\n",
    "        return self.encoded_y[idx] * linear_model >= 1\n",
    "    #_____________________________________________________#\n",
    "\n",
    "    def get_gradients(self, constraint, x, idx):\n",
    "        #Case1: J1\n",
    "        if constraint:\n",
    "            dw = self.w\n",
    "            db = 0\n",
    "            return dw, db\n",
    "        #Case2: J2\n",
    "        dw = self.w - self.C * np.dot(self.encoded_y[idx], x)\n",
    "        db = - self.C * self.encoded_y[idx]\n",
    "        return dw, db\n",
    "    #_____________________________________________________#\n",
    "\n",
    "    def update_params(self, dw, db):\n",
    "        #YOUR CODE\n",
    "        self.w -= self.lr * dw\n",
    "        self.b -= self.lr * db\n",
    "        #pass\n",
    "    #_____________________________________________________#\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.init_params(X)\n",
    "        self.encoded_y = self.get_class_map(y)\n",
    "\n",
    "        for _ in range(self.n_iters):\n",
    "            for idx, x in enumerate(X):\n",
    "                constraint = self.satisfy_constraint(x, idx)\n",
    "                dw, db = self.get_gradients(constraint, x, idx)\n",
    "                self.update_params(dw, db)\n",
    "\n",
    "    #_____________________________________________________#\n",
    "\n",
    "    def predict(self, X):\n",
    "        #YOUR CODE\n",
    "        estimate = np.dot(X, self.w) + self.b\n",
    "        prediction = np.sign(estimate)\n",
    "        return np.where(prediction == -1, 0, 1)\n",
    "        #pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gSvKw0pdAESm"
   },
   "source": [
    "#### <font color=\"#008080\" face=\"Times New Roman\" size=4>**Train**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HI0wPMJLAESm"
   },
   "outputs": [],
   "source": [
    "#Build the classifier\n",
    "clf = maxMargin_classifier(learning_rate=1e-3, C=10, n_iters=1000)\n",
    "#Fit the model on the train data\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DCicBNv3AESm"
   },
   "source": [
    "#### <font color=\"#008080\" face=\"Times New Roman\" size=4>**Evaluation**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n2ViYLYjAESn"
   },
   "source": [
    "##### <font color=\"#008080\" face=\"Times New Roman\" size=4> - Prediction</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ye6-qIisAESn"
   },
   "outputs": [],
   "source": [
    "#Predict the model on test data\n",
    "y_pred_test = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6fweiFAXAESn"
   },
   "source": [
    "##### <font color=\"#008080\" face=\"Times New Roman\" size=4> - Accuracy </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "brnmUn7rAESn",
    "outputId": "3a08c285-1a31-4d56-8458-5d50e4b74da5"
   },
   "outputs": [],
   "source": [
    "#Find the accuracy of model on test set using the predicted labels with their true values\n",
    "accuracy = np.sum(y_test==y_pred_test) / len(y_test)\n",
    "\n",
    "print(\"Maximal Margin Classifier Accuracy:\", accuracy*100, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rpEZrgzGAESn"
   },
   "source": [
    "#### <font color=\"#008080\" face=\"Times New Roman\" size=4>**Visualization**</font>\n",
    "<font face=\"Times New Roman\" size=3>\n",
    "Finally, we can plot the optimal boundary along with the obtained margin and confirm our results.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 522
    },
    "id": "7ouxoEg6AESn",
    "outputId": "3a90b6a8-93d6-4c4c-c24a-3064aaf9752b"
   },
   "outputs": [],
   "source": [
    "#a simple functin to get the hyperplane from the weights and bias of model\n",
    "def get_hyperplane(x, w, b, offset):\n",
    "    return (-w[0] * x - b + offset) / w[1]\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10,6))\n",
    "\n",
    "plt.set_cmap('PiYG')\n",
    "plt.scatter(X_train[:, 0], X_train[:, 1], marker='o', c=y_train, s=100, alpha=0.75)\n",
    "plt.scatter(X_test[:, 0], X_test[:, 1], marker=\"x\", c=y_test, s=100, alpha=0.75)\n",
    "\n",
    "x0_1 = np.amin(X_train[:, 0])\n",
    "x0_2 = np.amax(X_train[:, 0])\n",
    "\n",
    "x1_1 = get_hyperplane(x0_1, clf.w, clf.b, 0)\n",
    "x1_2 = get_hyperplane(x0_2, clf.w, clf.b, 0)\n",
    "\n",
    "x1_1_m = get_hyperplane(x0_1, clf.w, clf.b, -1)\n",
    "x1_2_m = get_hyperplane(x0_2, clf.w, clf.b, -1)\n",
    "\n",
    "x1_1_p = get_hyperplane(x0_1, clf.w, clf.b, 1)\n",
    "x1_2_p = get_hyperplane(x0_2, clf.w, clf.b, 1)\n",
    "\n",
    "ax.plot([x0_1, x0_2], [x1_1, x1_2], \"-\", c='k', lw=1, alpha=0.9)\n",
    "ax.plot([x0_1, x0_2], [x1_1_m, x1_2_m], \"--\", c='grey', lw=1, alpha=0.8)\n",
    "ax.plot([x0_1, x0_2], [x1_1_p, x1_2_p], \"--\", c='grey', lw=1, alpha=0.8)\n",
    "\n",
    "x1_min = np.amin(X[:, 1])\n",
    "x1_max = np.amax(X[:, 1])\n",
    "ax.set_ylim([x1_min - 3, x1_max + 3])\n",
    "\n",
    "for spine in ['top','right']:\n",
    "    ax.spines[spine].set_visible(False)\n",
    "\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "68iIHGEgAl1w"
   },
   "source": [
    "<font face=\"Times New Roman\"><div id=\"1-1-2\">\n",
    "### <font color=\"#800080\" size=5>**1-1-2. Using Scikit Learn Library**</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Odo3Fklfz8LM"
   },
   "source": [
    "<font face=\"Times New Roman\" size=3><div>\n",
    "Scikit-learn is an open source Python library that provides many tools for machine learning and statistical modeling of data such as classification, regression, clustering, and dimensionality reduction. This library is designed based on [Numpy](https://numpy.org/), [Pandas](https://pandas.pydata.org/), [Scipy](https://scipy.org/) and [Matplotlib](https://matplotlib.org/) libraries.\n",
    "To learn more about scikit-learn, you can visit its [website](https://scikit-learn.org/stable/).</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "14oWGLnbAl1y"
   },
   "source": [
    "#### <font color=\"#008080\" face=\"Times New Roman\" size=4>**Model**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IvWRpj_Yz8Pn"
   },
   "source": [
    "<font face=\"Times New Roman\" size=3>\n",
    "Like other classifiers, SVC is modeled with two arrays:\n",
    "\n",
    "*   The X matrix that holds the training samples and has the size of [n_samples, n_features].\n",
    "*   The y array that stores the target values. The label class is for test samples and is of size [n_samples].\n",
    "\n",
    "Many machine learning algorithms have hyperparameters that we must specify before training the model; For example, in this classification, we must specify the kernel hyperparameter. This hyperparameter will be explained more fully in the following; But for now, know that by setting it to linear, it actually trains the desired Maxiaml Margin Classifier.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ozKb37LXAl1y"
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# create Support Vector Classifier using a linear kernel\n",
    "clf = SVC(kernel='linear')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "osVXNTRxAl1y"
   },
   "source": [
    "#### <font color=\"#008080\" face=\"Times New Roman\" size=4>**Train**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UDiDGUSN1wP8"
   },
   "source": [
    "<font face=\"Times New Roman\" size=3>\n",
    "Finally, we fit the classifier on input X and output y by applying the fit method.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 74
    },
    "id": "zXCt34qrAl1z",
    "outputId": "3bf0c4cd-6d9e-44d3-ab1a-78b72bbf520f"
   },
   "outputs": [],
   "source": [
    "# train the classifier\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UNVQE4NhAl1z"
   },
   "source": [
    "###### <font color=\"#008080\" face=\"Times New Roman\" size=3> - Results </font>\n",
    "<font face=\"Times New Roman\" size=3>\n",
    "Now we can get the weight of each feature of the input data with the help of the <code>coef_</code> attribute and the bias of the separating hyperplane with the help of the <code>intercept_</code> attribute.\n",
    "</forn>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aGTyxoTtAl1z",
    "outputId": "d74f8ea5-3209-4a49-b26a-6c64def8e45a"
   },
   "outputs": [],
   "source": [
    "print(\"w =\", clf.coef_)\n",
    "print(\"b =\", clf.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T2jOGw_C2ZqS"
   },
   "source": [
    "<font face=\"Times New Roman\" size=3>\n",
    "The above values show that the equation of the separating hyperplane is: <br> <center>$f(X) = w_1x_1+w_2x_2+b = -0.1634x_1 -0.2455x_2 -0.8717 = 0$\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p4KkXjPL2_9N"
   },
   "source": [
    "<font face=\"Times New Roman\" size=3>\n",
    "In this model, the support vectors are stored in <code>support_vectors_</code> and we can see which examples contributed to determining the optimal hyperplane.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aq7_33rVAl10",
    "outputId": "2beecc9a-bd56-4475-84cb-5fa400d848af"
   },
   "outputs": [],
   "source": [
    "clf.support_vectors_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ls4Has4y5Ey_"
   },
   "source": [
    "<font face=\"Times New Roman\" size=3>\n",
    "We can also predict new data class using the <code>predict</code> method.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QoMKV0U55And",
    "outputId": "b9c721fa-cfac-4bac-e525-dfec99df8d4d"
   },
   "outputs": [],
   "source": [
    "new_samples = [[0, 5],\n",
    "              [-4, -1]]\n",
    "print(clf.predict(new_samples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SULBwRA6Al10"
   },
   "source": [
    "#### <font color=\"#008080\" face=\"Times New Roman\" size=4>**Visualization**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4dMMxYw_4YsX"
   },
   "source": [
    "<font face=\"Times New Roman\" size=3>\n",
    "Finally, with the help of the <code>plot_svc_decision_boundary</code> method written below, we draw the optimal line separating the two classes obtained in this model.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ib8Or3t8Al11"
   },
   "outputs": [],
   "source": [
    "# Plot helper function\n",
    "def plot_svc_decision_function(clf, ax=None):\n",
    "    \"\"\"Plot the decision function for a 2D SVC\"\"\"\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "\n",
    "    x = np.linspace(plt.xlim()[0], plt.xlim()[1], 30)\n",
    "    y = np.linspace(plt.ylim()[0], plt.ylim()[1], 30)\n",
    "    Y, X = np.meshgrid(y, x)\n",
    "\n",
    "    P = np.zeros_like(X)\n",
    "    for i, xi in enumerate(x):\n",
    "        for j, yj in enumerate(y):\n",
    "            P[i, j] = clf.decision_function([[xi, yj]])\n",
    "\n",
    "    # plot the margins\n",
    "    ax.contour(X, Y, P, colors='k',\n",
    "               levels=[-1, 0, 1], alpha=0.5,\n",
    "               linestyles=['--', '-', '--'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 565
    },
    "id": "jIrsI6dEAl11",
    "outputId": "7e6e5a2a-68a4-4408-e401-883d479cdf77"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "# plot data and margin\n",
    "scatter_data = plt.scatter(X[:, 0], X[:, 1], c=y, s=100, edgecolors='black', cmap='coolwarm', alpha=0.75)\n",
    "plt.legend(handles=scatter_data.legend_elements()[0], labels=['Class 1','Class 0'], fontsize='x-large', markerscale=2)\n",
    "plot_svc_decision_function(clf)\n",
    "\n",
    "# plot support vectors\n",
    "scatter_support = plt.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1],\n",
    "                              s=200, edgecolors='k', facecolors='none')\n",
    "\n",
    "plt.xlabel(\"$x_1$\")\n",
    "plt.ylabel(\"$x_2$\")\n",
    "plt.title(\"Optimal Separating Hyperplane with Scikit-learn\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OZyxj4cjnzW_"
   },
   "source": [
    "<font face=\"Times New Roman\"><div id=\"1-2\">\n",
    "## <font color=\"#800080\" size=6>**1-2. Soft Margin SVM**</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KKvBsCTN0V9x"
   },
   "source": [
    "<font face=\"Times New Roman\" size=3>\n",
    "Also called: Soft Margin Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0dkxBfoQgySR"
   },
   "source": [
    "<font face=\"Times New Roman\"><div id=\"1-3-1\">\n",
    "### <font color=\"#800080\" size=5>**1-2-1. What is the Challenge?**</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F_oMVJ-LYaHU"
   },
   "source": [
    "<font face=\"Times New Roman\" size=3>\n",
    "As you can guess and it is mentioned in the class, Maximal Margin Classifier only works correctly when the data of two classes are linearly separable. Also, the border of this model is determined by the samples that are placed on the border; In fact, if only one of the training samples close to the border has some size change, the decision border changes completely! It is important to mention that our data in reality has some noise in most cases and therefore the above problem can have an adverse effect on our decision boundary. Therefore, this model is highly capable of overfitting. For a better understanding, we will try the Maximal Margin Classifier model using Scikit-learn for the mentioned situation. First, we fit our model on the primary dataset that has linear separability; Then we add a noise sample to each class and fit a new model on them to see their effect on the decision boundary.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "09ucf-Qlbn3r"
   },
   "source": [
    "#### <font color=\"#008080\" face=\"Times New Roman\" size=4>**Dataset**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jfNUdSo-anuc"
   },
   "source": [
    "##### <font color=\"#008080\" face=\"Times New Roman\" size=4> - Loading Dataset </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P7dd4lURatnQ"
   },
   "outputs": [],
   "source": [
    "N = 250\n",
    "p = 2\n",
    "X, y = datasets.make_blobs(\n",
    "    n_samples=N, n_features=p, centers=2, cluster_std=1.05, random_state=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NxCQr48PbZLB"
   },
   "source": [
    "##### <font color=\"#008080\" face=\"Times New Roman\" size=4> - Pre-Processing Dataset</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u2KjWHwubasS"
   },
   "outputs": [],
   "source": [
    "# Add noise sample\n",
    "X_noisy = np.concatenate((X,[[-5,1], [-6,-1]]), axis=0)\n",
    "y_noisy = np.append(y,[0,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j4W1zcVSd9ag"
   },
   "source": [
    "##### <font color=\"#008080\" face=\"Times New Roman\" size=4> - Visualizing Dataset </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 488
    },
    "id": "PeH4lAXQb3HB",
    "outputId": "2db01713-a92f-4db6-b233-358630da1047"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot intial data\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.subplot(1,2,1)\n",
    "scatter = plt.scatter(X[:, 0], X[:, 1], c=y, s=100, cmap='coolwarm', edgecolor='black', alpha=0.75)\n",
    "plt.legend(handles=scatter.legend_elements()[0], labels=['Class 1','Class 0'], fontsize='x-large', markerscale=1.5)\n",
    "plt.xlabel(\"$x_1$\")\n",
    "plt.ylabel(\"$x_2$\")\n",
    "plt.title(\"Scatter Plot of Data\");\n",
    "\n",
    "# Plot noisy data\n",
    "plt.subplot(1,2,2)\n",
    "scatter = plt.scatter(X_noisy[:, 0], X_noisy[:, 1], c=y_noisy, s=100, cmap='coolwarm', edgecolor='black', alpha=0.75)\n",
    "plt.legend(handles=scatter.legend_elements()[0], labels=['Class 1','Class 0'], fontsize='x-large', markerscale=1.5)\n",
    "plt.xlabel(\"$x_1$\")\n",
    "plt.ylabel(\"$x_2$\")\n",
    "plt.title(\"Scatter Plot of Noisy Data\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KjPdsXy0bdYt"
   },
   "source": [
    "#### <font color=\"#008080\" face=\"Times New Roman\" size=4>**Model**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mLunGFozbEvq"
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "#Maximal Margin Classifier on initial dataset\n",
    "clf1 = SVC(kernel='linear', C=10)\n",
    "\n",
    "#Maximal Margin Classifier on the dataset with a noisy sample\n",
    "clf2 = SVC(kernel='linear', C=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QF0FPJzbeLc2"
   },
   "source": [
    "#### <font color=\"#008080\" face=\"Times New Roman\" size=4>**Train**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "arTgfIAieZUV"
   },
   "outputs": [],
   "source": [
    "#Maximal Margin Classifier on initial dataset\n",
    "clf1.fit(X, y);\n",
    "\n",
    "#Maximal Margin Classifier on the dataset with a noisy sample\n",
    "clf2.fit(X_noisy, y_noisy);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6HiZb9tge2J1"
   },
   "source": [
    "#### <font color=\"#008080\" face=\"Times New Roman\" size=4>**Visualization**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 488
    },
    "id": "zdrKJEine1Rz",
    "outputId": "4cbb36df-47ba-4a32-a8b9-735f0edcc88f"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,5))\n",
    "\n",
    "# plot data and margin on initial dataset\n",
    "plt.subplot(1,2,1)\n",
    "scatter1 = plt.scatter(X[:, 0], X[:, 1], c=y, s=100, cmap='coolwarm', alpha=0.75)\n",
    "plt.legend(handles=scatter1.legend_elements()[0], labels=['Class 1','Class 0'], fontsize='x-large', markerscale=1.5)\n",
    "plot_svc_decision_function(clf1)\n",
    "# plot support vectors\n",
    "plt.scatter(clf1.support_vectors_[:, 0], clf1.support_vectors_[:, 1], s=200, edgecolors='k', facecolors='none');\n",
    "plt.xlabel(\"$x_1$\")\n",
    "plt.ylabel(\"$x_2$\")\n",
    "plt.title(\"Decision Boundary and Margins on Initial Dataset\");\n",
    "\n",
    "\n",
    "# plot data and margin on noisy dataset\n",
    "plt.subplot(1,2,2)\n",
    "scatter2 = plt.scatter(X_noisy[:, 0], X_noisy[:, 1], c=y_noisy, cmap='coolwarm', s=100, alpha=0.75)\n",
    "plt.legend(handles=scatter2.legend_elements()[0], labels=['Class 1','Class 0'], fontsize='x-large', markerscale=1.5)\n",
    "plot_svc_decision_function(clf2)\n",
    "# plot support vectors\n",
    "plt.scatter(clf2.support_vectors_[:, 0], clf2.support_vectors_[:, 1], s=200, edgecolors='k', facecolors='none')\n",
    "plt.xlabel(\"$x_1$\")\n",
    "plt.ylabel(\"$x_2$\")\n",
    "plt.title(\"Decision Boundary and Margins on Noisy Dataset\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7dCWyfIkfv6j"
   },
   "source": [
    "<font face=\"Times New Roman\" size=3>\n",
    "As we can see, in the diagram on the left of the drawn decision boundary, its margin has a significant value; While in the graph on the right, due to the presence of two noise samples, the decision boundary and its margins have changed and the margin value has decreased a lot. Therefore, our confidence in the decision boundary will decrease due to the reduction of its margin.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A02dqB4zg6mi"
   },
   "source": [
    "<font face=\"Times New Roman\"><div id=\"1-3-1\">\n",
    "### <font color=\"#800080\" size=5>**1-2-2. Soft Margin Classifier**</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DlLpa_I9h59d"
   },
   "source": [
    "<font face=\"Times New Roman\" size=3>\n",
    "The solution to this problem is to use soft margin classifier. Instead of finding a hypermap that completely separates all the data of the two classes, you can use a hypermap that separates most of the data of the two classes from each other; Our motivation for doing this can be that our classifier has less sensitivity to each sample and thus has a lower probability of overfitting. In fact, our model misclassifies a relatively small number of training samples in order to have a better generalization on the test data.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eVbZVyBbDA_0"
   },
   "source": [
    "#### <font color=\"#008080\" face=\"Times New Roman\" size=4>**Dataset**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Iel5jWADA_0"
   },
   "source": [
    "##### <font color=\"#008080\" face=\"Times New Roman\" size=4> - Loading Dataset </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nRifFGuPDA_1"
   },
   "outputs": [],
   "source": [
    "N = 200\n",
    "p = 2\n",
    "X, y = datasets.make_blobs(\n",
    "    n_samples=N, n_features=p, centers=2, cluster_std=3, random_state=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jsg8bozFDA_1"
   },
   "source": [
    "##### <font color=\"#008080\" face=\"Times New Roman\" size=4> - Visualizing Dataset </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 565
    },
    "id": "Za-UpDrbDA_1",
    "outputId": "736d1d38-986e-49ef-b1ec-c886d5b997c2"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "scatter = plt.scatter(X[:, 0], X[:, 1], c=y, s=100, cmap='coolwarm', edgecolor='black', alpha=0.75)\n",
    "plt.legend(handles=scatter.legend_elements()[0], labels=['Class 1','Class 0'], fontsize='x-large', markerscale=1.5)\n",
    "plt.xlabel(\"$x_1$\")\n",
    "plt.ylabel(\"$x_2$\")\n",
    "plt.title(\"Scatter Plot of Data\");\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aeSGaeJfod5f"
   },
   "source": [
    "#### <font color=\"#008080\" face=\"Times New Roman\" size=4>**Model**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DYFaQUU5Xqfs"
   },
   "source": [
    "<font face=\"Times New Roman\" size=3>\n",
    "As you know from the class and theory notebooks, the soft margin machine has a hyperparameter C that we must determine. For this, we train four models with four different C values to see their results.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EhuxC0bponm6"
   },
   "outputs": [],
   "source": [
    "# Define C values\n",
    "C_vals = [0.01, 0.1, 1, 10]\n",
    "\n",
    "# Initialize models\n",
    "models = []\n",
    "for c in C_vals:\n",
    "  models.append(SVC(kernel='linear', C=c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n7BTS7r2o2j0"
   },
   "source": [
    "#### <font color=\"#008080\" face=\"Times New Roman\" size=4>**Train**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NEpv5P1ro5GP"
   },
   "outputs": [],
   "source": [
    "for clf in models:\n",
    "  clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q1cHt7zmpA4v"
   },
   "source": [
    "#### <font color=\"#008080\" face=\"Times New Roman\" size=4>**Visualization**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 873
    },
    "id": "IER02-MfjIs3",
    "outputId": "34aec072-f8eb-46f3-898f-fe89cbabf652"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 10))\n",
    "for i in range(len(models)):\n",
    "    plt.subplot(2,2,i+1)\n",
    "    scatter = plt.scatter(X[:, 0], X[:, 1], c=y, s=100, cmap='coolwarm', alpha=0.75)\n",
    "    plt.legend(handles=scatter.legend_elements()[0], labels=['Class 1','Class 0'], fontsize='x-large', markerscale=1.5)\n",
    "\n",
    "    # Select classifier model\n",
    "    clf = models[i]\n",
    "\n",
    "    # Plot using helper method\n",
    "    plot_svc_decision_function(clf)\n",
    "\n",
    "    # plot support vectors\n",
    "    plt.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1], s=200, edgecolors='k', facecolors='none');\n",
    "\n",
    "    plt.xlabel(\"$x_1$\")\n",
    "    plt.ylabel(\"$x_2$\")\n",
    "    plt.title(\"C = %.2f\" % C_vals[i])\n",
    "    del clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SoNcc_evpt98"
   },
   "source": [
    "<font face=\"Times New Roman\" size=3>\n",
    "As you can see, with the increase of C, the obtained margin has become smaller and also the number of support samples has also decreased; It means that fewer samples have played a role in defining this border. So, we can use the soft margin classifier for classification by obtaining a suitable value for this hyperparameter so that the model has more robustness than the training samples.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3pZCWtkeo2Q_"
   },
   "source": [
    "<font face=\"Times New Roman\"><div id=\"1-3\">\n",
    "## <font color=\"#800080\" size=6>**1-3. Kernel SVM**</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BG6JDKXAKthv"
   },
   "source": [
    "<font face=\"Times New Roman\"><div id=\"1-3-1\">\n",
    "### <font color=\"#800080\" size=5>**1-3-1. What is the Challenge?**</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2CgVV0wnt16T"
   },
   "source": [
    "<font face=\"Times New Roman\" size=3>\n",
    "Despite the use of Soft Margin Classifier, our model is still limited to detect linear boundaries; While linear models have many limitations and do not respond to many real world problems. In the following, we give an example of the inability of linear models to classify non-linear data.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I3TA7qSXJJug"
   },
   "source": [
    "#### <font color=\"#008080\" face=\"Times New Roman\" size=4>**Dataset**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vORapzaoJJug"
   },
   "source": [
    "##### <font color=\"#008080\" face=\"Times New Roman\" size=4> - Loading Dataset </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oipd4RysJJug"
   },
   "outputs": [],
   "source": [
    "# create dataset\n",
    "X, y = datasets.make_circles(200, factor=0.1, noise=0.1, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2JNOCPY3JJuh"
   },
   "source": [
    "##### <font color=\"#008080\" face=\"Times New Roman\" size=4> - Visualization </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 719
    },
    "id": "0ppzbAv8JJuh",
    "outputId": "6453ddd2-bad2-4555-8b68-1f14ab99cd96"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "scatter = plt.scatter(X[:, 0], X[:, 1], marker='o', c=y, s=100, cmap='coolwarm', alpha=0.75)\n",
    "plt.legend(handles=scatter.legend_elements()[0], labels=['Class 1','Class 0'], fontsize='x-large', markerscale=1.5)\n",
    "plt.xlabel(\"$x_1$\")\n",
    "plt.ylabel(\"$x_2$\")\n",
    "plt.title(\"Scatter Plot of the Dataset\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3bDvO3dZJXLK"
   },
   "source": [
    "##### <font color=\"#008080\" face=\"Times New Roman\" size=4> - Pre-Processing </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rRW7VC3UJbkA"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DdttC2gIJhFv"
   },
   "source": [
    "#### <font color=\"#008080\" face=\"Times New Roman\" size=4>**Model**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y-sIyOjZJhFv"
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "#Build the classifier\n",
    "clf = SVC(kernel='linear', C=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d1zcyxjIJnxg"
   },
   "source": [
    "#### <font color=\"#008080\" face=\"Times New Roman\" size=4>**Train**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 74
    },
    "id": "Xd3PDHHnJnxg",
    "outputId": "6cb92078-295d-41ae-8935-37a13cd556c3"
   },
   "outputs": [],
   "source": [
    "# Fit the model on the train data\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PcxB-PgbJ1FT"
   },
   "source": [
    "#### <font color=\"#008080\" face=\"Times New Roman\" size=4>**Evaluation**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ftuapgm7J1FT"
   },
   "source": [
    "##### <font color=\"#008080\" face=\"Times New Roman\" size=4> - Prediction</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6JBNkgHIJ4P2"
   },
   "outputs": [],
   "source": [
    "# Predict the model on test data\n",
    "y_pred_test = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ohDlrArFJ1FU"
   },
   "source": [
    "##### <font color=\"#008080\" face=\"Times New Roman\" size=4> - Accuracy </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5cUUwnGCJV5U",
    "outputId": "011710fb-bce1-4aa4-aa16-0b048ab7286d"
   },
   "outputs": [],
   "source": [
    "#Find the accuracy of model on test set by comparing the predicted labels with their true values\n",
    "accuracy = np.sum(y_test==y_pred_test) / len(y_test)\n",
    "\n",
    "print(\"Maximal Margin Classifier Accuracy:\", accuracy*100, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CC0CPETkugza"
   },
   "source": [
    "<font face=\"Times New Roman\" size=3>\n",
    "As you can see, the accuracy of the model is 52.5%, which is far from our ideal.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TOn8ae25J-Px"
   },
   "source": [
    "#### <font color=\"#008080\" face=\"Times New Roman\" size=4>**Visualization**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S4EUe66Tuu-2"
   },
   "source": [
    "<font face=\"Times New Roman\" size=3>\n",
    "We can also plot boundaries and margins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 719
    },
    "id": "M9hJfmz5J8UQ",
    "outputId": "542aad89-09f9-4b8f-dfa5-6296b65157ab"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "train_plt = plt.scatter(X_train[:, 0], X_train[:, 1], marker='o', c=y_train, cmap='coolwarm', s=100, alpha=0.75)\n",
    "test_plt = plt.scatter(X_test[:, 0], X_test[:, 1], marker=\"x\", c=y_test, cmap='coolwarm', s=100, alpha=0.75)\n",
    "plot_svc_decision_function(clf)\n",
    "plt.xlabel(\"$x_1$\")\n",
    "plt.ylabel(\"$x_2$\")\n",
    "plt.title(\"Decision Boundary and Margins on non-Linearly Separable Data\")\n",
    "plt.legend(handles=train_plt.legend_elements()[0], labels=['Class 1','Class 0'], fontsize='x-large', markerscale=1.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RnJ3PbxLKkyu"
   },
   "source": [
    "<font face=\"Times New Roman\"><div id=\"1-3-2\">\n",
    "### <font color=\"#800080\" size=5>**1-3-2. Introduction to Kernel**</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uRXLgc__ZPYW"
   },
   "source": [
    "<font face=\"Times New Roman\" size=3>\n",
    "As we said, sometimes the data of two classes may not be linearly separable and we should use a curve as a separating boundary. In fact, we should be able to obtain non-linear boundaries so that our algorithm can be used more. One of the possible ways to achieve this goal is to expand our feature space to higher dimensions through feature transformation. Consider the dataset from the previous section; As it is clear from the data distribution, a circular boundary can separate the data of two classes from each other; But our algorithm is limited to detect linear boundaries. Now we define a new feature as $X_3 = X_1^2+X_2^2$ for the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JGusVztLLKHA"
   },
   "source": [
    "#### <font color=\"#008080\" face=\"Times New Roman\" size=4>**Dataset**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D2hHo3jxLKHA"
   },
   "source": [
    "##### <font color=\"#008080\" face=\"Times New Roman\" size=4> - Loading Dataset </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SarC3dR8LKHA"
   },
   "outputs": [],
   "source": [
    "# create dataset\n",
    "X, y = datasets.make_circles(200, factor=0.1, noise=0.1, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GYdgEkdDLKHB"
   },
   "source": [
    "##### <font color=\"#008080\" face=\"Times New Roman\" size=4> - Pre-Processing </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EyNSQiZCLO2D"
   },
   "outputs": [],
   "source": [
    "#Add a new feature as: x_3 = x_1^2+x_2^2\n",
    "X_1 = X[:,0]\n",
    "X_2 = X[:, 1]\n",
    "X_3 = X_1**2 + X_2**2\n",
    "X3d = np.concatenate((X, X_3.reshape(-1,1)), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L-YMqtB5LKHB"
   },
   "source": [
    "##### <font color=\"#008080\" face=\"Times New Roman\" size=4> - Visualization </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 724
    },
    "id": "p9PwL5A-LFIY",
    "outputId": "c049360d-68b2-40cd-e0ce-2190045965f6"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "scatter2d = plt.scatter(X[:, 0], X[:, 1], marker='o', c=y, s=100, cmap='coolwarm', alpha=0.75)\n",
    "plt.legend(handles=scatter2d.legend_elements()[0], labels=['Class 1','Class 0'], fontsize='x-large', markerscale=1.5)\n",
    "plt.xlabel(\"$x_1$\", fontsize=15)\n",
    "plt.ylabel(\"$x_2$\", fontsize=15)\n",
    "plt.title(\"Original Feature Space\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 675
    },
    "id": "iVp9SGb0vssA",
    "outputId": "61700bf2-003e-4557-ad25-1f5c1b532683"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter3D(X3d[:, 0], X3d[:, 1], X3d[:, 2], marker='o', c=y, s=100, cmap='coolwarm', alpha=0.75)\n",
    "ax.view_init(elev=5, azim=30)\n",
    "ax.set_xlabel('$x_1$', fontsize=15)\n",
    "ax.set_ylabel('$x_2$', fontsize=15)\n",
    "ax.set_zlabel('$x_3=x_1^2+x_2^2$', fontsize=15)\n",
    "ax.set_title('New Feature Space')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gxOmuXIGa2fG"
   },
   "source": [
    "<font face=\"Times New Roman\" size=3>\n",
    "Now we can use a linear model on the new data to classify the two classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B-nGWBOrIrDe"
   },
   "source": [
    "<font face=\"Times New Roman\"><div id=\"1-3-3\">\n",
    "### <font color=\"#800080\" size=5>**1-3-3. Polynomial Kernel**</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z4EvVkkec1gn"
   },
   "source": [
    "<font face=\"Times New Roman\" size=3>\n",
    "As you saw in the previous example, adding the second-order feature made our data linearly separable in the new space. A good idea would be to add higher degrees to our feature space, resulting in a high-order classifier curve in the original space. Using the polynomial kernel we can add this to the feature space without having to add them ourselves; So, in this case, increasing the number of features will not cause any problem.\n",
    "<br>Next, on the following dataset, we use the 3rd order kernel once and the 9th order kernel once to train the model and draw their decision area. For this, it is enough to set the <code>kernel</code> hyperparameter to <code>poly</code> and specify its degree. Also, like the Soft Margin Classifier problem, we must also specify the <code>C</code> parameter. For simplicity and better display of separated regions of classes, we use the <code>plot_decision_region</code> function from the <code>mlxtend</code> library; so if you don't have it installed, install it using <code>pip install mlxtend</code>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1kwiWMyhL0Cw"
   },
   "source": [
    "#### <font color=\"#008080\" face=\"Times New Roman\" size=4>**Dataset**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EcUj0p-zL0Cw"
   },
   "source": [
    "##### <font color=\"#008080\" face=\"Times New Roman\" size=4> - Loading Dataset </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jm7STaN0L0Cw"
   },
   "outputs": [],
   "source": [
    "# create dataset\n",
    "X, y = datasets.make_moons(n_samples=200,noise=0.15, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZpIDH-M4L0Cx"
   },
   "source": [
    "##### <font color=\"#008080\" face=\"Times New Roman\" size=4> - Visualization </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 565
    },
    "id": "D-WXx_2yLfbr",
    "outputId": "bef33e90-4b1b-4c8a-d116-b4dfb93590ce"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "scatter = plt.scatter(X[:, 0], X[:, 1], marker='o', c=y, s=100, cmap='coolwarm', alpha=0.75)\n",
    "plt.legend(handles=scatter.legend_elements()[0], labels=['Class 1','Class 0'], fontsize='x-large', markerscale=1.5)\n",
    "plt.xlabel(\"$x_1$\")\n",
    "plt.ylabel(\"$x_2$\")\n",
    "plt.title(\"Scatter Plot of the Dataset\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "950JsRNjMAny"
   },
   "source": [
    "#### <font color=\"#008080\" face=\"Times New Roman\" size=4>**Model**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rpQN1W-iMAnz"
   },
   "outputs": [],
   "source": [
    "# A 3rd order poly kernel svc\n",
    "clf_3rd_order = SVC(kernel='poly', degree=3, C=10)\n",
    "\n",
    "# A 9th order poly kernel svc\n",
    "clf_9th_order = SVC(kernel='poly', degree=9, C=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iOPf8PsJMHZ4"
   },
   "source": [
    "#### <font color=\"#008080\" face=\"Times New Roman\" size=4>**Train**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 74
    },
    "id": "dhd4xKrtMHxd",
    "outputId": "370592a6-9f85-44a0-f6a2-486982872c15"
   },
   "outputs": [],
   "source": [
    "clf_3rd_order.fit(X, y)\n",
    "\n",
    "clf_9th_order.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a6xyh9qQMNUh"
   },
   "source": [
    "#### <font color=\"#008080\" face=\"Times New Roman\" size=4>**Visualization**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 565
    },
    "id": "afA_VukILhB5",
    "outputId": "ad556e9f-75d7-47f6-ef31-4b90e6eacbdf"
   },
   "outputs": [],
   "source": [
    "from mlxtend.plotting import plot_decision_regions\n",
    "plt.figure(figsize=(16, 6))\n",
    "\n",
    "# Plot the 3rd order poly kernel svc\n",
    "plt.subplot(1,2,1)\n",
    "plot_decision_regions(X, y, clf=clf_3rd_order, legend=1)\n",
    "plt.xlabel(\"$x_1$\")\n",
    "plt.ylabel(\"$x_2$\")\n",
    "plt.title(\"Decision Region of 3rd Order Polynomial Kernel\")\n",
    "\n",
    "# Plot the 9th order poly kernel svc\n",
    "plt.subplot(1,2,2)\n",
    "plot_decision_regions(X, y, clf=clf_9th_order, legend=1)\n",
    "plt.xlabel(\"$x_1$\")\n",
    "plt.ylabel(\"$x_2$\")\n",
    "plt.title(\"Decision Region of 9th Order Polynomial Kernel\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UEAItcU_fH3v"
   },
   "source": [
    "<font face=\"Times New Roman\" size=3>\n",
    "As you can see, adding higher degrees of features does not necessarily lead to the improvement of the decision boundary and the best degree should be found using cross validation. In fact, if our model is overfit, lower orders should be used as the degree of this kernel, and if the model is underfit, its complexity should be increased by increasing this degree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HBT59RLpIyUY"
   },
   "source": [
    "<font face=\"Times New Roman\"><div id=\"1-3-4\">\n",
    "### <font color=\"#800080\" size=5>**1-3-4. Radial Basis Function (RBF)**</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h9Um50WhhBpn"
   },
   "source": [
    "<font face=\"Times New Roman\" size=3>\n",
    "Another widely used kernel is RBF (Gaussian Radial Basis Function), which is described in the Theory Notebook. In the following, first, for better intuition, we draw this kernel on a data set, then we train two SVMs on two different data sets using this kernel and observe the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XjRZm_smMaVe"
   },
   "source": [
    "#### <font color=\"#008080\" face=\"Times New Roman\" size=4>**Kernel Visualization**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DTp7eB-ChSqv"
   },
   "source": [
    "<font face=\"Times New Roman\" size=3>\n",
    "The hyperparameter $\\gamma$ determines the value of the variance of this function. The lower this parameter is, the greater the variance and relatively more weight will be given to distant samples, and vice versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CRrF_IpNj6M_"
   },
   "outputs": [],
   "source": [
    "x1 = np.linspace(-3, 3, 100)\n",
    "x2 = np.linspace(-3, 3, 100)\n",
    "X1, X2 = np.meshgrid(x1, x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LFWF66wJjjuD"
   },
   "outputs": [],
   "source": [
    "def plot_RBF_kernel(gamma):\n",
    "  R = np.exp(-gamma*(X1 ** 2 + X2 ** 2))\n",
    "\n",
    "  fig = plt.figure(figsize=(10,8))\n",
    "  ax = fig.add_subplot(111, projection='3d')\n",
    "  surf = ax.plot_surface(X1, X2, R, cmap=plt.cm.coolwarm)\n",
    "  plt.title('RBF kernel with $\\gamma$ = {:.1f}'.format(gamma), fontsize=15)\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 679
    },
    "id": "l6kd5jOJx93p",
    "outputId": "8816d363-5f6b-42b4-8292-be658dc01286"
   },
   "outputs": [],
   "source": [
    "plot_RBF_kernel(gamma = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 679
    },
    "id": "QpMtu1AhkAAF",
    "outputId": "164b39d6-457d-4b01-fc00-44f478e4585d"
   },
   "outputs": [],
   "source": [
    "plot_RBF_kernel(gamma = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GjQTvUPCNGiw"
   },
   "source": [
    "#### <font color=\"#008080\" face=\"Times New Roman\" size=4>**Dataset 1**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o4WgMi8bkRC_"
   },
   "source": [
    "<font face=\"Times New Roman\" size=3>\n",
    "Consider the <code>make_circles</code> dataset again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uFwk2RXrNRgj"
   },
   "outputs": [],
   "source": [
    "X, y = datasets.make_circles(n_samples=200, factor=0.1, noise=0.1, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FnHgYu6jNcNn"
   },
   "source": [
    "#### <font color=\"#008080\" face=\"Times New Roman\" size=4>**Model**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GeHp-9LWk3jI"
   },
   "source": [
    "<font face=\"Times New Roman\" size=3>\n",
    "If we choose RBF function as a kernel (by setting <code>kernel=\"rbf\"</code>), we can see our decision boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JhylSmm9Nf28"
   },
   "outputs": [],
   "source": [
    "clf = SVC(kernel='rbf', gamma=0.7, C=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R-03l6T0knw_"
   },
   "source": [
    "#### <font color=\"#008080\" face=\"Times New Roman\" size=4>**Train**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 74
    },
    "id": "bocpiZvxkrgT",
    "outputId": "ac773ed1-8db2-4751-db36-f1885238f285"
   },
   "outputs": [],
   "source": [
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UuhpDpAfNhYJ"
   },
   "source": [
    "#### <font color=\"#008080\" face=\"Times New Roman\" size=4>**Visualization**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 724
    },
    "id": "987HHr3OLmLT",
    "outputId": "e0126796-a952-48ae-c665-0e261f80d676"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "scatter = plt.scatter(X[:, 0], X[:, 1], c=y, s=50, edgecolors='black', cmap='coolwarm', alpha=0.6)\n",
    "plot_svc_decision_function(clf)\n",
    "plt.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1], s=200, edgecolors='k', facecolors='none')\n",
    "plt.legend(handles=scatter.legend_elements()[0], labels=['Class 1','Class 0'], fontsize='x-large', markerscale=1.5)\n",
    "plt.xlabel(\"$x_1$\", fontsize=15)\n",
    "plt.ylabel(\"$x_2$\", fontsize=15)\n",
    "plt.title(\"Decision Boundary using RBF Kernel\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sb5j_Pm8NrmT"
   },
   "source": [
    "#### <font color=\"#008080\" face=\"Times New Roman\" size=4>**Dataset 2**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7M7SouKOlMAl"
   },
   "source": [
    "<font face=\"Times New Roman\" size=3>\n",
    "For the make_moons dataset, we will also have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sdtzHeVkNuKi"
   },
   "outputs": [],
   "source": [
    "X, y = datasets.make_moons(n_samples=200,noise=0.15, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0fK-03_HNula"
   },
   "source": [
    "#### <font color=\"#008080\" face=\"Times New Roman\" size=4>**Model**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UBuYNv4-NzCm"
   },
   "outputs": [],
   "source": [
    "clf = SVC(kernel='rbf', gamma=0.8, C=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "isCzIQwelU5m"
   },
   "source": [
    "#### <font color=\"#008080\" face=\"Times New Roman\" size=4>**Train**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 74
    },
    "id": "3nBNoR1slZxJ",
    "outputId": "6f1a7615-baf3-48af-971d-2ac7b474f12d"
   },
   "outputs": [],
   "source": [
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sq_TTicpN0Rz"
   },
   "source": [
    "#### <font color=\"#008080\" face=\"Times New Roman\" size=4>**Visualization**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 642
    },
    "id": "EdmIBzcyLoBT",
    "outputId": "bf74bbea-6664-4c93-8220-4c0ada66b0b8"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,7))\n",
    "plot_decision_regions(X, y, clf=clf, legend=1)\n",
    "plt.xlabel(\"$x_1$\")\n",
    "plt.ylabel(\"$x_2$\")\n",
    "plt.title(\"Decision Region of RBF Kernel\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y1jyp1Mcl0Lz"
   },
   "source": [
    "<font face=\"Times New Roman\" size=3>\n",
    "As you can see, by using the RBF kernel, the boundary of two classes of data is obtained much better, and this kernel usually has a better performance. Also, by default, the SVC model kernel is set on this kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1TgrioWuqBHB"
   },
   "source": [
    "<font face=\"Times New Roman\"><div id=\"1-4\">\n",
    "## <font color=\"#800080\" size=6>**1-4. Multi-Class SVM**</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MBYHBiq1plNn"
   },
   "source": [
    "<font face=\"Times New Roman\" size=3>\n",
    "Using the scikit-learn library, we will try the One-vs-One method on the following dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DXDtIWVI8y1_"
   },
   "source": [
    "#### <font color=\"#008080\" face=\"Times New Roman\" size=4>**Dataset**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DE3-Zg6i9L-E"
   },
   "source": [
    "##### <font color=\"#008080\" face=\"Times New Roman\" size=4> - Loading Dataset </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8WOdD8wi8y2B"
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "X, y = datasets.make_blobs(\n",
    "    n_samples=300, n_features=2, centers=3, cluster_std=1.5, random_state=10\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FE70nOX58y2C"
   },
   "source": [
    "##### <font color=\"#008080\" face=\"Times New Roman\" size=4> - Visualization </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 565
    },
    "id": "fmoX4Tf5qiIp",
    "outputId": "90e2bc9a-3437-4b14-f270-fe6246798111"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "scatter = plt.scatter(X[:, 0], X[:, 1], marker='o', c=y, s=100, cmap='coolwarm', alpha=0.75);\n",
    "plt.xlabel(\"$x_1$\")\n",
    "plt.ylabel(\"$x_2$\")\n",
    "plt.title(\"Scatter Plot of the Dataset\")\n",
    "plt.legend(handles=scatter.legend_elements()[0], labels=['Class 0','Class 1', 'Class 2'], fontsize='x-large', markerscale=2)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fnaXSgAZ-RoF"
   },
   "source": [
    "#### <font color=\"#008080\" face=\"Times New Roman\" size=4>**Model**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6wO4h7xtp3Gb"
   },
   "source": [
    "<font face=\"Times New Roman\" size=3>\n",
    "To change the classification type, just set the <code>decision_function_shape</code> hyperparameter to <code>ovo</code> (one-vs-one) or <code>ovr</code> (one-vs-rest). We use three kernels, linear, polynomial and RBF, and define three models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "61v3vcgf-RoG"
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "#Linear Kernel\n",
    "clf_linear = SVC(kernel='linear', decision_function_shape='ovo')\n",
    "\n",
    "#RBF Kernel\n",
    "clf_rbf = SVC(kernel='rbf', gamma=0.8, C=5, decision_function_shape='ovo')\n",
    "\n",
    "#Polynomial Kernel\n",
    "clf_poly = SVC(kernel='poly', degree=3, decision_function_shape='ovo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OuleUT7a-aLd"
   },
   "source": [
    "#### <font color=\"#008080\" face=\"Times New Roman\" size=4>**Train**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 74
    },
    "id": "Uaw8IViW-aLd",
    "outputId": "f04419f0-b510-41cd-9cb8-e200c27cb28c"
   },
   "outputs": [],
   "source": [
    "clf_linear.fit(X, y)\n",
    "clf_rbf.fit(X, y)\n",
    "clf_poly.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v3vX2AMI-jCL"
   },
   "source": [
    "#### <font color=\"#008080\" face=\"Times New Roman\" size=4>**Visualization**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 624
    },
    "id": "xQ8RwAJg-Nlp",
    "outputId": "cd5c6c7d-1ece-4249-f5d0-a9c06cf5faf3"
   },
   "outputs": [],
   "source": [
    "from mlxtend.plotting import plot_decision_regions\n",
    "\n",
    "plt.figure(figsize=(20,8))\n",
    "\n",
    "#Linear Kernel\n",
    "plt.subplot(1,3,1)\n",
    "plot_decision_regions(X, y, clf=clf_linear, legend=1)\n",
    "plt.xlabel(\"$x_1$\")\n",
    "plt.ylabel(\"$x_2$\")\n",
    "plt.title(\"Decision Region of Linear Kernel\")\n",
    "\n",
    "#Polynomial Kernel\n",
    "plt.subplot(1,3,3)\n",
    "plot_decision_regions(X, y, clf=clf_poly, legend=1)\n",
    "plt.xlabel(\"$x_1$\")\n",
    "plt.ylabel(\"$x_2$\")\n",
    "plt.title(\"Decision Region of Polynomial Kernel with degree=3\")\n",
    "\n",
    "#RBF Kernel\n",
    "plt.subplot(1,3,2)\n",
    "plot_decision_regions(X, y, clf=clf_rbf, legend=1)\n",
    "plt.xlabel(\"$x_1$\")\n",
    "plt.ylabel(\"$x_2$\")\n",
    "plt.title(\"Decision Region of RBF Kernel\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NB7H40DRqdBB"
   },
   "source": [
    "<font face=\"Times New Roman\"><div id=\"2\">\n",
    "# <font color=\"#800080\" size=6>**2. Support Vector Regression (SVR)**</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8P83uBer6bbR"
   },
   "source": [
    "<font face=\"Times New Roman\" size=3>\n",
    "The Margin idea can also be used for regression problems and thus Support Vector Regression (SVR) is defined. As in the case of classification, kernels can be used in the regression problem in a similar way, and rbf and polynomial kernels are also defined for. In the following, we will try to compare the effect of these kernels using <code>sklearn.svm</code> on a one-dimensional dummy dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0VlisYKK1BZ_"
   },
   "source": [
    "#### <font color=\"#008080\" face=\"Times New Roman\" size=4>**Dataset**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5IGZwfLK1cOi"
   },
   "source": [
    "##### <font color=\"#008080\" face=\"Times New Roman\" size=4> - Creating Dataset </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xhf6UWcu1B3h"
   },
   "outputs": [],
   "source": [
    "X = np.sort(5 * np.random.rand(50, 1), axis=0)\n",
    "y = np.sin(X).ravel() #y = sin(x)\n",
    "\n",
    "#Add noise to targets\n",
    "y[::5] += 3 * (0.5 - np.random.rand(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O_DwmSem1lzN"
   },
   "source": [
    "##### <font color=\"#008080\" face=\"Times New Roman\" size=4> - Visualization </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 449
    },
    "id": "iYH98IJ61ZE0",
    "outputId": "0e06be7e-f119-4870-d480-ed99749c6a18"
   },
   "outputs": [],
   "source": [
    "plt.scatter(X, y, color='darkorange', label='data')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lDvUSGJx15Dj"
   },
   "source": [
    "#### <font color=\"#008080\" face=\"Times New Roman\" size=4>**Model**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jWgBn7sIqgoh"
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "\n",
    "# Set the epsilon for the width of streets!\n",
    "eps=0.2\n",
    "\n",
    "# Linear Model\n",
    "svr_lin = SVR(kernel='linear', C=1e3, epsilon=eps)\n",
    "\n",
    "# RBF Model\n",
    "svr_rbf = SVR(kernel=\"rbf\", C=100, gamma=0.1, epsilon=eps)\n",
    "\n",
    "# Polynomial Model\n",
    "svr_poly = SVR(kernel=\"poly\", C=100, degree=3, epsilon=eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PHHocSy32QI1"
   },
   "source": [
    "#### <font color=\"#008080\" face=\"Times New Roman\" size=4>**Train**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rRe7JC062b7d"
   },
   "outputs": [],
   "source": [
    "# Linear Model\n",
    "y_lin = svr_lin.fit(X, y).predict(X)\n",
    "\n",
    "# RBF Model\n",
    "y_rbf = svr_rbf.fit(X, y).predict(X)\n",
    "\n",
    "# Polynomial Model\n",
    "y_poly = svr_poly.fit(X, y).predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q7t6YQtj2Kgg"
   },
   "source": [
    "#### <font color=\"#008080\" face=\"Times New Roman\" size=4>**Visualization**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 410
    },
    "id": "_EX4Vnln2XQS",
    "outputId": "4c631204-f8cd-4f52-e0f4-f97d3c2cec3d"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1,3,1)\n",
    "plt.scatter(X, y, color='darkorange', label='data')\n",
    "plt.plot(X, y_lin, color='k', lw=2)\n",
    "plt.title('Linear Model')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.fill_between(X.ravel(), y_lin - eps, y_lin + eps, edgecolor='none', color='#7D3C4A', alpha=0.3)\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "plt.scatter(X, y, color='darkorange')\n",
    "plt.plot(X, y_rbf, color='k', lw=2)\n",
    "plt.title('RBF Model')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.fill_between(X.ravel(), y_rbf - eps, y_rbf + eps, edgecolor='none', color='#7D3C4A', alpha=0.3)\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "plt.scatter(X, y, color='darkorange', label='data')\n",
    "plt.plot(X, y_poly, color='k', lw=2)\n",
    "plt.title('Polynomial Model with degree=3')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.fill_between(X.ravel(), y_poly - eps, y_poly + eps, edgecolor='none', color='#7D3C4A', alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L5c9JzN57wLK"
   },
   "source": [
    "<font face=\"Times New Roman\" size=3>\n",
    "As it is clear, by using RBF kernel, we have higher accuracy in fitting. We should also be able to obtain the appropriate value of hyperparameters of each model, which is done using Cross Validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8vTWwXrjgM3j"
   },
   "source": [
    "<font face=\"Times New Roman\"><div id=\"2\">\n",
    "# <font color=\"#008080\" size=6>**References**</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mOoZ1o_BgQSQ"
   },
   "source": [
    "\n",
    "- GeÌron, A. (2019). Hands-on machine learning with Scikit-Learn, Keras and TensorFlow: concepts, tools, and techniques to build intelligent systems (2nd ed.). O’Reilly.\n",
    "\n",
    "- https://github.com/asharifiz/Introduction_to_Machine_Learning/blob/main/Slides/Chapter_02_Classical_Models/Clustering/section%202-3.pdf\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "sBzocXtIiTbu",
    "rYaiPL1plrDm",
    "swwrxuSMAESj",
    "Ju9aCx5yAESj",
    "gYuiA7fFAESk",
    "helJaWTOAESk",
    "YhLhSsSfYKbR",
    "0BrjWqO-AESk",
    "iDTgWkM5AESi",
    "E5LZv8WxAESl",
    "gSvKw0pdAESm",
    "DCicBNv3AESm",
    "n2ViYLYjAESn",
    "6fweiFAXAESn",
    "rpEZrgzGAESn",
    "68iIHGEgAl1w",
    "14oWGLnbAl1y",
    "osVXNTRxAl1y",
    "UNVQE4NhAl1z",
    "SULBwRA6Al10",
    "OZyxj4cjnzW_",
    "0dkxBfoQgySR",
    "09ucf-Qlbn3r",
    "jfNUdSo-anuc",
    "NxCQr48PbZLB",
    "j4W1zcVSd9ag",
    "KjPdsXy0bdYt",
    "QF0FPJzbeLc2",
    "6HiZb9tge2J1",
    "A02dqB4zg6mi",
    "eVbZVyBbDA_0",
    "8Iel5jWADA_0",
    "Jsg8bozFDA_1",
    "aeSGaeJfod5f",
    "n7BTS7r2o2j0",
    "Q1cHt7zmpA4v",
    "3pZCWtkeo2Q_",
    "BG6JDKXAKthv",
    "I3TA7qSXJJug",
    "vORapzaoJJug",
    "2JNOCPY3JJuh",
    "3bDvO3dZJXLK",
    "DdttC2gIJhFv",
    "d1zcyxjIJnxg",
    "PcxB-PgbJ1FT",
    "Ftuapgm7J1FT",
    "ohDlrArFJ1FU",
    "TOn8ae25J-Px",
    "RnJ3PbxLKkyu",
    "JGusVztLLKHA",
    "D2hHo3jxLKHA",
    "GYdgEkdDLKHB",
    "L-YMqtB5LKHB",
    "B-nGWBOrIrDe",
    "1kwiWMyhL0Cw",
    "EcUj0p-zL0Cw",
    "ZpIDH-M4L0Cx",
    "950JsRNjMAny",
    "iOPf8PsJMHZ4",
    "a6xyh9qQMNUh",
    "HBT59RLpIyUY",
    "XjRZm_smMaVe",
    "GjQTvUPCNGiw",
    "FnHgYu6jNcNn",
    "R-03l6T0knw_",
    "UuhpDpAfNhYJ",
    "sb5j_Pm8NrmT",
    "0fK-03_HNula",
    "isCzIQwelU5m",
    "sq_TTicpN0Rz",
    "1TgrioWuqBHB",
    "DXDtIWVI8y1_",
    "DE3-Zg6i9L-E",
    "FE70nOX58y2C",
    "fnaXSgAZ-RoF",
    "OuleUT7a-aLd",
    "v3vX2AMI-jCL",
    "NB7H40DRqdBB",
    "0VlisYKK1BZ_",
    "5IGZwfLK1cOi",
    "O_DwmSem1lzN",
    "lDvUSGJx15Dj",
    "PHHocSy32QI1",
    "Q7t6YQtj2Kgg",
    "8vTWwXrjgM3j"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

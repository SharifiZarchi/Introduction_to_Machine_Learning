{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f6a97ad",
   "metadata": {
    "id": "5f6a97ad"
   },
   "source": [
    "<font face=\"XB Zar\" size=5><div dir=rtl align=center>\n",
    "<font face=\"XB Zar\" size=5>\n",
    "به نام خدا\n",
    "</font>\n",
    "<br> <br>\n",
    "<font size=3>\n",
    "دانشگاه صنعتی شریف - دانشکده مهندسی کامپیوتر\n",
    "</font>\n",
    "<br> <br>\n",
    "<font color=blue size=5>\n",
    "مقدمه‌ای بر یادگیری ماشین\n",
    "</font>\n",
    "\n",
    "<hr/> <br>\n",
    "<font color=red size=6>\n",
    "فصل سوم: یادگیری، ارزیابی و تنظیم‎کردن مدل‎ها \n",
    "<br>\n",
    "</font>\n",
    "<br>\n",
    "نویسندگان: <br> \n",
    "<br>علیرضا گرگوری مطلق، پیمان ناصری، علیرضا حیدری\n",
    "<hr>\n",
    "</div></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9430b8bb",
   "metadata": {
    "id": "9430b8bb"
   },
   "source": [
    "<font face=\"XB Zar\" size=4><div dir=rtl>\n",
    "در این فصل قصد داریم که به معرفی مفاهیم پایه‌ای و البته مهمی در یادگیری ماشین بپردازیم. تمرکز ما برای معرفی مفاهیم مرتبط، مدل **ماشین‌های بردار پشتیبان** \n",
    "(Support Vector Machines)\n",
    "می باشد؛ در ابتدا به پیاده‌سازی مدل ساده‌شده‌ای از SVM پرداخته و سپس به معرفی کتابخانه\n",
    "Scikit-Learn\n",
    "که یکی از مهمترین و قدرتمندترین کتابخانه‌های موجود در زمینه یادگیری ماشین است میپردازیم.\n",
    "در ادامه با استفاده از ماژول‌های این کتابخانه مدل‌های پیچیده‌تری را پیاده‌سازی خواهیم کرد و نحوه ارزیابی و انتخاب مدل‌های مناسب را بر اساس معیارهای مدنظرمان فرا خواهیم گرفت.\n",
    "<br> <br>\n",
    "پیش از شروع بحث بهتر است به معرفی ابرصفحه (Hyperplane) و نیز حاشیه (Margin) بپردازیم.\n",
    "<br><b> (در سراسر این فصل فرض می‌شود که $N$ تعداد نمونه‌های ما و $p$ تعداد ویژگی‌های هر نمونه می‌باشد.) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfbe84f3",
   "metadata": {
    "id": "dfbe84f3"
   },
   "source": [
    "<font face=\"XB Zar\" size=4><div dir=rtl>\n",
    "<font color=\"red\" size=5>ابرصفحه</font>\n",
    "<br> <br>\n",
    "یک ابرصفحه در فضای *p*بعدی در واقع زیرفضایی affine از بعد *p-1* می باشد.\n",
    "معادله زیر بیان کننده یک ابرصفحه در فضای *p*بعدی میباشد:  \n",
    "<br><center> $f(X) = b + w_1X_1 + w_2X_2 + ... + w_pX_p = w^T X + b = 0 $\n",
    "</div></font>\n",
    "    \n",
    "<font face=\"XB Zar\" size=4><div dir=rtl>\n",
    "<br>در معادله بالا بردار $w = (w_1, w_2, ..., w_p)^T $ بردار نرمال بر ابرصفحه مذکور است و در واقع نشانگر جهتی میباشد که بر ابرصفحه عمود است. به این بردار، بردار وزن نیز می‌گویند که در ادامه دلیل آن مشخص خواهد شد.\n",
    "    همچنین b نیز عرض‌ازمبدا(intercept) ابرصفحه می‌باشد که از آن با نام بایاس نیز یاد می‌شود.\n",
    "</div></font>\n",
    "    \n",
    "    \n",
    "<div>\n",
    "<center>\n",
    "<img src=\"resources/hyperplane.png\" width=\"800\">\n",
    "</center>\n",
    "</div>\n",
    "\n",
    "    \n",
    "<font face=\"XB Zar\" size=4><div dir=rtl>\n",
    "<br> همانطور که مشاهده میکنیم، در فضای 2 بعدی ابرصفحه خط راستی با شیب $w_1$ و عرض از مبدا $b$ می باشد و در فضای 3 بعدی نیز، یک ابرصفحه در واقع همان صفحه است. در ابعاد بالاتر نیز با تعمیم این صفحه، دارای ابرصفحه‌ای خواهیم بود که بردار $w$ بر آن عمود می باشد و فضای\n",
    "*$p$*-بعدی را به 2 نیم فضا تقسیم میکند؛ به گونه‌ای که در یک سمت این ابرصفحه، \n",
    "$f(X) > 0$ است و در سمت دیگر آن $f(X) < 0$ خواهد بود.\n",
    "</div></font>\n",
    "\n",
    "    \n",
    "<font face=\"XB Zar\" size=4><div dir=rtl>\n",
    "<br> حال مسئله‌ای را در نظر بگیرید که میخواهیم داده‌های دو کلاس را از یکدیگر تفکیک کنیم. در صورتی که بتوانیم ابرصفحه مناسبی را پیدا کنیم که داده‌های هر کلاس در یک طرف این ابرصفحه قرار بگیرند، و برچسب کلاس‌های هر گروه را به صورت $Y^{(i)} \\in \\{\\pm 1\\}$ کدگذاری کنیم، برای هر نمونه معادله زیر برقرار خواهد بود:\n",
    "<br> <center> $ Y^{(i)}f(X^{(i)}) > 0 \\quad \\forall i \\in \\{1,2,...,N\\} $\n",
    "</div></font>\n",
    "    \n",
    "<font face=\"XB Zar\" size=4><div dir=rtl>\n",
    "<br>در شکل زیر نمونه‌ای از یک صفحه جداکننده دو کلاس در فضای 2بعدی نمایش داده شده است:\n",
    "</div></font>\n",
    "    \n",
    "<div>\n",
    "<center>\n",
    "<br>\n",
    "<img src=\"resources/possible_hyperplane.png\" width=\"400\">\n",
    "</center>\n",
    "</div>\n",
    "    \n",
    "    \n",
    "<font face=\"XB Zar\" size=4><div dir=rtl>\n",
    "<font color=\"red\" size=5>حاشیه (Margin)</font>\n",
    "<br>\n",
    "<br> یک راه‌حل معقول برای جداسازی داده‌های دو کلاس، پیدا کردن ابرصفحه‌ای است که بیشترین حاشیه (Margin) را نسبت به داده‌های دو کلاس داشته باشد؛\n",
    "    منظور از حاشیه فاصله این ابرصفحه از نزدیک‌ترین داده هر کلاس است:\n",
    "</div></font>\n",
    "\n",
    "<div>\n",
    "<center>\n",
    "<br>\n",
    "<img src=\"resources/possible_hyperplane_margin.png\" width=\"400\">\n",
    "</center>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8dc937",
   "metadata": {
    "id": "fa8dc937"
   },
   "source": [
    "<font face=\"XB Zar\" size=4><div dir=rtl>\n",
    "<center><font color=\"red\" size=6>طبقه‌بند بیشینه‌نمای حاشیه (Maximal Margin Classifier)</font></center> <br>\n",
    "<br>از آنجا که هر ابرصفحه جداکننده‌ای را می‌توان با انتقال یا چرخش کوچکی به ابرصفحه دیگری تبدیل نمود که همچنان تمایزدهنده دو کلاس است، بی‌نهایت ابرصفحه میتواند جواب مسئله ما باشد؛ پس باید بتوانیم ابرصفحه‌ای که به طور متقارن از هر دو کلاس بیشینه فاصله را دارد، پیدا نماییم؛ به این ابرصفحه، **ابرصفحه جداکننده بهینه** \n",
    "(Optimal Separating Hyperplane)\n",
    "یا **ابرصفحه جداکننده بیشینه**\n",
    "(Maximal Separating Hyperplane)\n",
    "میگوییم.\n",
    "</div>\n",
    "\n",
    "<div>\n",
    "<center>\n",
    "<br>\n",
    "<img src=\"resources/possible_hyperplanes.png\" width=\"400\">\n",
    "</center>\n",
    "</div>\n",
    "\n",
    "<font face=\"XB Zar\" size=4><div dir=rtl>\n",
    "<br> <br>\n",
    "بنابراین مسئله ما پیدا کردن ابرصفحه‌ای شده است که حداکثر حاشیه ممکن از دو کلاس را داشته باشد؛ می‌توان به این مسئله به چشم پیدا کردن دو ابرصفحه دیگر \n",
    "$(\\mathcal{H}_1, \\mathcal{H}_2)$ \n",
    "که فاصله برابر و بیشینه‌ای از ابرصفحه مطلوب ما\n",
    "$\\mathcal{H}_0$\n",
    "دارند نیز نگاه کرد؛ به طوری که داده‌های دو کلاس در دو سمت متفاوت \n",
    "$\\mathcal{H}_0$\n",
    "قرار بگیرند و هیچ نمونه‌ای نیز در فضای بین آن‌ها نیفتد.\n",
    "شکل زیر بیانگر مطالب بالا می‌باشد:\n",
    "</div>\n",
    "\n",
    "<div>\n",
    "<center>\n",
    "<br>\n",
    "<img src=\"resources/extermum_hyperplanes.png\" width=\"400\">\n",
    "</center>\n",
    "</div>\n",
    "\n",
    "\n",
    "<div dir=rtl>\n",
    "<br>بدون از دست دادن کلیت مسئله، می‌توان دو صفحه \n",
    "$(\\mathcal{H}_1, \\mathcal{H}_2)$ \n",
    "را به صورت زیر تعریف نمود:  \n",
    "<br><center>  $\\mathcal{H}_1: \\quad w^T x^{(i)} + b \\geq 1 \\qquad if \\quad y^{(i)}=1 $\n",
    "<br><center>  $\\mathcal{H}_2: \\quad w^T x^{(i)} + b \\leq -1 \\qquad if \\quad y^{(i)}=-1 $\n",
    "</div>\n",
    "    \n",
    "<font face=\"XB Zar\" size=4><div dir=rtl>\n",
    "<br>دو معادله بالا را می‌توان با یکدگیر ترکیب نمود و به صورت زیر نوشت:\n",
    "<br><center> $y^{(i)}(w^T x^{(i)} + b) \\geq 1 \\quad \\forall i \\in \\{1,2,...,N\\} $\n",
    "</div></font>\n",
    "    \n",
    "    \n",
    "<div dir=rtl>\n",
    "<br>حال باید فاصله بین دو صفحه\n",
    "$(\\mathcal{H}_1, \\mathcal{H}_2)$ \n",
    "را پیدا کنیم تا بتوانیم با بیشینه کردن آن، صفحه\n",
    "$\\mathcal{H}_0$\n",
    "را به صورت ابرصفحه‌ای موازی با آنها که این فاصله را به دو قسمت برابر تقسیم میکند، مشخص نماییم.\n",
    "<br> <br>\n",
    "یک راه توصیف حاشیه می‌تواند اینگونه باشد که حاشیه را برداری عمود بر دو ابرصفحه مذکور در نظر گرفت که اندازه این بردار برابر با مقدار حاشیه خواهد بود. پس با تعریف بردار مناسبی برای توصیف این فاصله، کافی است اندازه آن را پیدا کنیم.\n",
    "<br>از آنجا که بردار وزن‌های\n",
    "$w$\n",
    " عمود بر ابرصفحه موردنظر ما است، می توان بردار یکه $u$ را برداری هم‌جهت با بردار $w$ در نظر گرفت؛\n",
    "پس بردار $u$ به صورت زیر می‌باشد:\n",
    "<br> <center> <font size=5> $ u = \\frac{w}{{\\lVert w \\rVert}_2} $\n",
    "</div>\n",
    "    \n",
    "<div dir=rtl>\n",
    "<br>حال کافی است بردار $u$ را در مقدار اسکالر $m$ ضرب نمود تا \n",
    "بردار $k$ را پیدا کرد که اندازه آن برابر با حاشیه $m$ می‌باشد و در راستای موردنظر نیز خواهد بود:\n",
    "<br> <center> <font size=5> $ k = m.u = m.\\frac{w}{{\\lVert w \\rVert}_2} $\n",
    "</div>\n",
    "\n",
    "<div dir=rtl>\n",
    "شکل زیر توضیحات بالا را به خوبی ترسیم می‌نماید:\n",
    "<center>\n",
    "<img src=\"resources/scalers.png\" width=\"400\">\n",
    "</center>\n",
    "</div>\n",
    "    \n",
    "    \n",
    "<div dir=rtl>\n",
    "حال نقطه‌ی دلخواه $x_0$\n",
    "را بر روی ابرصفحه\n",
    "$\\mathcal{H}_2$   \n",
    "در نظر بگیرید و آن را با بردار $k$ جمع کنید تا به نقطه $z_0$ بر روی ابرصفحه \n",
    "$\\mathcal{H}_1$ برسید.\n",
    "<center>\n",
    "<img src=\"resources/find_k.png\" width=\"400\">\n",
    "</center>\n",
    "</div>\n",
    "    \n",
    "<div dir=rtl>\n",
    "از آنجا که نقطه $x_0$ بر روی $\\mathcal{H}_2$ و $z_0$ بر روی $\\mathcal{H}_1$ قرار گرفته‌اند، پس در معادلات زیر صدق می‌کنند:\n",
    "<br> <center> $w^T z_0 + b = 1 $\n",
    "<br> <center> $w^T x_0 + b = -1 $\n",
    "</div>\n",
    "    \n",
    "<div dir=rtl> <br>\n",
    "با کم کردن دو معادله بالا از یکدیگر خواهیم داشت:\n",
    "<br> <center> $w^T (z_0 - x_0) = 2$\n",
    "</div>\n",
    "\n",
    "<div dir=rtl> <br>\n",
    "از طرفی $z_0 = x_0 + k$ می‌باشد؛ پس\n",
    "<br> <center> $w^T k = w^T . m\\frac{w}{{\\lVert w \\rVert}_2} = 2 \\Rightarrow $\n",
    "<br><br> <center> $ m.\\frac{{\\lVert w \\rVert}_2 ^2}{{\\lVert w \\rVert}_2} = m.{\\lVert w \\rVert}_2 = 2 \\Rightarrow$\n",
    "<br><br> <center> <font size=\"5\"> $ m = \\frac{2}{{\\lVert w \\rVert}_2} $ <br>\n",
    "</div>  \n",
    "    \n",
    "    \n",
    "<div dir=rtl> <br>\n",
    "پس با توجه به نتایج بالا، فاصله بین دو صفحه\n",
    "$(\\mathcal{H}_1, \\mathcal{H}_2)$ \n",
    "برابر با\n",
    "$ m = \\frac{2}{{\\lVert w \\rVert}_2} $\n",
    "می‌باشد و جهت بیشینه کردن این مقدار، باید مخرج آن، یعنی اندازه بردار نرمال را کمینه نمود.\n",
    "به نمونه‌هایی که بر روی دو صفحه\n",
    "$(\\mathcal{H}_1, \\mathcal{H}_2)$ \n",
    "قرار میگیرند و در تعیین نمودن ابرصفحه جداکننده بهینه نقش دارند، بردارهای پشتیبان (support vectors) گفته می‌شود. شکل زیر به طور نمادین تمام نتایج بالا را دارا میباشد:\n",
    "</div>\n",
    "    \n",
    "<div> <br>\n",
    "<center>\n",
    "<img src=\"resources/support_vectors.png\" width=\"400\">\n",
    "</center>\n",
    "</div>\n",
    "    \n",
    "\n",
    "<div dir=rtl> <br>\n",
    "<br> با جمع بندی نتایج و توضیحات بالا، ما به‌دنبال حل کردن مسئله بهینه‎سازی زیر می‌باشیم:\n",
    "\\begin{equation}\n",
    "    \\begin{aligned}\n",
    "        \\min_{w,b} \\quad & \\frac{1}{2}||w||_2 ^2\\\\\n",
    "        \\textrm{s.t.} \\quad & y^{(i)}(w^T x^{(i)} + b) \\geq 1 \\quad \\forall i \\in \\{1,2,...,N\\}\\\\\n",
    "    \\end{aligned}\n",
    "\\end{equation}\n",
    "    \n",
    "<br><div dir=rtl> (همانطور که می‌دانید کمینه کردن اندازه یک بردار با کمینه کردن توان دو اندازه آن بردار، معادل می‌باشد و برای سادگی محاسبات در ادامه از این نکته استفاده کرده‌ایم!) <br>\n",
    "(همچنین استفاده از ضریب $\\frac{1}{2}$ نیز جهت تسهیل نتایج مشتق گرفتن در ادامه می‌باشد!)\n",
    "</div>\n",
    "    \n",
    "    \n",
    "    \n",
    "<div dir=rtl> <br> مسئله مقید بالا یک مسئله بهینه‌سازی محدب می‌باشد و در ادبیات بهینه‌سازی محدب از آن به عنوان برنامه‌نویسی مرتبه دوم \n",
    "(Quadratic Programming)\n",
    "یاد می‌شود. در صورت علاقمندی به مطالعه بیشتر راجع‌به Quadratic Programming می‌توانید به لینک زیر مراجعه نمایید:\n",
    "<br> <div dir=ltr> <a href=\"https://en.wikipedia.org/wiki/Quadratic_programming\">Quadratic Programming (QP)</a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65a74c4",
   "metadata": {
    "id": "b65a74c4"
   },
   "source": [
    "<font face=\"XB Zar\" size=4><div dir=rtl>\n",
    "    \n",
    "<div dir=rtl> <br>\n",
    "با توجه به محدب بودن مسئله بالا، وجود جوابی جهت کمینه کردن تابع هدف تضمین می‌شود؛ پس می‌توان با معرفی تابع هزینه مناسبی برای مسئله بالا، به طوری که عبارت را از حالت مقید خارج سازد، آن را با روش‌های مرسوم بهینه‌سازی همچون Gradient Descent .حل نماییم\n",
    "</div> <br>\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "<div dir=rtl> <br>    \n",
    "<font color=\"red\" size=5>معرفی Gradient Descent</font> <br>\n",
    "در این بخش به طور خلاصه به روش Gradient Descent\n",
    "که الگوریتمی از خانواده الگوریتم‌های مرتبه اول در بهینه سازی است و جهت پیداکردن کمینه محلی (Local Minimum)\n",
    "    و در توابع محدب کمینه جهانی (Global Minimum)\n",
    "به کار می‌رود، اشاره می‌کنیم. در فصل‌های بعد به طور کامل‌تر با این روش آشنا خواهید شد. <br>\n",
    "این روش ساده از مرسوم‌ترین و پرطرفدارترین روش‌های بهینه‌سازی در یادگیری ماشین و خصوصا یادگیری عمیق می‌باشد.\n",
    "در واقع با استفاده از این روش ما سعی داریم که تابع هدف(یا در ادبیات یادگیری ماشین، تابع هزینه) خود را کمینه کنیم و پارامترهای بهینه را جهت این هدف پیدا کنیم. <br>\n",
    "همانطور که می‌دانیم، مشتق تابع در یک نقطه، نمایانگر شیب تابع در آن نقطه و جهتی است که تابع در راستای آن بیشترین افزایش مقدار را دارد؛ بنابراین با حرکت کردن در خلاف جهت مشتق تابع می‌توانیم کمینه مورد نظر را پیدا کنیم. این روش ساده و تعمیم‌های آن مبنای بهینه‌سازی بسیاری از مدل‌های یادگیری ماشین خواهند بود که در ادامه درس خواهید دید.\n",
    "</div>\n",
    "\n",
    "<div> <br>\n",
    "<center>\n",
    "<img src=\"resources/ball.png\" width=\"500\">\n",
    "</center>\n",
    "</div>\n",
    "    \n",
    "   \n",
    "<div dir=rtl> <br> \n",
    "فرض کنید هدف ما پیداکردن پارامترهایی است که تابع هزینه \n",
    "$J(w)$ را کمینه میکنند؛ یعنی:\n",
    "<br> <center> $w^{opt} = \\underset{w}{\\mathrm{argmin}}\\hspace{1mm} J(w)$ </div>\n",
    "    \n",
    "<div dir=rtl> <br>\n",
    "بنابراین روش ما برای پیداکردن پارامترهای بهینه به صورت زیر خواهد بود: <br>\n",
    "<br> 1. پارامترهای موردنظر را مقداردهی اولیه نماییم.(مقداردهی اولیه معمولا به صورت تصادفی خواهد بود، هرچند روش‌های خیلی پیشرفته‌تر و بهتری جهت مقداردهی اولیه وجود دارد.)\n",
    "<br><br> 2. مقدار و جهت آپدیت پارامترها را به کمک مشتق تابع به ازای آن پارامترها پیدا کنیم:\n",
    "<br><br> <font size=\"5\"> <center> $\\Delta w = -\\eta \\frac{\\partial J(w)}{\\partial w}$ <font> </div>\n",
    "<div dir=rtl> همانطور که از توضیحات بالا مشخص است، این روش یک روش تکرارشونده است و مقدار حرکت در هر گام را با استفاده از پارامتر $\\eta$ که از آن به نام ضریب یادگیری (Learning Rate) یاد می‌شود تنظیم میکنیم. \n",
    "<br><br> 3. پارامترهای موردنظر را به‌روز نماییم:\n",
    "<br><br> <font size=\"5\"> <center> $w^{new} = w^{old} + \\Delta w$ <font> </div>\n",
    "<div dir=rtl> <br>\n",
    "4. مراحل 2و3 را آنقدر تکرار میکنیم تا جایی‌که مشتق تابع هزینه در آن نقطه صفر شود و در نتیجه $w^{new} = w^{old}$. پارامترهای بهینه برابر با این مقدار $w$ می‌باشد. \n",
    "</div>\n",
    "    \n",
    "\n",
    "<div dir=rtl><br><br> از موارد بسیار مهم در این روش تنظیم مقدار ضریب یادگیری $\\eta$  می‌باشد؛\n",
    "همانطور که گفته شد، انداره حرکت ما در هر گام به سمت کمینه محلی را این ضریب کنترل می‌کند.\n",
    "در صورتی که مقدار این ضریب کوچک باشد، قدم‌های ما بسیار آرام خواهند بود و در نتیجه الگوریتم ما زمان بیشتری جهت همگراشدن نیاز دارد.\n",
    "همچنین در صورتی که مقدار این ضریب بسیار بزرگ باشد، الگوریتم ما در نزدیکی نقطه بهینه ممکن است دچار پرش به عقب و جلو شود و هیچوقت به مقدار بهینه نرسد؛ شکل زیر به خوبی این اثر را نمایش می‌دهد:\n",
    "</div>\n",
    "\n",
    "<div> <br>\n",
    "<center>\n",
    "<img src=\"resources/sgd_learning_rates.gif\" width=\"800\">\n",
    "</center>\n",
    "</div>\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3536595d",
   "metadata": {
    "id": "3536595d"
   },
   "source": [
    "<font face=\"XB Zar\" size=4><div dir=rtl> \n",
    "<font color=\"red\" size=5>Gradient Descent برای Maximal Margin Classifier</font> <br> <br>\n",
    "از آنجایی که اشاره شد مسئله بهینه‌سازی Maximal Margin Classifier یک مسئله محدب است،\n",
    "پس با تعریف تابع هزینه مناسبی برای آن و خارج کردن مسئله از حالت مقید، می‌توانیم با استفاده از روش Graident Descent، بردار $\\beta$ بهینه که باعث بدست‌آمدن ابرصفحه مناسب ما است را پیدا کنیم. <br>\n",
    "<br> تابع هزینه ما برای این مسئله به صورت زیر تعریف می‌شود: <br>\n",
    "<br> <center> <font size=\"5\"> $J(w, b) = \\frac{1}{2} {\\lVert w \\rVert}_2 ^2 + C \\sum_{i=1}^{N} \\max\\big(0, 1-y^{(i)}(w^T x^{(i)} + b) \\big)$ </div>\n",
    "\n",
    "    \n",
    "<div dir=rtl> <br>\n",
    "تا بدین‌جا بخشی از تابع هزینه بالا باید برای شما آشنا بنظر برسد؛ جمله اول تابع هزینه بالا درواقع نقش بیشینه‌کردن حاشیه را برای ما دارد که هدف ما نیز بوده است.\n",
    "جمله دوم اما، نقش قیدهای مسئله بهینه‌سازی ما را ایفا می‌کنند که تضمین‌گر برچسب‌زنی درست نمونه‌ها و جداشدن داده‌های دو کلاس توسط یک ابرصفحه مطلوب با مقدار حاشیه موردنظر است. به جمله دوم در عبارت بالا Hinge Loss نیز گفته می‌شود.\n",
    "<br> در واقع Hinge Loss  با جمع بستن بر روی تمام نمونه‌هایی که حداقل حاشیه موردنظر ما را رعایت نکرده‌اند، مقدار خطایی را به تابع هزینه ما اضافه میکند و در صورتی که نمونه‌ای به درستی برچسب زده شده باشد مقدار این خطا برابر با 0 خواهد بود. \n",
    "این خطا با استفاده از هایپرپارامتر $C$ نیز کنترل می‌شود که در بخش‌های بعد اثر آن را خواهید دید.\n",
    "پس توانستیم با استفاده از Hinge Loss مسئله را بهینه‌سازی خود را از حالت مقید خارج کنیم.\n",
    "<br><br> خبر خوب این است که تابع هزینه بالا جمع دو تابع محدب است و مجموع تعدادی تابع محدب، تابعی محدب خواهد بود؛ در نتیجه می‌توانیم با استفاده از روش Gradient Descent بردار $w$ و $b$ بهینه را پیدا کنیم که با $w^*, b^*$ آن‌ها را نمایش می‌دهیم:\n",
    "<br> <center> <font size=\"5\"> $w^*, b^* = \\underset{w, b}{\\mathrm{argmin}}\\hspace{1mm} J(w, b)$ </div>\n",
    "    \n",
    "    \n",
    "<div dir=rtl> <br>\n",
    "برای پیداکردن گرادیان تابع هزینه باید بین دو حالت تمایز قائل شویم: \n",
    "<br><br>\n",
    "\\begin{cases}\n",
    "    J_1 = \\frac{1}{2} {\\lVert w \\rVert}_2 ^2 \\hspace{28mm} if \\quad y^{(i)}(w^T x^{(i)} + b) \\geq 1 \\\\\n",
    "    \\\\\n",
    "    J_2 = \\frac{1}{2} {\\lVert w \\rVert}_2 ^2 + 1 - y^{(i)}(w^T x^{(i)} + b) \\qquad otherwise\n",
    "\\end{cases} </div>\n",
    "    \n",
    "<div dir=rtl> <br>\n",
    "    با مشتق گرفتن در حالت اول خواهیم داشت:\n",
    "<br><br>\n",
    "\\begin{cases}\n",
    "    \\frac{\\partial J_1}{\\partial w} = w\\\\\n",
    "    \\\\\n",
    "    \\frac{\\partial J_1}{\\partial b} = 0\n",
    "\\end{cases} </div>\n",
    "    \n",
    "    \n",
    "<div dir=rtl> <br>\n",
    "    در حالت دوم نیز خواهیم داشت:\n",
    "<br><br>\n",
    "\\begin{cases}\n",
    "    \\frac{\\partial J_2}{\\partial w} = w - C\\sum_{i=1}^{N} y^{(i)}x^{(i)}\\\\\n",
    "    \\\\\n",
    "    \\frac{\\partial J_2}{\\partial b} = -C\\sum_{i=1}^{N} y^{(i)}\n",
    "\\end{cases} </div>\n",
    "    \n",
    "<div dir=rtl> <br>\n",
    "(در صورتی که مشتق گرفتن نسبت به بردار در بالا برای شما مشخص نیست، پیشنهاد می‌شود نسبت به تک‌تک درایه‌های بردار $w$ مشتق گرفته و آن‌ها را در یک بردار قرار دهید و به نتایج بالا برسید؛\n",
    "    زیرا همانطور که می‌دانید مشتق گرفتن تابع نسبت به بردار به معنای مشتق گرفتن نسبت به درایه‌های آن بردار می‌باشد!)\n",
    "<br> <br>  حال که تابع هزینه و گرادیان‌های مورد نیاز جهت کمینه کردن آن را بدست آورده‌ایم، می‌توانیم پیاده‌سازی الگوریتم بالا را شروع نماییم:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d2437d1",
   "metadata": {
    "id": "8d2437d1"
   },
   "source": [
    "<font face=\"XB Zar\" size=4><div dir=rtl>\n",
    "<font color=\"red\" size=\"6\">پیاده‌سازی Maximal Margin Classifier</font> <br>\n",
    "<br> با توجه به توضیحات بخش‌های بالاتر، حل این مسئله را می‌توان به 4 بخش کوچکتر تقسیم کرد: <br>\n",
    "<br> 1. پیاده سازی اولیه و وزن‌دهی اولیه پارامتر‎های مسئله\n",
    "<br> 2. نگاشت کلاس‌ها از \n",
    "    $\\{0,1\\}$ به\n",
    "    $\\{-1,1\\}$\n",
    "<br> 3. انجام gradient descent به تعداد گام‌های مناسبی جهت همگرا شدن الگوریتم (این قسمت شامل محاسبه گرادیان‎ها و آپدیت پارامترها می‌باشد.)\n",
    "<br> 4. بدست آوردن ابرصفحه بهینه و پیشبینی کلاس داده‌های جدید\n",
    "</div>\n",
    "    \n",
    "    \n",
    "<div dir=rtl> <br><br>\n",
    "    برای این مقصود، ما کلاسی به نام \n",
    "    <code> maxMargin_classifier </code>\n",
    "    را تعریف می‌کنیم که درون خود متدهای مورد نیاز جهت پیاده‌سازی این الگوریتم را دارد.\n",
    "    در ادامه، یک تعریف اولیه و کلیت کلاس موردنظر ما برای پیاده‌سازی الگوریتم به عنوان نقشه‌ای در ادامه مسیر آورده شده است؛ با توجه به توضیحات هر متد سعی در کامل‌کردن آن خواهیم داشت:(تنها کتابخانه مورد نیاز ما برای این پیاده‌سازی کتابخانه numpy می‌باشد.)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b48ee38",
   "metadata": {
    "id": "9b48ee38"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class maxMargin_classifier:\n",
    "    \n",
    "    def __init__(self, learning_rate=1e-3, C=10, n_iters=1000):\n",
    "        \"\"\"\n",
    "        Class initializer: Initialize class parameters including:\n",
    "            lr: learning rate of gradient descent algorithm, default=0.001\n",
    "            C: C hyperparameter of hinge loss in cost function, default=10\n",
    "            n_iters: number of iterations in gradient_descent, default=1000\n",
    "            w: weights(normal vector) of maximal margin hyperplane (parameters of the algorithm)\n",
    "            b: intercept(bias) of maximal margin hyperplane (parameter of the algorithm)\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    #-------------------------------------------#\n",
    "    \n",
    "    def init_params(self, X):\n",
    "        \"\"\"\n",
    "        Initializer of the algorithm's parameters (w & b)\n",
    "        Inputs:\n",
    "            X: N*p matrix including N samples with p features\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    #-------------------------------------------#\n",
    "\n",
    "    def get_class_map(self, y):\n",
    "        \"\"\"\n",
    "        Mapping the targets' classes from {0,1} to {-1,1}\n",
    "        Inputs:\n",
    "            y: N-vector of class labels\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    #-------------------------------------------#\n",
    "    \n",
    "    def satisfy_constraint(self, x, idx):\n",
    "        \"\"\"\n",
    "        Checks wether the constraint of hinge loss is satisfied, i.e. which loss function(J1 or J2)\n",
    "        and corresponding gradients are needed to be calculated.\n",
    "        Inputs:\n",
    "            x: a training sample \n",
    "            idx: index of x\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    #-------------------------------------------#\n",
    "    \n",
    "    def get_gradients(self, constraint, x, idx):\n",
    "        \"\"\"\n",
    "        Get the gradient for a training sample\n",
    "        Inputs:\n",
    "            constraint: wether or not the \"satisfy_constraint\" method returns true\n",
    "            x: a training sample\n",
    "            idx: index of x\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    #-------------------------------------------#\n",
    "    \n",
    "    def update_params(self, dw, db):\n",
    "        \"\"\"\n",
    "        Update the parameters, weights(w) and bias(b)\n",
    "        Inputs:\n",
    "            dw: partial derivative of cost function w.r.t. weights\n",
    "            db: partial derivative of cost function w.r.t. bias\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    #-------------------------------------------#\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit the model on the training set\n",
    "        Inputs:\n",
    "            X: N*p matrix of training set, N samples with p features\n",
    "            y: target values of binary classes\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    #-------------------------------------------#\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict the class of new samples.\n",
    "        Inputs:\n",
    "            X: k*p matrix of new samples, k samples with p features\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca81e07",
   "metadata": {
    "id": "1ca81e07"
   },
   "source": [
    "<font face=\"XB Zar\" size=4><div dir=rtl>\n",
    "حال بخش‌های مختلف این کلاس را کامل می‌کنیم؛ در ابتدا متدهای پایه‌ای این کلاس را پیاده‌سازی میکنیم و پارامترهای کلاس را مقداردهی اولیه میکنیم: (برای سادگی مقدار اولیه وزن‌ها و بایاس را برابر با صفر در نظر میگیریم.)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa56a98",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3aa56a98",
    "outputId": "771db89f-b1b5-422c-8ba0-d959dd76a0a3"
   },
   "outputs": [],
   "source": [
    "...\n",
    "def __init__(self, learning_rate=1e-3, C=10, n_iters=1000):\n",
    "    self.lr = learning_rate\n",
    "    self.C = C\n",
    "    self.n_iters = n_iters\n",
    "    self.w = None\n",
    "    self.b = None\n",
    "\n",
    "def init_params(self, X):\n",
    "    p = X.shape[1]\n",
    "    self.w = np.zeros(p)\n",
    "    self.b = 0\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afec8223",
   "metadata": {
    "id": "afec8223"
   },
   "source": [
    "<font face=\"XB Zar\" size=4><div dir=rtl>\n",
    "همانطور که گفته شد، در این الگوریتم نیاز داریم برچسب داده‌های ما به جای \n",
    "    $\\{0,1\\}$ به صورت\n",
    "    $\\{-1,1\\}$ باشد.\n",
    "    بنابراین از آنجایی که در اکثر موارد برچسب‌های اولیه به صورت 0 و 1 هستند، نیاز به متدی داریم که این تبدیل را انجام دهد. با توجه به این موضوع، سعی کنید متد \n",
    "    <code> get_class_map </code>\n",
    "    را کامل کنید. ورودی این تابع، بردار باینری از برچسب داده‌هاست و شما باید این نگاشت را انجام داده و بردار $y$ جدید را بازگردانید:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862b1b2e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "862b1b2e",
    "outputId": "1a82fc3a-3d48-4c53-cc02-6c14e1eba19a"
   },
   "outputs": [],
   "source": [
    "...\n",
    "def get_class_map(self, y):\n",
    "    return #YOUR CODE\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a70688",
   "metadata": {
    "id": "53a70688"
   },
   "source": [
    "<font face=\"XB Zar\" size=4><div dir=rtl>\n",
    "همچنین نیاز به متدهایی داریم که بتوانیم بسته به مقدار کنونی پارامترهای مدل، گرادیان‌های متناظر آن‌ها را بدست آورده و آپدیت نماید. برای این منظور 3 متد نوشته شده که نخست چک کنیم در کدام حالت تابع هزینه هستیم؛ همانطور که از تابع هزینه تعریف‌شده مشخص است، این شرط به صورت زیر است:\n",
    "<br><br> <center> $ y^{(i)}(w^T x^{(i)} + b) \\geq 1 $\n",
    "</div>\n",
    "    \n",
    "<div dir=rtl> <br>\n",
    "    سپس با توجه به نتیجه این شرط، گرادیان‌ها را نسبت به پارامترهای مدل، $w$ و $b$ بدست آوریم و آن‌ها را آپدیت کنیم:\n",
    "    (البته که شما می‌توانید هر 3 تابع را با یکدیگر ترکیب کرده و آن‌ها را در یک متد پیاده‌سازی نمایید.)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7311a25",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a7311a25",
    "outputId": "8e52c7ee-024b-46f5-dfc3-dda5e5bd45fa"
   },
   "outputs": [],
   "source": [
    "...\n",
    "def satisfy_constraint(self, x, idx):\n",
    "    ##Checks wether   y(w^T x + b) >= 1\n",
    "    linear_model = np.dot(x, self.w) + self.b\n",
    "    return self.encoded_y[idx] * linear_model >= 1 \n",
    "\n",
    "def get_gradients(self, constraint, x, idx):\n",
    "    #Case1: J1\n",
    "    if constraint: \n",
    "        dw = self.w\n",
    "        db = 0\n",
    "        return dw, db\n",
    "    \n",
    "    #Case2: J2\n",
    "    ### YOUR CODE\n",
    "    return dw, db\n",
    "\n",
    "def update_params(self, dw, db):\n",
    "    #YOUR CODE\n",
    "    pass\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677b2a4e",
   "metadata": {
    "id": "677b2a4e"
   },
   "source": [
    "<font face=\"XB Zar\" size=4><div dir=rtl>\n",
    "حال باید به کمک توابعی که در بخش قبل پیاده‌سازی کرده‌ایم، مدل خود را بر روی نمونه‌های آموزشی برازش (fit) کنیم.\n",
    "پس ابتدا پارامترهای مدل را مقداردهی اولیه میکنیم و سپس با نگاشت برچسب داده‌ها، به کمک الگوریتم gradient descent پارامترهای بهینه را پیدا میکنیم:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62169640",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "62169640",
    "outputId": "58c4c746-a531-4fe0-caa9-45538a9ce3c6"
   },
   "outputs": [],
   "source": [
    "...\n",
    "def fit(self, X, y):\n",
    "    self.init_params(X) #Initialization of parameters\n",
    "    self.encoded_y = self.get_class_map(y) #Encode y using the \"get_class_map\" method\n",
    "\n",
    "    #Iterate and update your parameters\n",
    "    for _ in range(self.n_iters):\n",
    "        for idx, x in enumerate(X):\n",
    "            constraint = self.satisfy_constraint(x, idx) #Check the constraint for cost function\n",
    "            dw, db = self.get_gradients(constraint, x, idx) #get the gradient using the constraint\n",
    "            self.update_params(dw, db) #Update the parameters\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c98717e",
   "metadata": {
    "id": "5c98717e"
   },
   "source": [
    "<font face=\"XB Zar\" size=4><div dir=rtl>\n",
    "در نهایت باید بتوانیم با استفاده از مدل خود کلاس نمونه‌های جدید را پیش‌بینی نماییم؛ همانطور که گفته شد، می‌توان این کار را به کمک علامت نمونه نسبت به ابرصفحه برازش‌شده پیدا کرد؛ یعنی:\n",
    "<br> <center>  $f(x) = w^T x + b$ <br> <br>\n",
    "\\begin{cases}\n",
    "    \\hat{y} = +1 \\qquad if \\quad f(x) > 0\\\\\n",
    "    \\\\\n",
    "    \\hat{y} = -1 \\qquad if \\quad f(x) < 0\n",
    "\\end{cases} </div>\n",
    "    \n",
    "<div dir=rtl> <br> \n",
    "    با استفاده از توضیحات بالا، متد \n",
    "    <code> predict </code>\n",
    "    را پیاده‌سازی کنید:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a616449",
   "metadata": {
    "id": "2a616449"
   },
   "outputs": [],
   "source": [
    "...\n",
    "def predict(self, X):\n",
    "    #YOUR CODE\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc5eb28",
   "metadata": {
    "id": "ddc5eb28"
   },
   "source": [
    "<font face=\"XB Zar\" size=4><div dir=rtl>\n",
    " حال تمام قطعات پازل مورد نیاز برای کامل کردن کلاس خود را داریم و کافی است آن‌ها را کنار هم قرار دهیم:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7d8b24",
   "metadata": {
    "id": "ec7d8b24"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class maxMargin_classifier:\n",
    "    def __init__(self, learning_rate=1e-3, C=10, n_iters=1000):\n",
    "        self.lr = learning_rate\n",
    "        self.C = C\n",
    "        self.n_iters = n_iters\n",
    "        self.w = None\n",
    "        self.b = None\n",
    "    #_____________________________________________________#\n",
    "    \n",
    "    def init_params(self, X):\n",
    "        p = X.shape[1]\n",
    "        self.w = np.zeros(p)\n",
    "        self.b = 0\n",
    "    #_____________________________________________________#\n",
    "    \n",
    "    def get_class_map(self, y):\n",
    "        #YOUR CODE\n",
    "        return np.where(y <= 0, -1, 1)\n",
    "        #pass\n",
    "        \n",
    "    #_____________________________________________________#\n",
    "    \n",
    "    \n",
    "    def satisfy_constraint(self, x, idx):\n",
    "        ##Checks wether   y(w^T x + b) >= 1\n",
    "        linear_model = np.dot(x, self.w) + self.b\n",
    "        return self.encoded_y[idx] * linear_model >= 1 \n",
    "    #_____________________________________________________#\n",
    "    \n",
    "    def get_gradients(self, constraint, x, idx):\n",
    "        #Case1: J1\n",
    "        if constraint: \n",
    "            dw = self.w\n",
    "            db = 0\n",
    "            return dw, db\n",
    "        #Case2: J2\n",
    "        dw = self.w - self.C * np.dot(self.encoded_y[idx], x)\n",
    "        db = - self.C * self.encoded_y[idx]\n",
    "        return dw, db\n",
    "    #_____________________________________________________#\n",
    "    \n",
    "    def update_params(self, dw, db):\n",
    "        #YOUR CODE\n",
    "        self.w -= self.lr * dw\n",
    "        self.b -= self.lr * db\n",
    "        #pass\n",
    "    #_____________________________________________________#\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.init_params(X)\n",
    "        self.encoded_y = self.get_class_map(y)\n",
    "        \n",
    "        for _ in range(self.n_iters):\n",
    "            for idx, x in enumerate(X):\n",
    "                constraint = self.satisfy_constraint(x, idx)\n",
    "                dw, db = self.get_gradients(constraint, x, idx)\n",
    "                self.update_params(dw, db)\n",
    "                \n",
    "    #_____________________________________________________#\n",
    "    \n",
    "    def predict(self, X):\n",
    "        #YOUR CODE\n",
    "        estimate = np.dot(X, self.w) + self.b\n",
    "        prediction = np.sign(estimate)\n",
    "        return np.where(prediction == -1, 0, 1)\n",
    "        #pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25fb1784",
   "metadata": {
    "id": "25fb1784"
   },
   "source": [
    "<font face=\"XB Zar\" size=4><div dir=rtl>\n",
    "اکنون می‌توانیم کلاسی که نوشته‌ایم را بر روی یک دیتاست امتحان کنیم؛ برای سادگی و قابلیت نمایش، از یک دیتاست ساختگی با 2 ویژگی و 250 نمونه استفاده شده است و دیتای ما طوری ساخته شده است که قابلیت تفکیک‌پذیری خطی را به طور کامل دارد؛ پس انتظار داریم که دقت الگوریتم ما روی این داده‌ها 100% باشد.\n",
    "<br><br>\n",
    "    (تکه کد پایین صرفا جهت قرار دادن نمودارها در مرکز صفحه نوشته شده است و اهمیت خاصی ندارد.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc09191e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML as Center\n",
    "\n",
    "Center(\"\"\" <style>\n",
    ".output_png {display: table-cell; text-align: center; vertical-align: middle;}\n",
    "</style> \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b3f6a6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "56b3f6a6",
    "outputId": "ff8adc67-2e1a-403e-bcd9-668c33875fd3"
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Generate an artifical linearly-separable dataset with 2 features and 250 samples\n",
    "#using make_blobs function from scikit-learn.datasets\n",
    "N = 250\n",
    "p = 2\n",
    "X, y = datasets.make_blobs(\n",
    "    n_samples=N, n_features=p, centers=2, cluster_std=1.05, random_state=1\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "scatter = plt.scatter(X[:, 0], X[:, 1], marker='o', cmap='coolwarm', c=y, s=100, alpha=0.75);\n",
    "plt.xlabel(\"$x_1$\")\n",
    "plt.ylabel(\"$x_2$\")\n",
    "plt.title(\"Scatter Plot of the Dataset\")\n",
    "plt.legend(handles=scatter.legend_elements()[0], labels=['Class 1','Class 0'], fontsize='x-large', markerscale=1.5)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01cc36a7",
   "metadata": {
    "id": "01cc36a7"
   },
   "source": [
    "<font face=\"XB Zar\" size=4><div dir=rtl>\n",
    "برای ساختن مدل، ابتدا مدل خود را بر روی داده‌های آموزش برازش می‌کنیم و سپس صحت پیشبینی آن را بر روی داده‌های تست بررسی میکنیم؛ در اینجا از 80% داده‌ها برای آموزش و از سایر 20% جهت تست‌کردن مدل استفاده شده است: \n",
    "<br>    (از آنجا که دیتای تولید شده به صورت رندوم بوده است، نیازی به shuffle کردن پیش از تقسیم‌بندی نداریم.)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679146bc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 381
    },
    "id": "679146bc",
    "outputId": "290f7fc2-0908-4529-9643-6cdc0854ed80"
   },
   "outputs": [],
   "source": [
    "#Take 80% of data for training and the other 20% to test the accuracy of model\n",
    "frac = 0.8\n",
    "train_size = int(frac*N)\n",
    "X_train = X[:train_size, :]\n",
    "y_train = y[:train_size]\n",
    "\n",
    "X_test = X[train_size:, :]\n",
    "y_test = y[train_size:]\n",
    "\n",
    "#Build the classifier\n",
    "clf = maxMargin_classifier(learning_rate=1e-3, C=10, n_iters=1000)\n",
    "#Fit the model on the train data\n",
    "clf.fit(X_train, y_train)\n",
    "#Predict the model on test data\n",
    "y_pred_test = clf.predict(X_test)\n",
    "\n",
    "#Find the accuracy of model on test set using the predicted labels with their true values\n",
    "accuracy = np.sum(y_test==y_pred_test) / len(y_test)\n",
    "\n",
    "print(\"Maximal Margin Classifier Accuracy:\", accuracy*100, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe3353e",
   "metadata": {
    "id": "4fe3353e"
   },
   "source": [
    "<font face=\"XB Zar\" size=4><div dir=rtl>\n",
    "در نهایت نیز می‌توانیم مرز بهینه را همراه با حاشیه بدست آمده ترسیم کنیم و نتایج خود را تایید نماییم: \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a734bb53",
   "metadata": {
    "id": "a734bb53"
   },
   "outputs": [],
   "source": [
    "#a simple functin to get the hyperplane from the weights and bias of model\n",
    "def get_hyperplane(x, w, b, offset):\n",
    "    return (-w[0] * x - b + offset) / w[1]\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10,6))\n",
    "\n",
    "plt.set_cmap('PiYG')\n",
    "plt.scatter(X_train[:, 0], X_train[:, 1], marker='o', c=y_train, s=100, alpha=0.75)\n",
    "plt.scatter(X_test[:, 0], X_test[:, 1], marker=\"x\", c=y_test, s=100, alpha=0.75)\n",
    "\n",
    "x0_1 = np.amin(X_train[:, 0])\n",
    "x0_2 = np.amax(X_train[:, 0])\n",
    "\n",
    "x1_1 = get_hyperplane(x0_1, clf.w, clf.b, 0)\n",
    "x1_2 = get_hyperplane(x0_2, clf.w, clf.b, 0)\n",
    "\n",
    "x1_1_m = get_hyperplane(x0_1, clf.w, clf.b, -1)\n",
    "x1_2_m = get_hyperplane(x0_2, clf.w, clf.b, -1)\n",
    "\n",
    "x1_1_p = get_hyperplane(x0_1, clf.w, clf.b, 1)\n",
    "x1_2_p = get_hyperplane(x0_2, clf.w, clf.b, 1)\n",
    "\n",
    "ax.plot([x0_1, x0_2], [x1_1, x1_2], \"-\", c='k', lw=1, alpha=0.9)\n",
    "ax.plot([x0_1, x0_2], [x1_1_m, x1_2_m], \"--\", c='grey', lw=1, alpha=0.8)\n",
    "ax.plot([x0_1, x0_2], [x1_1_p, x1_2_p], \"--\", c='grey', lw=1, alpha=0.8)\n",
    "\n",
    "x1_min = np.amin(X[:, 1])\n",
    "x1_max = np.amax(X[:, 1])\n",
    "ax.set_ylim([x1_min - 3, x1_max + 3])\n",
    "\n",
    "for spine in ['top','right']:\n",
    "    ax.spines[spine].set_visible(False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7fefd0b",
   "metadata": {
    "id": "d7fefd0b"
   },
   "source": [
    "<font face=\"XB Zar\" size=4><div dir=rtl>\n",
    "<center><font color=\"red\" size=\"6\">معرفی کتابخانه Scikit-Learn</font> <br> <br> \n",
    "<img src=\"resources/sk_learn.png\" width=\"800\">\n",
    "</center>\n",
    "</div> <br><br>\n",
    "    \n",
    "<font face=\"XB Zar\" size=4><div dir=rtl>\n",
    "<a href=\"https://scikit-learn.org/\" style=\"text-decoration:none;\">scikit-learn</a> از کتابخانه‌های متن‌باز پایتون است که ابزارهای کاربردی زیادی را به منظور یادگیری ماشین و مدل‌سازی آماری داده‌ها همچون طبقه‌بندی (classification) ، رگرسیون، خوشه‌بندی و کاهش ابعاد فراهم می‌کند. این کتابخانه بر پایه‌ی کتابخانه‌های <a href=\"https://numpy.org/\" target=\"_blank\" rel=\"noopener noreferrer\"> Numpy </a>،<a href=\"https://pandas.pydata.org/\" target=\"_blank\" rel=\"noopener noreferrer\"> Pandas</a> ،<a href=\"https://scipy.org/\" target=\"_blank\" rel=\"noopener noreferrer\"> Scipy</a> و <a href=\"https://matplotlib.org/\" target=\"_blank\" rel=\"noopener noreferrer\"> Matplotlib</a> طراحی شده است.</p>\n",
    "\n",
    "<br>\n",
    "\n",
    "<h2 style=\"text-align: justify;\"><strong>ویژگی‌های Scikit-Learn</strong></h2>\n",
    "\n",
    "<p style=\"text-align: justify;\">علاوه بر کاربرد این کتابخانه در مواردی مانند بارگذاری، تغییر و دستکاری داده‌ها، Scikit-Learn در مدل‌سازی داده، تمرکز ویژه‌ای دارد. برخی از مدل‌سازی‌های مشهور در این کتابخانه عبارتند از:</p>\n",
    "<ul style=\"text-align: justify;\">\n",
    "<font color='blue'><li>الگوریتم‌های یادگیری نظارت شده (Superuised Learning)</li>\n",
    "</ul>\n",
    "<p style=\"text-align: justify;\">تقریبا تمامی الگویتم‌های یادگیری نظارت شده معروف رگرسیون خطی، ماشین بردار پشتیبان (SVM)، درخت تصمیم‌گیری و… در این کتابخانه موجود هستند.</p>\n",
    "<ul style=\"text-align: justify;\">\n",
    "<font color='blue'><li>الگوریتم‌های یادگیری بدون نظارت (Unsuperuised Learning)</li>\n",
    "</ul>\n",
    "<p style=\"text-align: justify;\">تمامی الگوریتم‌های نظارت نشده همچون روش‌های خوشه‌بندی، تحلیل فاکتوری، PCA، شبکه‌های عصبی نظارت نشده و… بخشی از کتابخانه‌ی Superuised Learning می‌باشند.</p>\n",
    "<ul style=\"text-align: justify;\">\n",
    "<font color='blue'><li>اعتبارسنجی (Cross-Validation)</li>\n",
    "</ul>\n",
    "<p style=\"text-align: justify;\">به منظور کنترل دقت مدل نظارتی بر روی داده‌های تست، مورد استفاده قرار می‌گیرد.</p>\n",
    "<ul style=\"text-align: justify;\">\n",
    "<font color='blue'><li>کاهش ابعاد داده‌ها</li>\n",
    "</ul>\n",
    "<p style=\"text-align: justify;\">به منظور کاهش ویژگی‌های داده‌ها جهت خلاصه‌سازی، مصورسازی و نیز انتخاب ویژگی مورد استفاده قرار می‌گیرد.</p>\n",
    "<ul style=\"text-align: justify;\">\n",
    "<font color='blue'><li>روش‌های گروه‌بندی Ensemble Methods</li>\n",
    "</ul>\n",
    "<p style=\"text-align: justify;\">همان‌طور که از نامش پیداست، این روش برای ترکیب مدل‌های یادگیری نظارت شده، به منظور پیش‌بینی برچسب داده‌های تست به کار می‌رود و در فصل آینده بیشتر با آن‌ها آشنا خواهید شد.</p>\n",
    "<ul style=\"text-align: justify;\">\n",
    "<font color='blue'><li>استخراج ویژگی (Feature extraction)</li>\n",
    "</ul>\n",
    "<p style=\"text-align: justify;\">به منظور تعریف ویژگی‌های جدید از روی ویژگی‌های اصلی جهت استخراج داده‌های مفید به کار می‌رود.</p>\n",
    "<ul style=\"text-align: justify;\">\n",
    "<font color='blue'><li>انتخاب ویژگی (Feature selection)</li>\n",
    "</ul>\n",
    "<p style=\"text-align: justify;\">روش‌هایی جهت انتخاب ویژگی برای ایجاد مدل‌هایی با دقت بالاتر یا افزونگی کمتر می‌باشند.</p>\n",
    "</div>\n",
    "                                                                    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397b23bd",
   "metadata": {
    "id": "397b23bd"
   },
   "source": [
    "<font face=\"XB Zar\" size=4><div dir=rtl>\n",
    "<h2 style=\"text-align: justify;\">پیاده‌سازی Maximal Margin Classifier با استفاده از Scikit-learn<strong></strong></h2> \n",
    "<p style=\"text-align: justify;\">Scikit-Learn مجموعه داده‌های نمونه‌ی محدودی همچون Iris و digit برای طبقه‌بندی (Classification) و قیمت خانه در Boston برای رگرسیون دارد که توسط ماژول \n",
    "    <code>sklearn.datasets</code> در دسترس هستند؛\n",
    "     اما ما در این قسمت از یک دیتاست مصنوعی 2بعدی استفاده می‌کنیم تا بتوانیم نمایش مشخصی از مدل بدست آمده داشته باشیم.</p>\n",
    "    <br> در ادامه سعی می‌کنیم پیاده‌سازی بالا را با استفاده از این کتابخانه انجام دهیم.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98090928",
   "metadata": {},
   "source": [
    "<div><font face=\"XB Zar\" size=4><div dir=rtl>\n",
    "<h4>انگیزه : بهترین مرز جدا کننده </h4><br>\n",
    "ما به دنبال بهترین مرز جدا کننده هستیم اما این بهترین مرز چیست و چگونه پیدا میشود؟\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "FbYW5KyL_TTu",
   "metadata": {
    "id": "FbYW5KyL_TTu"
   },
   "outputs": [],
   "source": [
    "N = 250\n",
    "p = 2\n",
    "X, y = datasets.make_blobs(\n",
    "    n_samples=N, n_features=p, centers=2, cluster_std=1.05, random_state=1\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "scatter = plt.scatter(X[:, 0], X[:, 1], marker='o', c=y, s=100, cmap='coolwarm', alpha=0.75);\n",
    "plt.xlabel(\"$x_1$\")\n",
    "plt.ylabel(\"$x_2$\")\n",
    "plt.title(\"Scatter Plot of the Dataset\")\n",
    "plt.legend(handles=scatter.legend_elements()[0], labels=['Class 1','Class 0'], fontsize='x-large', markerscale=2)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057b90a4",
   "metadata": {},
   "source": [
    "ِ<div><font face=\"XB Zar\" size=4><div dir=rtl>\n",
    "اگر فاصله نزدیک‌ترین نقطه به مرز را حاشیه بنامیم، بهترین خط جداکننده دو کلاس، خطی است که این فاصله را بیشینه می‌کند. نمونه‌ای از خطوط جداکننده را همراه با حاشیه‌های آن‌ها در ادامه ترسیم می‌کنیم. \n",
    "<div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c575e239",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 4))\n",
    "\n",
    "# draw separators and margins\n",
    "x_values = np.linspace(-14, 2)\n",
    "for i, (w, b, m) in enumerate([(-1, -3, 2.2), (-0.75, -4, 4), (-0.5, -5, 1.2)]):\n",
    "    plt.subplot(1, 3, i + 1)\n",
    "    \n",
    "    # draw data\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, s=50, edgecolors='black', cmap='coolwarm', alpha=0.6)\n",
    "    y_values = w * x_values + b\n",
    "    \n",
    "    # draw separator and margin\n",
    "    plt.plot(x_values, y_values, '-k')\n",
    "    plt.fill_between(x_values, y_values - m, y_values + m, edgecolor='none', color='#7D3C4A', alpha=0.3)\n",
    "    plt.xlim(-14, 2)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "C6jk2D9aBoRm",
   "metadata": {
    "id": "C6jk2D9aBoRm"
   },
   "source": [
    "<font face=\"XB Zar\" size=4><div dir=rtl><div dir=\"rtl\" class=\"blog-sc\">\n",
    "\n",
    "<h3 style=\"text-align: justify;\"><strong>آموزش مدل</strong></h3>\n",
    "\n",
    "<p style=\"text-align: justify;\"> Scikit-Learn الگوریتم‌های یادگیری ماشین گسترده‌ای دارد که رابط یا اینترفیس ثابتی برای مدل کردن، پیش‌بینی دقت و فراخوانی برای تمام الگوریتم‌ها ارائه می‌دهد.</p>\n",
    "\n",
    "\n",
    "<p style=\"text-align: justify;\">مثال زیر نحوه‌ی آموزش SVC (Support Vector Classifier) را نمایش می‌دهد.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "NZGCcSErGKwg",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "NZGCcSErGKwg",
    "outputId": "031252dc-8026-4a77-b6c0-354a99dc8bd2"
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# create Support Vector Classifier using a linear kernel\n",
    "clf = SVC(kernel='linear')\n",
    "\n",
    "# train the classifier\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dEEq9puEHFyO",
   "metadata": {
    "id": "dEEq9puEHFyO"
   },
   "source": [
    "<font face=\"XB Zar\" size=4><div dir=\"rtl\" class=\"blog-sc\">\n",
    "\n",
    "<p style=\"text-align: justify;\">همانند سایر طبقه‌بندها، SVC با دو آرایه مدل می‌شود:</p>\n",
    "\n",
    "\n",
    "<ul style=\"text-align: justify;\">\n",
    "<li>ماتریس X که نگهدارنده‌ی نمونه‌های آموزشی است و در اندازه‌ی [n_samples, n_features] است.</li>\n",
    "<li>آرایه‌ی y که مقادیر هدف یا target را نگهداری می‌کند. کلاس برچسب برای نمونه‌های آزمایشی است و در اندازه‌ی [n_samples] است.</li>\n",
    "</ul>\n",
    "\n",
    "<p style=\"text-align: justify;\">بسیاری از الگوریتم‌های یادگیری ماشین دارای هایپرپارمترهایی هستند که باید پیش از آموزش مدل آن‌ها را مشخص کنیم؛ به طور مثال در این طبقه‌بند نیز باید هایپرپارامتر kernel را مشخص کنیم. درباره این هایپرپارامتر در ادامه کامل‌تر توضیح داده خواهد شد؛ \n",
    "    اما فعلا بدانید که با تنظیم آن بر روی <code>linear</code> درواقع همان Maxiaml Margin Classifier موردنظر ما را آموزش می‌دهد.</p>\n",
    "\n",
    "<p style=\"text-align: justify;\">در نهایت طبقه‌بند را بر روی ورودی X و خروجی Y با اعمال تابع fit برازش می‌کنیم.</p>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zDsCY1J1KLnx",
   "metadata": {
    "id": "zDsCY1J1KLnx"
   },
   "source": [
    "<font face=\"XB Zar\" size=4><p dir=rtl style=\"text-align: justify;\">حال می‌توانیم به کمک attribute (صفت) _coef از این کلاس، وزن اختصاصی هر ویژگی از داده‌های ورودی و به کمک صفت _intercept نیز بایاس ابرصفحه جداکننده را به دست آوریم :</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Pr3Vq4JEKO01",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "Pr3Vq4JEKO01",
    "outputId": "32359a9a-ca69-4279-f303-b739f141b3f1"
   },
   "outputs": [],
   "source": [
    "print(\"w =\", clf.coef_)\n",
    "print(\"b =\", clf.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "A73CjEaNLUh6",
   "metadata": {
    "id": "A73CjEaNLUh6"
   },
   "source": [
    "<font face=\"XB Zar\" size=4><p dir=rtl style=\"text-align: justify;\">\n",
    "    مقادیر بالا نشان می‌دهند معادله ابرصفحه جداکننده به صورت\n",
    "    $f(X) = w_1x_1+w_2x_2+b = -0.1634x_1 -0.2455x_2 -0.8717 = 0$\n",
    "    می‌باشد. <br>\n",
    "    همچنین می‌توانیم به کمک تابع <code>predict</code> کلاس داده‌های جدید را پیش‌بینی کنیم:\n",
    "    </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "AslwU95ILX34",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "AslwU95ILX34",
    "outputId": "4222c799-4099-408f-eab9-0ab202787074"
   },
   "outputs": [],
   "source": [
    "new_samples = [[0, 5],\n",
    "              [-4, -1]]\n",
    "print(clf.predict(new_samples))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pgoKlWy7LxSZ",
   "metadata": {
    "id": "pgoKlWy7LxSZ"
   },
   "source": [
    "<div><font face=\"XB Zar\" size=4><div dir=rtl>\n",
    "در این مدل بردارهای پشتیبان در \n",
    "    <code>support_vectors_</code>\n",
    "    ذخیره می‌شوند و می‌توانیم ببینیم کدام نمونه‌ها در مشخص‌کردن ابرصفحه بهینه نقش داشته‌اند:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "DQTFg731MGZW",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "id": "DQTFg731MGZW",
    "outputId": "f98a3d6d-526e-4e6b-83ce-28f805500c79"
   },
   "outputs": [],
   "source": [
    "clf.support_vectors_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b431f108",
   "metadata": {},
   "source": [
    "<font face=\"XB Zar\" size=4><div dir=rtl>\n",
    "    همانطور که می‌بینیم، فقط 2 نمونه support داریم. <br> \n",
    "    در نهایت نیز به کمک تابع <code>plot_svc_decision_boundary</code> که در ادامه نوشته شده است، خط بهینه جداکننده‌ی دو کلاس که در این مدل بدست آمده است را ترسیم می‌کنیم."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169ec900",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_svc_decision_function(clf, ax=None):\n",
    "    \"\"\"Plot the decision function for a 2D SVC\"\"\"\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "        \n",
    "    x = np.linspace(plt.xlim()[0], plt.xlim()[1], 30)\n",
    "    y = np.linspace(plt.ylim()[0], plt.ylim()[1], 30)\n",
    "    Y, X = np.meshgrid(y, x)\n",
    "    \n",
    "    P = np.zeros_like(X)\n",
    "    for i, xi in enumerate(x):\n",
    "        for j, yj in enumerate(y):\n",
    "            P[i, j] = clf.decision_function([[xi, yj]])\n",
    "    \n",
    "    # plot the margins\n",
    "    ax.contour(X, Y, P, colors='k', \n",
    "               levels=[-1, 0, 1], alpha=0.5,\n",
    "               linestyles=['--', '-', '--'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mPN24pdjMQpS",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "mPN24pdjMQpS",
    "outputId": "76cb6065-273f-4b6c-942e-21ece8298282"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "# plot data and margin\n",
    "scatter_data = plt.scatter(X[:, 0], X[:, 1], c=y, s=100, edgecolors='black', cmap='coolwarm', alpha=0.75)\n",
    "plt.legend(handles=scatter_data.legend_elements()[0], labels=['Class 1','Class 0'], fontsize='x-large', markerscale=2)\n",
    "plot_svc_decision_function(clf)\n",
    "\n",
    "# plot support vectors\n",
    "scatter_support = plt.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1],\n",
    "                              s=200, edgecolors='k', facecolors='none')\n",
    "\n",
    "plt.xlabel(\"$x_1$\")\n",
    "plt.ylabel(\"$x_2$\")\n",
    "plt.title(\"Optimal Separating Hyperplane with Scikit-learn\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca1da22",
   "metadata": {
    "id": "dca1da22"
   },
   "source": [
    "<br>\n",
    "<font face=\"XB Zar\" size=4><div dir=rtl>\n",
    "<center><font color=\"red\" size=\"6\">طبقه‌بند بردار پشتیبان (Support Vector Classifier)</font> </center> <br> <br>\n",
    "    همانطور که می‌توانید حدس بزنید و در کلاس نیز اشاره شده است، طبقه‌بند بیشینه‌نمای حاشیه (Maximal Margin Classifier) فقط در شرایطی عملکرد صحیحی دارد که\n",
    "    داده‌های دو کلاس به طور خطی تفکیک‌پذیر باشند. همچنین مرز این مدل توسط نمونه‌هایی مشخص می‌شود که بر روی حاشیه قرار میگیرند؛ در واقع در صورتی که فقط یکی از نمونه‌های آموزشی نزدیک به مرز مقداری تغییر اندازه داشته باشد، مرز تصمیم‌گیری به طور کامل تغییر میکند! \n",
    "    ذکر این نکته حائز اهمیت است که داده‌های ما در واقعیت در اکثر موارد مقداری Noise دارند و بنابراین مشکل بالا می‌تواند اثر نامطلوبی روی مرز تصمیم‌گیری ما داشته باشد. بنابراین این مدل به شدت قابلیت بیش‌برازش شدن (Overfitting) دارند.\n",
    "<br>\n",
    "<br> در ادامه، برای حالت گفته‌شده، مدل Maximal Margin Classifier را با استفاده از Scikit-learn امتحان میکنیم:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aacba54",
   "metadata": {
    "id": "8aacba54"
   },
   "source": [
    "<font face=\"XB Zar\" size=4><div dir=rtl>\n",
    "<font color=\"red\" size=\"5\">محدودیت اول: وابستگی به نقاط مرزی  </font> <br> <br> \n",
    "خط بهینه در این حالت به نقاط مرزی وابستگی زیادی دارد؛ بنابرین تغییر کوچکی در هر یک از آن‌ها(همچون وجود نویز یا دلایل دیگر) معادله خط جدا کننده را تحت تاثیر شدید قرار می‌دهد؛ بنابراین، این مدل به شدت قابلیت بیش‌برازش دارد.\n",
    "<br>در ادامه بر روی دیتاست اولیه که قابلیت جدایی‌پذیری خطی را دارد، مدل خود را برازش می‌کنیم؛ سپس یک نمونه‌ی نویزی به هر کلاس اضافه میکنیم و اثر آن‌ها را بر روی مرز تصمیم‌گیری مشاهده می‌کنیم:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52641899",
   "metadata": {
    "id": "52641899"
   },
   "outputs": [],
   "source": [
    "N = 250\n",
    "p = 2\n",
    "X, y = datasets.make_blobs(\n",
    "    n_samples=N, n_features=p, centers=2, cluster_std=1.05, random_state=1\n",
    ")\n",
    "\n",
    "X_noisy = np.concatenate((X,[[-5,1], [-6,-1]]), axis=0)\n",
    "y_noisy = np.append(y,[0,1])\n",
    "\n",
    "#Maximal Margin Classifier on initial dataset \n",
    "clf1 = SVC(kernel='linear', C=10)\n",
    "clf1.fit(X, y);\n",
    "\n",
    "#Maximal Margin Classifier on the dataset with an added noisy sample for class 0\n",
    "clf2 = SVC(kernel='linear', C=10)\n",
    "clf2.fit(X_noisy, y_noisy);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ed5c9b",
   "metadata": {
    "id": "33ed5c9b",
    "outputId": "5e72596a-7825-47c8-fc0e-07dd9031a059"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18,8))\n",
    "\n",
    "# plot data and margin on initial dataset\n",
    "plt.subplot(1,2,1)\n",
    "scatter1 = plt.scatter(X[:, 0], X[:, 1], c=y, s=100, cmap='coolwarm', alpha=0.75)\n",
    "plt.legend(handles=scatter1.legend_elements()[0], labels=['Class 1','Class 0'], fontsize='x-large', markerscale=1.5)\n",
    "plot_svc_decision_function(clf1)\n",
    "# plot support vectors\n",
    "plt.scatter(clf1.support_vectors_[:, 0], clf1.support_vectors_[:, 1], s=200,\n",
    "            edgecolors='k', cmap='coolwarm', facecolors='none');\n",
    "plt.xlabel(\"$x_1$\")\n",
    "plt.ylabel(\"$x_2$\")\n",
    "plt.title(\"Decision Boundary and Margins on Initial Dataset\");\n",
    "\n",
    "\n",
    "# plot data and margin on noisy dataset\n",
    "plt.subplot(1,2,2)\n",
    "scatter2 = plt.scatter(X_noisy[:, 0], X_noisy[:, 1], c=y_noisy, cmap='coolwarm', s=100, alpha=0.75)\n",
    "plt.legend(handles=scatter2.legend_elements()[0], labels=['Class 1','Class 0'], fontsize='x-large', markerscale=1.5)\n",
    "plot_svc_decision_function(clf2)\n",
    "# plot support vectors\n",
    "plt.scatter(clf2.support_vectors_[:, 0], clf2.support_vectors_[:, 1], s=200,\n",
    "            edgecolors='k', cmap='coolwarm', facecolors='none')\n",
    "plt.xlabel(\"$x_1$\")\n",
    "plt.ylabel(\"$x_2$\")\n",
    "plt.title(\"Decision Boundary and Margins on Noisy Dataset\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3384e6d3",
   "metadata": {
    "id": "3384e6d3"
   },
   "source": [
    "<font face=\"XB Zar\" size=4><div dir=rtl> \n",
    "همانطور که مشاهده می‌کنیم، در نمودار سمت چپ مرز تصمیم‌گیری رسم شده حاشیه‌ی آن مقدار قابل توجهی دارد؛ درحالیکه در نمودار سمت راست، به دلیل وجود دو نمونه نویزی، مرز تصمیم‌گیری و حاشیه‌های آن تغییراتی داشته‌اند و مقدار حاشیه آن کاهش زیادی داشته است. بنابراین اطمینان ما از مرز تصمیم‌گیری به دلیل کاهش حاشیه‌ی آن، کم خواهد شد."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f36b259",
   "metadata": {
    "id": "a438d3ca"
   },
   "source": [
    "<font face=\"XB Zar\" size=4><div dir=rtl> \n",
    "<font color=\"blue\" size=5>راه‌حل: استفاده از Soft Margin Classifier</font><br> <br>\n",
    "    برای حل این مشکل می‎توان به‌جای پیدا کردن ابرصفحه‌ای که تمام داده‌های دو کلاس را به طور کامل از یکدیگر جدا نماید، ابرصفحه‌ای را استفاده نمود که **بیشتر** داده‌های دو کلاس را از یکدیگر تفکیک نماید؛ انگیزه ما از این کار می‌تواند بدین صورت باشد که طبقه‌بند ما حساسیت کمتری به هر نمونه داشته باشد و در نتیجه احتمال کمتری بابت بیش‌برازش شدن داشته باشد. درواقع مدل ما تعداد نسبتا کمی از نمونه‌های آموزشی را اشتباه طبقه‌بندی می‌کند به منظور اینکه بتواند عمومیت\n",
    "    (generalization) بهتری بر داده‌های تست داشته باشد. \n",
    "<br> برای این منظور می‌توان به جای پیدا کردن ابرصفحه‌ای که به دنبال بیشترین حاشیه جهت تفکیک داده‌های دو کلاس است، اجازه دهیم برخی نمونه‌ها حاشیه تعریف شده را نقض کنند؛ البته این تخطی از مرز نیز باید به صورت کنترل‌شده‌ای باشد که باعث عملکرد ضعیف مدل ما نیز نباشد. این کار با اضافه‌شدن متغیرهای جدیدی به مسئله بهینه‌سازی انجام می‌شود که به این متغیرها **slack variables** می‌گویند و با $\\xi$ نمایش می‌دهند.\n",
    "این متغیر برای هر نمونه مقدار تخطی آن از حاشیه تعریف شده را مشخص می‌کند که در شکل زیر این موضوع به خوبی قابل مشاهده است.\n",
    "</div>\n",
    "    \n",
    "<div> <br>\n",
    "<center>\n",
    "<img src=\"resources/SVM-with-soft-margin-kernel-with-different-cases-of-slack-variables.jpg\" width=\"500\">\n",
    "</center>\n",
    "</div>\n",
    "    \n",
    "    \n",
    "<font face=\"XB Zar\" size=4><div dir=rtl> \n",
    "در واقع در این حالت اجازه می‌دهیم بعضی نمونه‌های آموزشی در سمت اشتباه حاشیه و یا حتی در سمت اشتباه ابرصفحه جداکننده قرار گیرند که این موضوع زمانی که ابرصفحه جداکننده‌ای وجود نداشته باشد غیرقابل اجتناب می‌باشد؛ بنابراین به جای حالت قبل، مرزهای ما حالت نرم‌تری به خود می‌گیرند که به این طبقه‌بند **Soft Margin Classifier** می‌گویند.\n",
    "    مسئله بهینه‎سازی ما در این حالت به صورت زیر خواهد بود:\n",
    "\\begin{equation*}\n",
    "    \\begin{aligned}\n",
    "        \\min_{w,b,\\xi} \\quad & \\frac{1}{2}||w||_2 ^2+C\\sum_{i=1}^{N}{\\xi_{i}}\\\\\n",
    "        \\textrm{s.t.} \\quad & y^{(i)}(w^T x^{(i)} + b) \\geq 1-\\xi_{i}\\\\\n",
    "        &\\xi_i\\geq0 \\quad \\forall i \\in \\{1,2,...,N\\}   \\\\\n",
    "    \\end{aligned}\n",
    "    \\end{equation*} </div>\n",
    "    \n",
    "<font face=\"XB Zar\" size=4><div dir=rtl>\n",
    "<br> در مسئله بهینه‌سازی بالا، $\\xi_1, \\xi_2 , ..., \\xi_N$ همان slack variables می‌باشند و به هر نمونه اجازه می‌دهند که از حاشیه یا ابرصفحه جداکننده تخطی کند؛ هرچند مجموع این خطاها نیز به تابع هدف جهت کمینه‌شدن اضافه شده است و سعی می‌کنیم با این کار، از زیاد شدن این خطا جلوگیری کنیم. این متغیرهای نامنفی در واقع نشان می‌دهند که هر نمونه نسبت به حاشیه و ابرصفحه کجا قرار گرفته است. در صورتی‌که $\\xi_i = 0$ باشد، نمونه متناظر در سمت درستی نسبت به حاشیه قرار گرفته است. در صورتی‌که $\\xi_i > 0$ باشد اما، نمونه $i$م حاشیه را نقض کرده و در صورتی که $\\xi_i > 1$ باشد نیز، نمونه متناظر در سمت اشتباهی نسبت به ابرصفحه جداکننده قرار می‌گیرد. <br>\n",
    "<br> هایپرپارامتر C نوعی regularization parameter است که مشخص می‌کند در تابع هدف مسئله بهینه‌سازی، چقدر به اندازه حاشیه یا به دقت روی داده‌های آموزشی توجه کنیم. \n",
    "    به بیان دیگر، هرچه C بزرگتر باشد، اهمیت عدم اشتباه پیش‌بینی نکردن نمونه‌ها بیشتر می‌شود و در مسئله بهینه‌سازی سعی می‌شود مجموع slack variables کوچک شود؛ حتی اگر حاشیه ما کم شود. در صورتی‌که C کوچک باشد، اهمیت بیشتری به بیشینه کردن حاشیه داده خواهد شد و هزینه کمتری بابت قرارگرفتن نمونه‌ها در سمت اشتباه حاشیه یا ابرصفحه جداکننده به تابع هدف اضافه می‌شود. پس به نوعی، هایپرپارامتر C از طریق کنترل‌کردن عرض حاشیه، trade-off بین بایاس و واریانس مدل را در اختیار دارد و مقدار بهینه آن را باید با استفاده از cross-validation مشخص کنیم. در ادامه سعی می‌کنیم، اثر این هایپرپارامتر را بر روی دیتایی که تفکیک‌پذیر خطی نیست را مشاهده کنیم."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77045050",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 200\n",
    "p = 2\n",
    "X, y = datasets.make_blobs(\n",
    "    n_samples=N, n_features=p, centers=2, cluster_std=3, random_state=1\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "scatter = plt.scatter(X[:, 0], X[:, 1], c=y, s=100, cmap='coolwarm', edgecolor='black', alpha=0.75)\n",
    "plt.legend(handles=scatter.legend_elements()[0], labels=['Class 1','Class 0'], fontsize='x-large', markerscale=1.5)\n",
    "plt.xlabel(\"$x_1$\")\n",
    "plt.ylabel(\"$x_2$\")\n",
    "plt.title(\"Scatter Plot of Data\");\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17caddb4",
   "metadata": {},
   "source": [
    "<font face=\"XB Zar\" size=4><div dir=rtl>\n",
    "همانطور که از توزیع داده‌ها مشخص هست، داده‌ها در این حالت کاملا تفکیک‌پذیر خطی نیستند؛ در ادامه سعی کنید با کامل‌کردن قطعه کد پایین، اثر هایپرپارامتر C را ترسیم کنید:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc65bbe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "C_vals = [0.01, 0.1, 1, 10]\n",
    "\n",
    "plt.figure(figsize=(18, 12))\n",
    "\n",
    "for i in range(len(C_vals)):\n",
    "    plt.subplot(2,2,i+1)\n",
    "    scatter = plt.scatter(X[:, 0], X[:, 1], c=y, s=100, cmap='coolwarm', alpha=0.75)\n",
    "    plt.legend(handles=scatter.legend_elements()[0], labels=['Class 1','Class 0'], fontsize='x-large', markerscale=1.5)\n",
    "    #YOUR CODE (Fit the model and plot its support vectors and decision boundary)\n",
    "    clf = SVC(kernel='linear', C=C_vals[i]).fit(X, y)\n",
    "    plot_svc_decision_function(clf)\n",
    "    # plot support vectors\n",
    "    plt.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1], s=200, edgecolors='k', facecolors='none');\n",
    "    \n",
    "    plt.xlabel(\"$x_1$\")\n",
    "    plt.ylabel(\"$x_2$\")\n",
    "    plt.title(\"C = %.2f\" % C_vals[i])\n",
    "    del clf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc2b34d",
   "metadata": {},
   "source": [
    "<font face=\"XB Zar\" size=4><div dir=rtl>\n",
    "        همانطور که می‌بینید، با افزایش C حاشیه بدست آمده کوچک‌تر شده و همچنین تعداد نمونه‌های support نیز کمتر شده‌اند؛ یعنی نمونه‌های کمتری در مشخص‌کردن این مرز نقش داشته‌اند. پس می‌توانیم با بدست آوردن مقدار مناسبی برای این هایپرپارامتر، از soft margin classifier جهت طبقه‌بندی استفاده کنیم تا مدل نسبت به نمونه‌های آموزشی robustness بیشتری داشته باشد.\n",
    "<br><br> با وجود تمام خاصیت‌های خوبی که soft margin classifier به مدل اولیه ما اضافه می‌کند، ما همچنان محدود به مرزهای خطی هستیم. پس باید به‌دنبال راهی برای رفع این محدودیت نیز باشیم: <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bab293c",
   "metadata": {
    "id": "9bab293c"
   },
   "source": [
    "<font face=\"XB Zar\" size=4><div dir=rtl>\n",
    "<font color=\"red\" size=\"5\">محدودیت دوم: عدم تفکیک‌پذیری خطی داده‌ها</font> <br> <br>\n",
    "    با وجود استفاده از Soft Margin Classifier نیز، مدل ما همچنان محدود به تشخیص مرز‌های خطی می‌باشد؛ درحالیکه مدل‌های خطی محدودیت‌های زیادی دارند و پاسخگوی بسیاری از مسائل دنیای واقعی نمی‌باشند. \n",
    "    در ادامه بر روی دیتای نمودار زیر مدل خود را بررسی میکنیم؛ همانطور که از توزیع نمونه‌ها مشخص است، توسط یک مرز خطی تفکیک نمی‌شوند:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7f79b3",
   "metadata": {
    "id": "9c7f79b3",
    "outputId": "147e9e5d-0440-4e18-f13a-24dddb08ad00"
   },
   "outputs": [],
   "source": [
    "# create dataset\n",
    "X, y = datasets.make_circles(200, factor=0.1, noise=0.1, random_state=1)\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "scatter = plt.scatter(X[:, 0], X[:, 1], marker='o', c=y, s=100, cmap='coolwarm', alpha=0.75)\n",
    "plt.legend(handles=scatter.legend_elements()[0], labels=['Class 1','Class 0'], fontsize='x-large', markerscale=1.5)\n",
    "plt.xlabel(\"$x_1$\")\n",
    "plt.ylabel(\"$x_2$\")\n",
    "plt.title(\"Scatter Plot of the Dataset\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f9ad7f",
   "metadata": {
    "id": "98f9ad7f"
   },
   "source": [
    "<font face=\"XB Zar\" size=4><div dir=rtl>\n",
    "می‌توانیم همانند بخش قبل مدل خود را بر داده‌های آموزش fit کرده و دقت آن را بر روی داده‌های تست بسنجیم. برای تقسیم داده‌ها به نمونه‌های آموزش و تست، می‌توانیم از تابع <code>train_test_split</code> که در ماژول <code>sklearn.model_selection</code> موجود می‌باشد استفاده کنیم."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06690bb2",
   "metadata": {
    "id": "06690bb2",
    "outputId": "5cb8f71b-c726-49d2-b414-c0b2addde4c7"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2)\n",
    "\n",
    "#Build the classifier\n",
    "clf = SVC(kernel='linear', C=10)\n",
    "#Fit the model on the train data\n",
    "clf.fit(X_train, y_train)\n",
    "#Predict the model on test data\n",
    "y_pred_test = clf.predict(X_test)\n",
    "\n",
    "#Find the accuracy of model on test set by comparing the predicted labels with their true values\n",
    "accuracy = np.sum(y_test==y_pred_test) / len(y_test)\n",
    "\n",
    "print(\"Maximal Margin Classifier Accuracy:\", accuracy*100, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1ac15f",
   "metadata": {
    "id": "ba1ac15f"
   },
   "source": [
    "<font face=\"XB Zar\" size=4><div dir=rtl>\n",
    "همانطور که مشاهده میکنید، دقت مدل 52.5% می‌باشد که از مطلوب ما بسیار فاصله دارد. می‌توانیم مرزهای تصمیم‌گیری را نیز مانند بخش قبل ترسیم کنیم: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985e83d0",
   "metadata": {
    "id": "985e83d0",
    "outputId": "44320686-bd76-4b4e-91dc-9a199b3deb62"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "train_plt = plt.scatter(X_train[:, 0], X_train[:, 1], marker='o', c=y_train, cmap='coolwarm', s=100, alpha=0.75)\n",
    "test_plt = plt.scatter(X_test[:, 0], X_test[:, 1], marker=\"x\", c=y_test, cmap='coolwarm', s=100, alpha=0.75)\n",
    "plot_svc_decision_function(clf)\n",
    "plt.xlabel(\"$x_1$\")\n",
    "plt.ylabel(\"$x_2$\")\n",
    "plt.title(\"Decision Boundary and Margins on non-Linearly Separable Data\")\n",
    "plt.legend(handles=train_plt.legend_elements()[0], labels=['Class 1','Class 0'], fontsize='x-large', markerscale=1.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a69219",
   "metadata": {
    "id": "59a69219"
   },
   "source": [
    "<font face=\"XB Zar\" size=4><div dir=rtl> \n",
    "<font color=\"blue\" size=5>راه‌حل: استفاده از Kernel </font><br> <br>\n",
    "همانطور که گفتیم، گاهی ممکن است داده‌های دو کلاس به طور خطی تفکیک پذیر نباشند و میبایست از یک منحنی به عنوان مرز جداساز استفاده کنیم. درواقع باید بتوانیم مرزهای غیرخطی را نیز بدست آوریم تا الگوریتم ما قابلیت استفاده بیشتری داشته باشد. یکی از راه‌های ممکن برای رسیدن به این هدف، گسترش فضای ویژگی ما به ابعاد بالاتر از طریق تبدیل ویژگی‌ها می‌باشد. یعنی با استفاده از ویژگی‌هایی همچون $X_1^2, X_2^2, X_1X_2 , ...$ بتوانیم در فضای ویژگی‌های جدید، مدل خطی خود را برازش کنیم؛ این موضوع منجر به یک مرز غیرخطی در فضای ویژگی‌های اولیه می‌شود.<br>\n",
    "    دیتاست بالا را درنظر بگیرید؛ همانطور که از توزیع داده‎ها مشخص است، یک مرز دایروی می‌تواند داده‌های دو کلاس را از یکدیگر تفکیک کند؛ اما الگوریتم ما محدود به تشخیص مرزهای خطی می‌باشد. حال یک ویژگی جدید به صورت $X_3 = X_1^2+X_2^2$ برای داده‌ها تعریف می‌کنیم."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0674290c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X2d, y = datasets.make_circles(200, factor=0.1, noise=0.1, random_state=1)\n",
    "\n",
    "#Add a new feature as: x_3 = x_1^2+x_2^2\n",
    "X_3 = X2d[:,0]**2 + X2d[:, 1]**2\n",
    "X3d = np.concatenate((X2d, X_3.reshape(-1,1)), axis=1)\n",
    "\n",
    "plt.figure(figsize=(16,6))\n",
    "plt.subplot(1,2,1)\n",
    "scatter2d = plt.scatter(X2d[:, 0], X2d[:, 1], marker='o', c=y, s=100, cmap='coolwarm', alpha=0.75)\n",
    "plt.legend(handles=scatter2d.legend_elements()[0], labels=['Class 1','Class 0'], fontsize='x-large', markerscale=1.5)\n",
    "plt.xlabel(\"$x_1$\", fontsize=15)\n",
    "plt.ylabel(\"$x_2$\", fontsize=15)\n",
    "plt.title(\"Original Feature Space\")\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(12,8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter3D(X3d[:, 0], X3d[:, 1], X3d[:, 2], marker='o', c=y, s=100, cmap='coolwarm', alpha=0.75)\n",
    "ax.view_init(elev=5, azim=30)\n",
    "ax.set_xlabel('$x_1$', fontsize=15)\n",
    "ax.set_ylabel('$x_2$', fontsize=15)\n",
    "ax.set_zlabel('$x_3=x_1^2+x_2^2$', fontsize=15)\n",
    "#plt.legend(labels=['Class 1','Class 0'], fontsize='x-large', markerscale=1.5)\n",
    "ax.set_title('New Feature Space')\n",
    "#ax.legend(['Class 1','Class 0'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c107ac82",
   "metadata": {},
   "source": [
    "<font face=\"XB Zar\" size=4><div dir=rtl> \n",
    "    همانطور که مشاهده می‌کنید، در فضای جدید داده‌های دو کلاس توسط یک صفحه جداپذیر هستند و بهترین صفحه را می‌توانیم با با استفاده از Maximal Margin Classifier یا Soft Margin Classifier بدست آوریم؛ برای مثال یکی از صفحه‌های جداکننده طبق شکل بالا،\n",
    "    $$X_3 = 0.4 \\Rightarrow X_1^2+X_2^2 = 0.4$$\n",
    "    می‌باشد که نشان‌دهنده‌ی مرز دایروی در فضای ویژگی اولیه است؛ پس بدین ترتیب توانستیم به یک منحنی جداکننده در فضای اولیه برسیم.\n",
    "    <br><br> اما همانطور که می‌دانید، پیداکردن این ویژگی‌های جدید که  منجر به یافتن مرز خطی در فضای جدید شوند همیشه قابل انجام نیست؛ زیرا در بسیاری از موارد ابعاد داده‌های ما قابل نمایش نیستند و همچنین یافتن این تبدیل ویژگی‌های مناسب، شاید مانند مثال بالا ساده نباشد. همچنین در صورتی که بخواهیم از ویژگی‌هایی که حاصل بسط چندجمله‌ای هستند استفاده کنیم، افزودن درجات بالاتر باعث می‌شود تعداد ویژگی‌های ما به طرز قابل توجهی افزایش یابد و پیچیدگی مدل را بالا برده و در نتیجه سرعت الگوریتم کاهش می‌یابد. بنابراین باید بتوانیم از یک روش هوشمندانه‌تر و قابل تعمیم برای این مقصد استفاده کنیم. <br><br> \n",
    "    بار دیگر مسئله بهینه‌سازی Soft Margin Classifier را بنویسیم:\n",
    "\\begin{equation*}\n",
    "    \\begin{aligned}\n",
    "        \\min_{w,b,\\xi} \\quad & \\frac{1}{2}||w||_2 ^2+C\\sum_{i=1}^{N}{\\xi_{i}}\\\\\n",
    "        \\textrm{s.t.} \\quad & y^{(i)}(w^T x^{(i)} + b) \\geq 1-\\xi_{i}\\\\\n",
    "        &\\xi_i\\geq0 \\quad \\forall i \\in \\{1,2,...,N\\}   \\\\\n",
    "    \\end{aligned}\n",
    "\\end{equation*}\n",
    "<br> برای هر مسئله بهینه‌سازی محدب می‌توان یک مسئله معادل تعریف کرد که به آن مسئله دوگان یا Dual گفته می‌شود.\n",
    "    (جزئیات مسئله دوگان فراتر از مباحث این درس می‌باشد و در دروس تخصصی مرتبط به بهینه‌سازی، آن را کامل‌تر می‎توانید مطالعه کنید.)\n",
    "     می‌توان نشان داد مسئله دوگان Soft Margin Classifier نیز با استفاده از لاگرانژین(Lagrangian) آن به صورت زیر می‌باشد:\n",
    "\\begin{equation*}\n",
    "    \\begin{aligned}\n",
    "        \\max_{\\alpha} \\quad & \\sum_{i=1}^{N}{\\alpha_i} - \\frac{1}{2} \\sum_{i=1}^{N}{\\sum_{j=1}^{N} {\\alpha_i \\alpha_j y^{(i)} y^{(j)} {x^{{(i)}^T} x^{(j)}}}} \\\\\n",
    "        \\textrm{s.t.} \\quad & \\sum_{i=1}^{N} \\alpha_i y^{(i)} = 0\\\\\n",
    "        &0 \\leq \\alpha_i \\leq C \\quad \\forall i \\in \\{1,2,...,N\\}\n",
    "        \\\\\n",
    "    \\end{aligned}\n",
    "\\end{equation*}\n",
    "<br>  مسئله بالا همچنان یک مسئله بهینه‌سازی محدب می‌باشد و در صورتی که بتوانیم تمام $\\alpha_i$ ها را بدست آوریم، وزن‌های بهینه به صورت زیر بدست خواهند آمد:\n",
    " $$w = \\sum_{i=1}^{N}{\\alpha_i y^{(i)}x^{(i)}}$$\n",
    " یک نکته خیلی جالب این است که پس از بدست آوردن ضرایب $\\alpha_i$، می‌بینیم که بسیاری از آن‌ها صفر هستند و فقط تعدادی از آن‌ها غیرصفر می‌باشند که به نمونه متناظر آن‌ها نیز نمونه Support گفته می‌شود. حالا شاید برای شما واضح‌تر باشد چرا بالاتر نیز اشاره شده بود که فقط تعداد کمی از داده‎ها نمونه‌ی Support هستند و در تعیین مرز تصمیم‌گیری نقش مستقیمی دارند! بایاس این معادله را نیز می‌توان نشان داد که(با استفاده از شرایط KKT) از طریق یکی از نمونه‌های Support بدست می‌آید. در نهایت با پیداکردن بردار وزن‌ها و بایاس، تابع طبقه‌بند Support Vector Classifier ما به صورت زیر خواهد شد: <br> <br>\n",
    "   $$f(x) = w^T x + b \\Rightarrow$$\n",
    "\\begin{equation*}\n",
    "        f(x) = b + \\sum_{i \\in \\mathcal{S}} \\alpha_i y^{(i)}{x^T x^{(i)}} = b + \\sum_{i \\in \\mathcal{S}} \\alpha_i y^{(i)}{\\langle x, x^{(i)} \\rangle}\n",
    "\\end{equation*}\n",
    "که $\\mathcal{S}$ مجموعه Support می‌باشد. از معادله بالا اینگونه برمی‌آید که برای بدست آوردن تابع طبقه‌بند، فقط نیازمند ضرب داخلی بین نمونه‌ها $\\langle x^{(i)}, x^{(j)} \\rangle$ هستیم! \n",
    "    <br> <br>\n",
    "    نگاشت ویژگی \n",
    "    $T: x \\rightarrow \\phi (x) $ را درنظر بگیرید. می‌توان به جای تمام جاهایی که الگوریتم ما به ضرب داخلی \n",
    "    $\\langle x^{(i)}, x^{(j)} \\rangle$\n",
    "    نیاز دارد، ضرب داخلی نگاشت آن‌ها، یعنی\n",
    "    $\\langle \\phi(x^{(i)}), \\phi(x^{(j)}) \\rangle$\n",
    "    را جایگزین نمود و الگوریتم ما دقیقا مانند قبل عمل کند؛ به طوری‌که صرفا ویژگی‌های اولیه تبدیل به ویژگی‌های جدیدی از طریق نگاشت T شده‌اند.\n",
    "    به طور مشخص، ما کرنل (Kernel) متناظر این تبدیل را به صورت زیر تعریف می‌کنیم: <br> <br>\n",
    "    \\begin{equation*}\n",
    "        K(x, x') = \\langle \\phi(x),\\phi(x') \\rangle = \\phi(x)^T \\phi(x') \n",
    "    \\end{equation*}\n",
    "    در نتیجه در این حالت، مدل ما با استفاده از ویژگی‌های جدید $\\phi$ آموزش می‌بیند!\n",
    "    </div>\n",
    "    \n",
    "<font face=\"XB Zar\" size=4><div dir=rtl> <br><br>\n",
    "     حال سوالی که پیش می‌آید این است که کرنل مناسب را چگونه پیدا کنیم؟ <br>\n",
    "    ماتریس کرنل K را به صورت زیر تعریف می‌کنیم:\n",
    "    $$K_{i,j} = K(x^{(i)}, x^{(j)})$$\n",
    "    یعنی در هر درایه آن، حاصل تابع کرنل بین دو نمونه قرار داده شده است. می‌توان نشان داد تابعی برای کرنل معتبر است که منجر به ماتریس کرنل متقارن و نیمه‌مثبت معین شود و این قضیه شرط لازم و کافی است.(قضیه Mercer) <br>\n",
    "    در نهایت نیز با حل مسئله بهینه‌سازی جدید که ضرب‌داخلی‌ها با کرنل جایگزین شده‌اند، تابع طبقه‌بند ما به صورت زیر خواهد شد:\n",
    "\\begin{equation*}\n",
    "    f(x) = b + \\sum_{i \\in \\mathcal{S}} \\alpha_i y^{(i)}{K(x,x^{(i)})}\n",
    "\\end{equation*}\n",
    "<br><br> حال احتمالا باید برایتان مشخص شده باشد که چرا در تابع SVC هایپرپارامتر آن را بر روی <code>linear</code> تنظیم می‌کردیم. در این حالت مدل به سادگی از ضرب داخلی در فضای اولیه ویژگی‌ها استفاده می‌کند و نگاشت خاصی صورت نمی‌گیرد(درواقع نگاشت همانی صورت گرفته است و تغییری در ویژگی‌ها انجام نشده است!).\n",
    "    پس مدل ما محدود به پیداکردن یک طبقه‌بند خطی خواهد شد و درنتیجه همان Soft Margin Classifier می‌باشد. <br> <br>\n",
    "    در ادامه به معرفی دو کرنل معروف و پرکاربرد میپردازیم که در کتابخانه Scikit-learn نیز قابل استفاده هستند:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8047df",
   "metadata": {},
   "source": [
    "<font face=\"XB Zar\" size=4><div dir=rtl> \n",
    "<h4>کرنل چندجمله‌ای (Polynomial Kernel) </h4> <br>\n",
    "    همانطور که در مثال اول این بخش دیدید، اضافه کردن ویژگی مرتبه دوم باعث شد در فضای جدید داده‌های ما جدایی‌پذیر خطی باشند. یک ایده خوب می‌تواند اضافه کردن درجات بالاتر به فضای ویژگی‌های ما باشد که منجر به یک منحنی طبقه‌بند از مرتبه بالا در فضای اولیه شود. با استفاده از کرنل چندجمله‌ای می‌توانیم این کار را بدون آن‌که نیاز باشد خودمان آن‌ها را اضافه کنیم، به فضای ویژگی‌ها اضافه کنیم؛ پس در این حالت افزایش تعداد ویژگی‌ها نیز مشکلی ایجاد نخواهد کرد. درباره پیچیدگی محاسبه و برتری استفاده از کرنل‌ها نسبت به اضافه کردن دستی ویژگی‌های ابعاد بالا در ادامه توضیحاتی خواهیم داشت. <br> <br>\n",
    "    کرنل چندجمله‌ای از مرتبه d به صورت زیر تعریف می‌شود:\n",
    "    $$K(x,y) = (1+\\langle x,y \\rangle)^d$$\n",
    "    <br> برای مثال، کرنل چندجمله‌ای از مرتبه دوم را برای داده‌های دوبعدی درنظر بگیرید:\n",
    "    $$K(x,y) = (1+\\langle x,y \\rangle)^2 = (1+x_1y_1+x_2y_2)^2 \\Rightarrow$$\n",
    "    $$K(x,y) = 1 + 2x_1y_1 + 2x_2y_2 + 2x_1x_2y_1y_2 + x_1^2y_1^2 + x_2^2y_2^2$$ <br>\n",
    "    از طرفی از آنجا که $K(x,y) = \\langle \\phi(x),\\phi(y) \\rangle$ می‌باشد، پس نگاشت انجام‌شده به صورت\n",
    "    $$ \\phi(x) = (1, \\sqrt{2}x_1, \\sqrt{2}x_2, \\sqrt{2}x_1x_2, x_1^2, x_2^2)$$\n",
    "    است. پس بدون آن‌که مستقیما فضای ویژگی جدید را ایجاد کنیم، با استفاده از این کرنل توانسته‌ایم ویژگی‌های دلخواه خود را به مسئله بیافزاییم. آیا از همین مثال ساده برتری \n",
    "    محاسباتی استفاده از کرنل برای شما مشخص است؟ <br><br>\n",
    "     در ادامه بر روی دیتاست زیر، یک‌بار از کرنل مرتبه 3 و بار دیگر از کرنل مرتبه 9 برای آموزش مدل استفاده می‌کنیم و ناحیه تصمیم‌گیری آن‌ها را ترسیم می‌کنیم.\n",
    "    برای این‌کار کافیست هایپرپارامتر <code>kernel</code> را به روی <code>poly</code> تنظیم نماییم و درجه آن را مشخص نماییم. همچنین همانند مسئله Soft Margin Classifier باید پارامتر C را نیز مشخص کنیم.\n",
    "    <br><br> (برای سادگی و نمایش بهتر ناحیه‌های جداشده کلاس‌ها از تابع <code>plot_decision_region</code> از کتابخانه <code>mlxtend</code> استفاده می‌کنیم؛ پس اگر آن را نصب ندارید با استفاده از <code>pip install mlxtend</code> آن را نصب نمایید.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4634fd41",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = datasets.make_moons(n_samples=200,noise=0.15, random_state=0)\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "scatter = plt.scatter(X[:, 0], X[:, 1], marker='o', c=y, s=100, cmap='coolwarm', alpha=0.75)\n",
    "plt.legend(handles=scatter.legend_elements()[0], labels=['Class 1','Class 0'], fontsize='x-large', markerscale=1.5)\n",
    "plt.xlabel(\"$x_1$\")\n",
    "plt.ylabel(\"$x_2$\")\n",
    "plt.title(\"Scatter Plot of the Dataset\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874758a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.plotting import plot_decision_regions\n",
    "plt.figure(figsize=(16, 6))\n",
    "\n",
    "#a 3rd order poly kernel svc\n",
    "clf = SVC(kernel='poly', degree=3, C=10)\n",
    "clf.fit(X, y)\n",
    "plt.subplot(1,2,1)\n",
    "plot_decision_regions(X, y, clf=clf, legend=1)\n",
    "plt.xlabel(\"$x_1$\")\n",
    "plt.ylabel(\"$x_2$\")\n",
    "plt.title(\"Decision Region of 3rd Order Polynomial Kernel\")\n",
    "\n",
    "#a 9th order poly kernel svc\n",
    "clf = SVC(kernel='poly', degree=9, C=10)\n",
    "clf.fit(X, y)\n",
    "plt.subplot(1,2,2)\n",
    "plot_decision_regions(X, y, clf=clf, legend=1)\n",
    "plt.xlabel(\"$x_1$\")\n",
    "plt.ylabel(\"$x_2$\")\n",
    "plt.title(\"Decision Region of 9th Order Polynomial Kernel\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6dbbbd3",
   "metadata": {},
   "source": [
    "<font face=\"XB Zar\" size=4><div dir=rtl> \n",
    "    همانطور که مشاهده می‌کنید، افزودن درجه‌های بالاتر از ویژگی‌ها لزوما منجر به بهبود مرز تصمیم‌گیری نیست و باید بهترین درجه را با استفاده از cross validation پیدا نمود. درواقع اگر مدل ما overfit شده باشد، باید از مرتبه‎های پایین‎تری به عنوان درجه این کرنل استفاده نمود و در صورتی‌که مدل underfit شده باشد، باید پیچدگی آن را با استفاده از افزایش این درجه زیاد نمود.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddfe17eb",
   "metadata": {},
   "source": [
    "<font face=\"XB Zar\" size=4><div dir=rtl> \n",
    "<h4>کرنل RBF (Radial Basis Function)</h4> <br>\n",
    "    دید دیگری که می‌توان به کرنل داشت، این است که کرنل به‌عنوان معیاری از شباهت مورد استفاده قرار گیرد. یعنی از تابعی به عنوان کرنل استفاده کنیم که مشخص‌کننده میزان شباهت و نزدیکی نمونه‌ها به یکدیگر باشد؛ پس هرچه دو نمونه به یکدیگر نزدیک‌تر باشند، این تابع مقدار بزرگتری اخذ کند و اگر دو نمونه از هم دور باشند، مقدار این تابع کوچک‌تر باشد. <br> \n",
    "    یک تابع مناسب برای این مقصد، تابع RBF گوسی (Gaussian Radial Basis Function) می‌باشد که به صورت زیر تعریف می‌شود: <br> <br>\n",
    "    \\begin{equation*}\n",
    "\t\t\tK(x,y) = \\exp(-\\gamma \\lVert x-y \\rVert ^2)\n",
    "\t\\end{equation*}\n",
    "    در شکل زیر این تابع را برای داده‌های دوبعدی ترسیم می‌کنیم:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0qwREIYQ8-t",
   "metadata": {
    "id": "a0qwREIYQ8-t"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "x1 = np.linspace(-3, 3, 100)\n",
    "x2 = np.linspace(-3, 3, 100)\n",
    "X1, X2 = np.meshgrid(x1, x2)\n",
    "\n",
    "gamma = 0.5\n",
    "R = np.exp(-gamma*(X1 ** 2 + X2 ** 2))\n",
    "surf = ax.plot_surface(X1, X2, R, cmap=plt.cm.coolwarm)\n",
    "plt.title('RBF kernel with $\\gamma$ = {:.1f}'.format(gamma), fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5a661b",
   "metadata": {},
   "source": [
    "<font face=\"XB Zar\" size=4><div dir=rtl> \n",
    "    هایپرپارامتر $\\gamma$ تعیین‌کننده میزان واریانس این تابع است؛ یعنی هرچه این پارامتر کمتر باشد، واریانس بیشتر است و به نمونه‌های دورتر وزن نسبتا بیشتری داده خواهد شد و برعکس:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1934ad3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "x1 = np.linspace(-3, 3, 100)\n",
    "x2 = np.linspace(-3, 3, 100)\n",
    "X1, X2 = np.meshgrid(x1, x2)\n",
    "\n",
    "gamma = 0.2\n",
    "R = np.exp(-gamma*(X1 ** 2 + X2 ** 2))\n",
    "surf = ax.plot_surface(X1, X2, R, cmap=plt.cm.coolwarm)\n",
    "plt.title('RBF kernel with $\\gamma$ = {:.1f}'.format(gamma), fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52567c45",
   "metadata": {},
   "source": [
    "<font face=\"XB Zar\" size=4><div dir=rtl> \n",
    "    حال دیتاست <code>make_circles</code> را بار دیگر درنظر بگیرید؛ در صورتی‌که از این تابع به‌عنوان کرنل انتخاب کنیم\n",
    "    (با تنظیم <code>\"kernel=\"rbf</code>)، مرز‌های تصمیم‌گیری خود را می‌توانیم مشاهده کنیم:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "RlrHxGbURpbl",
   "metadata": {
    "id": "RlrHxGbURpbl"
   },
   "outputs": [],
   "source": [
    "X, y = datasets.make_circles(n_samples=200, factor=0.1, noise=0.1, random_state=1)\n",
    "clf = SVC(kernel='rbf', gamma=0.7, C=10)\n",
    "clf.fit(X, y)\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "scatter = plt.scatter(X[:, 0], X[:, 1], c=y, s=50, edgecolors='black', cmap='coolwarm', alpha=0.6)\n",
    "plot_svc_decision_function(clf)\n",
    "plt.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1], s=200, edgecolors='k', facecolors='none')\n",
    "plt.legend(handles=scatter.legend_elements()[0], labels=['Class 1','Class 0'], fontsize='x-large', markerscale=1.5)\n",
    "plt.xlabel(\"$x_1$\", fontsize=15)\n",
    "plt.ylabel(\"$x_2$\", fontsize=15)\n",
    "plt.title(\"Decision Boundary using RBF Kernel\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0906f03",
   "metadata": {},
   "source": [
    "<font face=\"XB Zar\" size=4><div dir=rtl> \n",
    "    برای دیتاست <code>make_moons</code> نیز خواهیم داشت:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e0b7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = datasets.make_moons(n_samples=200,noise=0.15, random_state=0)\n",
    "clf = SVC(kernel='rbf', gamma=0.8, C=5)\n",
    "clf.fit(X, y)\n",
    "\n",
    "plt.figure(figsize=(10,7))\n",
    "plot_decision_regions(X, y, clf=clf, legend=1)\n",
    "plt.xlabel(\"$x_1$\")\n",
    "plt.ylabel(\"$x_2$\")\n",
    "plt.title(\"Decision Region of RBF Kernel\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12bf1492",
   "metadata": {},
   "source": [
    "<font face=\"XB Zar\" size=4><div dir=rtl> \n",
    "    همانطورکه می‌بینید، با استفاده از کرنل RBF مرز تقسیم‌بندی داده‌های دو کلاس بسیار بهتر بدست آمده است و این کرنل معمولا عملکرد بهتری دارد و به صورت پیش‌فرض نیز کرنل مدل SVC بر روی این کرنل تنظیم شده است.\n",
    "    <br> <br> علاوه بر کرنل‌های ذکرشده، کرنل‌های دیگری نیز همچون <code>String kernel</code> وجود دارند، هرچند پرکاربرد و محبوب نیستند و فقط در مواردی خاص مورد استفاده قرار می‌گیرند."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82689f2e",
   "metadata": {},
   "source": [
    "<font face=\"XB Zar\" size=4><div dir=rtl> \n",
    "<h4>پیچدگی محاسبه در SVM</h4> <br>\n",
    "    در ابتدا سعی می‌کنیم با یک مثال ساده نشان دهیم که استفاده از کرنل چه تاثیری بر پیچیدگی محاسبه دارد و برتری استفاده از کرنل به جای گسترش فضای ویژگی‌ها(Feature Expansion) به صورت دستی را مشاهده نماییم.\n",
    "    در واقع نشان می‌دهیم که اگر بتوانیم ماتریس کرنل را به صورت بهینه محاسبه نماییم، خواهیم توانست مدل‌های SVM خود را در فضای ویژگی با ابعاد بالاتر که با $\\phi$ گسترش داده شده است آموزش دهیم، بدون آن‌که نیاز باشد صراحتا بردارهای $\\phi (x)$ را پیدا کنیم! <br>\n",
    "    مثال زیر را درنظر بگیرید. بردارهای $x, y \\in R^p$ هستند و فرض کنید تابع کرنل به صورت زیر تعریف شود:\n",
    "    $$K(x,y) = (x^Ty)^2$$\n",
    "    می‌توانیم تابع کرنل بالا را به صورت زیر بنویسیم:\n",
    "    $$K(x,y) = \\bigg(\\sum_{i=1}^{p}x_i y_i \\bigg) \\bigg(\\sum_{j=1}^{p}x_i y_i \\bigg) $$\n",
    "    $$ = \\sum_{i=1}^{p} \\sum_{j=1}^{p} x_i x_j y_i y_j $$\n",
    "    $$ = \\sum_{i,j=1}^{p} (x_i x_j) (y_i y_j) $$\n",
    "    از طرفی،\n",
    "    $K(x,y) = \\phi(x)^T \\phi(y)$ می‌باشد؛ پس می‌توان تشخیص داد که نگاشت ویژگی‌های $\\phi$ به صورت زیر است: (برای سادگی p=3 قرار داده شده است.)\n",
    "    \\begin{align}\n",
    "    \\phi(x) &= \\begin{bmatrix}\n",
    "           x_1 x_1 \\\\\n",
    "           x_1 x_2 \\\\\n",
    "           x_1 x_3 \\\\\n",
    "           x_2 x_1 \\\\\n",
    "           x_2 x_2 \\\\\n",
    "           x_2 x_3 \\\\\n",
    "           x_3 x_1 \\\\\n",
    "           x_3 x_2 \\\\\n",
    "           x_3 x_3 \\\\\n",
    "         \\end{bmatrix}\n",
    "  \\end{align}\n",
    "    همانطور که می‌بینید، با وجود اینکه محاسبه $\\phi(x)$ با بعد بالا نیازمند $O(p^2)$ زمان است،\n",
    "    محاسبه $K(x,y)$ فقط نیازمند به $O(p)$ زمان دارد که نشان می‌دهد نسبت به بعد اولیه ویژگی‌ها خطی است!\n",
    "    بنابراین با استفاده از این کرنل توانستیم فضای ویژگی گفته شده را با پیچیدگی محاسباتی به مراتب کمتری بسازیم. <br> <br>\n",
    "    در حالت کلی‌تر، می‌توان نشان داد کرنل $K(x,y) = (x^Ty + c)^d$ معادل با نگاشت ویژگی‌ها (Feature Mapping) به فضای ویژگی‌هایی از بعد \n",
    "    $ p+d \\choose d$ خواهد شد که شامل تمام تک‌جمله‌ای‌(monomial)هایی به شکل \n",
    "    $x_{i_1}x_{i_2}...x_{i_k}$ تا مرتبه d می‌باشند.\n",
    "    با این حال، با وجود اینکه داریم در فضایی $O(p^d)$بعدی مدل خود را آموزش می‌دهیم، محاسبه‌ی $K(x,y)$ همچنان تنها نیازمند به $O(p)$ زمان دارد و در نتیجه ما هیچگاه نیاز به تعریف این فضای ویژگی به طور صریح نخواهیم داشت؛ درحالیکه مدل ما به طور ضمنی در حال یادگیری در این فضا می‌باشد!\n",
    "    <br><br><br>\n",
    "    از لحاظ عملیاتی و پیاده‌سازی با استفاده از کتابخانه <code>scikit-learn</code> نیز ذکر چند نکته خالی از لطف نیست. کلاس <code>linearSVC</code> یک کلاس برای پیاده‌سازی SVM خطی می‌باشد و کرنل‌ها را پشتیبانی نمی‌کند؛ اما از لحاظ پیچیدگی محاسباتی نسبت به تعداد داده‌ها و تعداد ویژگی‌ها خطی است و پیچیدگی آن از مرتبه $O(N\\times p)$ می‌باشد. کلاس <code>SVC</code> که بالاتر نیز از آن استفاده نمودیم از کرنل‌ها پشتیبانی می‌کند ولی پیچیدگی محاسباتی آن معمولا بین \n",
    "    $O(N^2 \\times p)$ و $O(N^3 \\times p)$ است که نشان می‌دهد این الگوریتم نسبت به تعداد نمونه‌ها به شدت اثرپذیر است و با افزایش آن‌ها این الگوریتم کند می‌شود. استفاده از این کلاس هنگام داشتن دیتاست‌های پیچیده کوچک یا متوسط مناسب‌تر به نظر می‌رسد. با این وجود، این الگوریتم نسبت به تعداد ویژگی‌ها نیز خطی است.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ef589b",
   "metadata": {},
   "source": [
    "<br>\n",
    "<font face=\"XB Zar\" size=4><div dir=rtl>\n",
    "<center><font color=\"red\" size=\"6\">طبقه‌بند بردار پشتیبان چندکلاسه (Multi-Class SVC)</font> </center> <br> <br>\n",
    "    در تمام مثال‌های بالا هدف ما طبقه‌بندی داده‌های دو کلاس بود و در واقع یک binary classifier داشتیم؛ اما در بسیاری از مسائل تعداد کلاس‌ها بیشتر از دو می‌باشد و باید بتوانیم الگوریتم خود را برای این حالت‌ها نیز استفاده کنیم. برای رسیدن به این مقصود دو روش کلی وجود دارد:\n",
    "    <h3>One-vs-Rest (One-vs-All)</h3> <br>\n",
    "    در این روش، ما نیازمند ابرصفحه‌ای هستیم که داده‌های یک کلاس را از داده‌های سایر کلاس‌ها به طور همزمان جداسازی نماید؛ در واقع با این روش تمام نمونه‌ها در بدست آوردن ابرصفحه بهینه در نظر گرفته خواهند شد؛ به طوریکه داده‌ها به ازای هر classifier به دو دسته تقسیم می‌شوند: یک گروه به ازای داده‌های کلاس مورد نظر و گروه دیگر شامل تمام نقاط سایر کلاس‌ها. بدین ترتیب برای یک مسئله طبقه‌بندی با K دسته، نیازمند K طبقه‌بند هستیم. برای مثال، در شکل زیر خط سبز حاشیه میان داده‌های کلاس سبز را با سایر داده‌ها بیشینه می‌کند.\n",
    "<div> <br>\n",
    "<center>\n",
    "<img src=\"resources/multiclass_classification.jpg\" width=\"500\">\n",
    "</center>\n",
    "</div>    \n",
    "<br>\n",
    "    همانطور که مشاهده می‌کنید، ممکن است در بعضی ناحیه‌ها تداخل پیش آید و هر classifier به نفع کلاس خود رای دهد؛ اما همانطور که قبلا نیز مطرح شده بود، یک معیار خوب جهت بدست آوردن میزان اطمینان از پیش‌بینی طبقه‌بند ما می‌تواند فاصله نمونه از ابرصفحه باشد؛ یعنی داده‌ای که در سمت درست ابرصفحه قرار گیرد، هرچه فاصله بیشتری از ابرصفحه داشته باشد امتیاز بیشتری می‌گیرد و در نتیجه با اطمینان بیشتری می‌توانیم آن نمونه را در کلاس موردنظر طبقه‌بندی کنیم. پس به طور خلاصه روش ما به صورت زیر می‌باشد: <br>\n",
    "    تمام K طبقه‌بندهای 2کلاسه $\\hat{f}_k (x), \\forall k \\in \\{1,2,...,K\\}$ را به صورت یک کلاس در برابر سایر کلاس‌ها فیت نماییم. داده جدید *x را در کلاسی قرار دهیم که $\\hat{f}_k (x^*)$ آن بیشینه باشد.\n",
    "    <h3>One-vs-One</h3> <br>\n",
    "    در این روش، به جای اینکه از K طبقه‌بند برای جداسازی داده‌های یک کلاس از سایر کلاس‌ها استفاده کنیم، نیازمند ابرصفحه بهینه‌ای هستیم که داده‌های یک کلاس را از کلاس دیگر جدا نماید؛ به طوری‌که داده‌های سایر کلاس‌ها را در نظر نگیرد. پس برای هر classifier نیازمند داده‌های دو کلاس هستیم و در کل نیازمند ${K \\choose 2}$ طبقه‌بند خواهیم بود. در نهایت داده‌ی جدید *x به کلاسی تعلق می‌گیرد که بیشترین رای را در بین تمام طبقه‌بندهای 2کلاسه دریافت کرده باشد.\n",
    "    برای مثال در شکل زیر، خط آبی-قرمز سعی در بیشینه‌کردن حاشیه بین کلاس‌های آبی و قرمز دارد، بدون آنکه داده‌های کلاس سبز را در نظر بگیرد.\n",
    "<div> <br>\n",
    "<center>\n",
    "<img src=\"resources/multiclass_classification2.jpg\" width=\"500\">\n",
    "</center>\n",
    "</div>    \n",
    "<br>\n",
    "    در ادامه با استفاده از کتابخانه <code>scikit-learn</code> روش One-vs-One را بر روی دیتاست زیر امتحان می‌کنیم. (برای تغییر نوع طبقه‌بندی کافیست هایپرپارامتر <code>decision_function_shape</code> را بر روی <code>ovo (one-vs-one)</code> یا <code>ovr (one-vs-rest)</code> تنظیم کنیم.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d348927",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "X, y = datasets.make_blobs(\n",
    "    n_samples=300, n_features=2, centers=3, cluster_std=1.5, random_state=10\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "scatter = plt.scatter(X[:, 0], X[:, 1], marker='o', c=y, s=100, cmap='coolwarm', alpha=0.75);\n",
    "plt.xlabel(\"$x_1$\")\n",
    "plt.ylabel(\"$x_2$\")\n",
    "plt.title(\"Scatter Plot of the Dataset\")\n",
    "plt.legend(handles=scatter.legend_elements()[0], labels=['Class 0','Class 1', 'Class 2'], fontsize='x-large', markerscale=2)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f6a6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from mlxtend.plotting import plot_decision_regions\n",
    "\n",
    "plt.figure(figsize=(20,8))\n",
    "\n",
    "#Linear Kernel\n",
    "clf = SVC(kernel='linear', decision_function_shape='ovo')\n",
    "clf.fit(X, y)\n",
    "\n",
    "plt.subplot(1,3,1)\n",
    "plot_decision_regions(X, y, clf=clf, legend=1)\n",
    "plt.xlabel(\"$x_1$\")\n",
    "plt.ylabel(\"$x_2$\")\n",
    "plt.title(\"Decision Region of Linear Kernel\")\n",
    "\n",
    "#RBF Kernel\n",
    "clf = SVC(kernel='rbf', gamma=0.8, C=5, decision_function_shape='ovr')\n",
    "clf.fit(X, y)\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "plot_decision_regions(X, y, clf=clf, legend=1)\n",
    "plt.xlabel(\"$x_1$\")\n",
    "plt.ylabel(\"$x_2$\")\n",
    "plt.title(\"Decision Region of RBF Kernel\")\n",
    "\n",
    "#Polynomial Kernel\n",
    "clf = SVC(kernel='poly', degree=3, decision_function_shape='ovr')\n",
    "clf.fit(X, y)\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "plot_decision_regions(X, y, clf=clf, legend=1)\n",
    "plt.xlabel(\"$x_1$\")\n",
    "plt.ylabel(\"$x_2$\")\n",
    "plt.title(\"Decision Region of Polynomial Kernel with degree=3\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea73a31a",
   "metadata": {},
   "source": [
    "<br>\n",
    "<font face=\"XB Zar\" size=4><div dir=rtl>\n",
    "<center><font color=\"red\" size=\"6\">رگرسیون بردار پشتیبان (Support Vector Regression)</font> </center> <br> <br>\n",
    "به طور کلی SVM یکی از محبوب‌ترین و پرکاربردترین الگوریتم‌های مورد استفاده است که معمولا برای طبقه‌بندی استفاده می‌شود. (Support Vector Classification)\n",
    "    با این وجود، می‌توان از ایده Margin برای مسائل رگرسیون نیز استفاده نمود و بدین ترتیب Support Vector Regression (SVR) تعریف می‌شود. <br> <br>\n",
    "    ایده‌ی مورد استفاده برای SVR، پیداکردن بهترین ابرصفحه‌ای است که همه نمونه‌ها در یک حاشیه با آستانه $\\epsilon$ از این ابرصفحه قرار بگیرند. شکل زیر این ایده را به خوبی توضیح می‌دهد:\n",
    "<div> <br>\n",
    "<center>\n",
    "<img src=\"resources/svr.png\" width=\"500\">\n",
    "</center>\n",
    "</div>    \n",
    "<br><br><br>\n",
    "    بدین ترتیب می‌توان مسئله بهینه‌سازی SVR را به صورت زیر تعریف نمود:\n",
    "\\begin{equation*}\n",
    "    \\begin{aligned}\n",
    "        \\min_{w,b} \\quad & \\frac{1}{2} ||w||_2 ^2\\\\\n",
    "            \\textrm{s.t.} \\quad & |y^{(i)} - (w^T x^{(i)} + b)| \\leq \\epsilon \\quad \\forall i \\in \\{1,2,...,N\\}\\\\\n",
    "    \\end{aligned}\n",
    "\\end{equation*}\n",
    "<br><br>\n",
    "    به دلایل مشابه با حالت Soft Margin Classifier، یعنی حساسیت این ابرصفحه به نمونه‌های Support و نیز احتمال وجود نمونه‌هایی که به دلایلی همچون نویز یا وجود داده‌ی پرت حاشیه زیادی ایجاد می‌کنند، می‌توان از Soft Support Vector Regression نیز استفاده نمود و با اضافه کردن Slack Variables ($\\xi$)، مسئله بهینه‌سازی آن را به صورت زیر تغییر داد:\n",
    "<div> <br>\n",
    "<center>\n",
    "<img src=\"resources/soft_svr.png\" width=\"500\">\n",
    "</center>\n",
    "</div>    \n",
    "<br>\n",
    "\\begin{equation*}\n",
    "    \\begin{aligned}\n",
    "        \\min_{w,b} \\quad & \\frac{1}{2} ||w||_2 ^2 + C\\sum_{i=1}^{N}{\\xi_i}\\\\\n",
    "        \\textrm{s.t.} \\quad & |y^{(i)} - (w^T x^{(i)} + b)| \\leq \\epsilon + \\xi_i \\\\\n",
    "        &\\xi_i \\geq 0 \\quad \\forall i \\in \\{1,2,...,N\\}\\\\\n",
    "    \\end{aligned}\n",
    "\\end{equation*}\n",
    "<br><br>\n",
    "    همانند حالت طبقه‌بندی، در مسئله رگرسیون نیز به طریق مشابه می‌توان از کرنل‌ها استفاده نمود و برای SVR نیز کرنل‌های <code>rbf</code> و <code>polynomial</code> تعریف می‌شود. <br> <br>\n",
    "    در ادامه سعی می‌کنیم با استفاده از <code>sklearn.svm</code> بر روی یک دیتاست یک‌بعدی(جهت نمایش‌دادن) ساختگی، اثر این کرنل‌ها را مقایسه کنیم:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed54ca15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "\n",
    "X = np.sort(5 * np.random.rand(50, 1), axis=0)\n",
    "y = np.sin(X).ravel() #y = sin(x)\n",
    "\n",
    "#Add noise to targets\n",
    "y[::5] += 3 * (0.5 - np.random.rand(10))\n",
    "\n",
    "#Set the epsilon for the width of streets!\n",
    "eps=0.2\n",
    "\n",
    "#Linear Model\n",
    "svr_lin = SVR(kernel='linear', C=1e3, epsilon=eps)\n",
    "y_lin = svr_lin.fit(X, y).predict(X)\n",
    "\n",
    "#RBF Model\n",
    "svr_rbf = SVR(kernel=\"rbf\", C=100, gamma=0.1, epsilon=eps)\n",
    "y_rbf = svr_rbf.fit(X, y).predict(X)\n",
    "\n",
    "#Polynomial Model\n",
    "svr_poly = SVR(kernel=\"poly\", C=100, degree=3, epsilon=eps)\n",
    "y_poly = svr_poly.fit(X, y).predict(X)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "\n",
    "plt.subplot(1,3,1)\n",
    "plt.scatter(X, y, color='darkorange', label='data')\n",
    "plt.plot(X, y_lin, color='k', lw=2)\n",
    "plt.title('Linear Model')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.fill_between(X.ravel(), y_lin - eps, y_lin + eps, edgecolor='none', color='#7D3C4A', alpha=0.3)\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "plt.scatter(X, y, color='darkorange')\n",
    "plt.plot(X, y_rbf, color='k', lw=2)\n",
    "plt.title('RBF Model')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.fill_between(X.ravel(), y_rbf - eps, y_rbf + eps, edgecolor='none', color='#7D3C4A', alpha=0.3)\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "plt.scatter(X, y, color='darkorange', label='data')\n",
    "plt.plot(X, y_poly, color='k', lw=2)\n",
    "plt.title('Polynomial Model with degree=3')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.fill_between(X.ravel(), y_poly - eps, y_poly + eps, edgecolor='none', color='#7D3C4A', alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc86ec2",
   "metadata": {},
   "source": [
    "<font face=\"XB Zar\" size=4><div dir=rtl>\n",
    "    همانطور که مشخص است، با استفاده از کرنل <code>rbf</code> توانسته‌ایم ارتباط بین متغیر و خروجی را بهتر مدل‌سازی نماییم. همچنین باید بتوانیم که مقدار مناسب هایپرپارامترهای هر مدل را نیز بدست آوریم که با استفاده از Cross Validation انجام داده می‌شود و در ادامه توضیح داده خواهد شد."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3952cdf0",
   "metadata": {},
   "source": [
    "<br>\n",
    "<font face=\"XB Zar\" size=4><div dir=rtl>\n",
    "<center><font color=\"red\" size=\"6\">تنظیم هایپرپارامتر‌ها و انتخاب مدل</font> </center> <br> <br>\n",
    "    ابتدا برای آشنایی با موضوع اعتبارسنجی(Validation) به این <a href=\"https://github.com/asharifiz/Introduction_to_Machine_Learning/blob/main/Jupyter_Notebooks/Chapter_03_Train_and_Evaluation/Validation.ipynb\">نوتبوک</a> مراجعه فرمایید. در ادامه ما از دیتاست مصنوعی مارپیچی (<a href=\"https://en.wikipedia.org/wiki/Spiral\">spiral</a>) استقاده میکنیم تا با این مبحث بهتر آشنا شوید ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee8c1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib nbagg\n",
    "\n",
    "from ipywidgets import interact, FloatLogSlider, Checkbox\n",
    "import math\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "\n",
    "def make_spiral(density=1, max_radius=6.5, c=0):\n",
    "    \"\"\" Generate spiral dataset.\n",
    "    \n",
    "    Arguments:\n",
    "        density (int)  : Density of the points\n",
    "        maxRadius (float) : Maximum radius of the spiral\n",
    "        c (int) : Class of this spiral\n",
    "    \n",
    "    Returns:\n",
    "        array: Return spiral data and its class\n",
    "    \"\"\"\n",
    "    \n",
    "    # Spirals data and labels\n",
    "    data, labels = [], []\n",
    "\n",
    "    # Number of interior data points to generate\n",
    "    N = 96 * density \n",
    "\n",
    "    # Generate points\n",
    "    for i in range(0, N):\n",
    "        angle = (i * math.pi) / (16 * density)\n",
    "        # Radius is the maximum radius * the fraction of iterations left\n",
    "        radius = max_radius * ((104 * density) - i) / (104 * density)\n",
    "\n",
    "        # Get x and y coordinates\n",
    "        x = radius * math.cos(angle)\n",
    "        y = radius * math.sin(angle)\n",
    "\n",
    "        data.append([x, y])\n",
    "        labels.append([c])\n",
    "\n",
    "    return data, labels\n",
    "\n",
    "def make_spirals(density=1, max_radius=6.5):\n",
    "    \"\"\" Generate two class spiral dataset.\n",
    "\n",
    "    Arguments:\n",
    "        density (int)  : Density of the points\n",
    "        maxRadius (float) : Maximum radius of the spiral\n",
    "    Returns:\n",
    "        array: Return spirals data and its class\n",
    "    \"\"\"\n",
    "    data , labels = [], []\n",
    "\n",
    "    # First spirals data and class\n",
    "    data1, labels1 = make_spiral(density, max_radius)\n",
    "\n",
    "    # Construct complete two spirals dataset\n",
    "    for d in data1:\n",
    "        data.append(d)  # First spirals coordinate\n",
    "        data.append([-d[0], -d[1]])  # Second spirals coordinate\n",
    "\n",
    "    # Construct complete two spirals classes\n",
    "    for lbl in labels1:\n",
    "        labels.append(lbl)  # First spirals class\n",
    "        labels.append([1])  # Second spirals class\n",
    "\n",
    "    return np.array(data), np.array(labels).ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d451aa2",
   "metadata": {},
   "source": [
    "<font face=\"XB Zar\" size=4><div dir=rtl> \n",
    "    برای تولید این دیتاست از تابع نوشته‌شده  <code>make_spirals</code> استفاده می‌کنیم:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a04a817",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_spirals()\n",
    "\n",
    "# Plot spirals\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, edgecolors='black', cmap=plt.cm.coolwarm, alpha=0.6)\n",
    "plt.title(\"Original Spiral Data\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874db075",
   "metadata": {},
   "source": [
    "<font face=\"XB Zar\" size=4><div dir=rtl> \n",
    "     در این قسمت می‌توان تاثیر هایپرپارامترهای $\\gamma$ و $C$ را مشاهده کنید:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ba30a5",
   "metadata": {},
   "outputs": [],
   "source": [
    " fig, ax = plt.subplots(1)\n",
    "\n",
    "X1, X2 = np.meshgrid(np.linspace(-7, 7, 200), np.linspace(-7, 7, 200))\n",
    "\n",
    "def update_plot(C, gamma, show_svs, show_levels):\n",
    "    ax.clear()\n",
    "    \n",
    "    # create and train SVM classifier\n",
    "    clf = SVC(C=C, gamma=gamma)\n",
    "    clf.fit(X, y)\n",
    "    sv = clf.support_vectors_\n",
    "    \n",
    "    # compute contours\n",
    "    Z = clf.decision_function(np.c_[X1.ravel(), X2.ravel()])\n",
    "    Z = Z.reshape(X1.shape)\n",
    "\n",
    "    # visualize parameter's effect on decision function\n",
    "    ax.contourf(X1, X2, Z, cmap=plt.cm.coolwarm)\n",
    "    \n",
    "    # plot decision boundaries\n",
    "    if show_levels:\n",
    "        ax.contour(X1, X2, Z, linewidths=1, colors='k', levels=[-1, 0, 1], linestyles=['--', '-', '--'])\n",
    "    \n",
    "    # plot data\n",
    "    ax.scatter(X[:, 0], X[:, 1], c=y, edgecolors='black', cmap=plt.cm.coolwarm, alpha=1.0)\n",
    "    \n",
    "    # plot support vectors\n",
    "    if show_svs:\n",
    "        sv = clf.support_vectors_\n",
    "        ax.scatter(sv[:, 0], sv[:, 1], s=150, facecolors='none', edgecolors='k')\n",
    "    \n",
    "    title = \"$\\gamma = 10^{:1.0g}, C = 10^{:1.0g}, SV = {:d}$\"\n",
    "    ax.set_title(title.format(np.log10(gamma), np.log10(C), len(sv)), size='large')\n",
    "    ax.set_xticks(())\n",
    "    ax.set_yticks(())\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "C = FloatLogSlider(value=10, base=10, min=-3, max=5, step=1, description='C:')\n",
    "gamma = FloatLogSlider(value=1, base=10, min=-3, max=5, step=1, description='gamma:')\n",
    "show_svs = Checkbox(value=False, description='Show Support Vectors')\n",
    "show_levels = Checkbox(value=True, description='Show Levels')\n",
    "\n",
    "f = interact(update_plot, C=C, gamma=gamma, show_svs=show_svs, show_levels=show_levels);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43d1c3b",
   "metadata": {},
   "source": [
    "<font face=\"XB Zar\" size=4><div dir=rtl> \n",
    "می‌توانید با تغییر هایپرپارامترهای $C$ و $\\gamma$ در اسلایدبارهای متناظر، تغییر ناحیه طبقه‌بندی را مشاهده کنید و همچنین Support Vectorهای آن را نمایش دهید.\n",
    "    <br> در ادامه به ازای ترکیب‌های مختلف این هایپرپارامترها، ناحیه تقسیم‌بندی دو کلاس را همزمان ترسیم می‌کنیم تا بتوانیم آن‌ها را با یکدیگر مقایسه نماییم:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55691a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "C_range = [0.1, 10, 1000]\n",
    "gamma_range = [0.1, 1, 10]\n",
    "\n",
    "classifiers = []\n",
    "\n",
    "for C in C_range:\n",
    "    for gamma in gamma_range:\n",
    "        clf = SVC(C=C, gamma=gamma)\n",
    "        clf.fit(X, y)\n",
    "        classifiers.append((clf, C, gamma))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed1f447",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "X1, X2 = np.meshgrid(np.linspace(-7, 7, 200), np.linspace(-7, 7, 200))\n",
    "\n",
    "for i, (clf, C, gamma) in enumerate(classifiers):\n",
    "    # evaluate decision function in a grid\n",
    "    Z = clf.decision_function(np.c_[X1.ravel(), X2.ravel()])\n",
    "    Z = Z.reshape(X1.shape)\n",
    "\n",
    "    # visualize decision function for these parameters\n",
    "    plt.subplot(len(C_range), len(gamma_range), i + 1)\n",
    "    plt.title(\"$\\gamma = 10^{%d}, C = 10^{%d}$\" % (np.log10(gamma), np.log10(C)))\n",
    "\n",
    "    # visualize parameter's effect on decision function\n",
    "    plt.contourf(X1, X2, Z, cmap=plt.cm.coolwarm)\n",
    "    plt.contour(X1, X2, Z, linewidths=1, colors='k', levels=[-1, 0, 1], linestyles=['--', '-', '--'])\n",
    "    plt.scatter(X[:, 0], X[:, 1], s=15, c=y, edgecolors='k', cmap=plt.cm.coolwarm, alpha=0.6)\n",
    "    plt.xticks(())\n",
    "    plt.yticks(())\n",
    "    plt.axis('tight')\n",
    "    \n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56350680",
   "metadata": {},
   "source": [
    "<font face=\"XB Zar\" size=4><div dir=rtl>\n",
    "    آیا از شکل‌های بالا اثر این هایپرپارامترها را می‌توانید تشخیص دهید؟ <br>\n",
    "    حال در این قسمت ما سعی می‌کنیم با استفاده از روش k-fold CV به صورت Grid Search بهترین $\\gamma$ و $C$ را پیدا کنیم: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8273e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = np.random.randn(10, 2)\n",
    "\n",
    "cv = KFold(n_splits=5)\n",
    "\n",
    "for train_idx, val_idx in cv.split(X1):\n",
    "    print(\"Train = {} | Validation ={}\".format(train_idx, val_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135b3fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = np.random.randn(9, 2)\n",
    "y1 = np.array([0, 0, 0, 1, 1, 1, 1, 1, 1])\n",
    "\n",
    "cv = KFold(n_splits=3)\n",
    "\n",
    "for trn_idx, val_idx in cv.split(X1, y1):\n",
    "    print(\"Train={} | Valid={} | Labels={}, {}\".format(trn_idx, val_idx, y1[trn_idx], y1[val_idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8300ec70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1: split data to train and validation\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# 2: specify range of hyper-parameters\n",
    "C_range = np.logspace(-3, 5, 9)         # 1e-3, 1e-2, 1e-1, 1e0, 1e1, 1e2, 1e3, 1e4, 1e5\n",
    "gamma_range = np.logspace(-3, 5, 9)     # 1e-3, 1e-2, 1e-1, 1e0, 1e1, 1e2, 1e3, 1e4, 1e5\n",
    "param_grid = dict(gamma=gamma_range, C=C_range)\n",
    "\n",
    "# 3: create classifier\n",
    "grid = GridSearchCV(SVC(), param_grid=param_grid, cv=cv)\n",
    "\n",
    "# 4: Train\n",
    "grid.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49acad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = grid.cv_results_['mean_test_score'].reshape(len(C_range), len(gamma_range))\n",
    "\n",
    "plt.figure(figsize=(7, 7))\n",
    "\n",
    "# Draw heatmap of the validation accuracy as a function of gamma and C\n",
    "import seaborn as sns\n",
    "sns.heatmap(scores, annot=True, cmap='Blues', square=True);\n",
    "\n",
    "plt.xlabel('$\\gamma$')\n",
    "plt.ylabel('C')\n",
    "plt.xticks(np.arange(len(gamma_range)), gamma_range, rotation=45)\n",
    "plt.yticks(np.arange(len(C_range)), C_range, rotation=-45)\n",
    "plt.title('Validation accuracy')\n",
    "plt.show();\n",
    "\n",
    "print(\"The best parameters are %s with a score of %0.2f\" % (grid.best_params_, grid.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9240eabc",
   "metadata": {},
   "source": [
    "<br>\n",
    "<font face=\"XB Zar\" size=4><div dir=rtl>\n",
    "<center><font color=\"red\" size=\"6\">معیارهای ارزیابی مدل‌ها</font> </center> <br> <br>\n",
    "در این بخش، با معیار های مختلف ارزیابی مدل ها در مسائل دسته‌بندی و رگرسیون آشنا می‌شویم. <br> <br>\n",
    "<font color=\"red\" size=5> آشنایی با دیتاست </font> <br> <br>\n",
    "      دیتاستی که در این بخش استفاده خواهد شد، دیتاست MNIST است که شامل 70000 تصویر از عددهای تک‌رقمی دست‌نویس هستند که می‌توانید آن را با استفاده از تابع \n",
    "    <code>fetch_openml</code> دانلود و استفاده نمایید.\n",
    "      <br />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7902b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_openml\n",
    "mnist = fetch_openml('mnist_784', version=1, as_frame=False)\n",
    "mnist.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ec4f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = mnist[\"data\"], mnist[\"target\"]\n",
    "y = y.astype(np.uint8)\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419a22d5",
   "metadata": {},
   "source": [
    "<font face=\"XB Zar\" size=4> <div dir=rtl>\n",
    "    همانطورکه مشخص است داده‌های ما عکس‌های ۲۸ در ۲۸ هستند. در ادامه یکی از نمونه‌ها را نمایش می‌دهیم:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f18140",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_digit(data):\n",
    "    image = data.reshape(28, 28)\n",
    "    plt.imshow(image, cmap = mpl.cm.binary,\n",
    "               interpolation=\"nearest\")\n",
    "    plt.axis(\"off\")\n",
    "  \n",
    "\n",
    "digit = X[0]\n",
    "plot_digit(digit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158ae4ca",
   "metadata": {},
   "source": [
    "<font face=\"XB Zar\" size=4> <div dir=rtl>\n",
    "<font color=\"red\" size=5> آموزش دادن یک Binary Classifier </font> <br><br>\n",
    "    در این بخش یک binary classifier که تشخیص می‌دهد یک تصویر ۵ است یا نه را آموزش دهیم.\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e2561a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "y_train_5 = (y_train == 5)\n",
    "y_test_5 = (y_test == 5)\n",
    "\n",
    "y_train_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470d2a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "svm_clf = SVC(kernel='rbf', C = 0.03, random_state=42)\n",
    "svm_clf.fit(X_train, y_train_5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4418b9c",
   "metadata": {},
   "source": [
    "<font face=\"XB Zar\" size=4><div dir=rtl>\n",
    "می‌توانیم مدل آموزش داده‌شده را روی تصویری که بالاتر نمایش دادیم تست کنیم:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38515f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_clf.predict([digit])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ddf117",
   "metadata": {},
   "source": [
    "<font face=\"XB Zar\" size=4> <div dir=rtl>\n",
    "<font color=\"red\" size=5> محاسبه دقت </font> <br><br>\n",
    "    دقت این classifier را به کمک تابع <code>accuracy_score</code> از ماژول \n",
    "    <code>sklearn.metrics</code>\n",
    "    به دست می‌آوریم: <br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a393f021",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "svm_clf_predictions = svm_clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test_5, svm_clf_predictions)\n",
    "\n",
    "print(\"Accuracy of the 5 classifier is:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff7f3a7",
   "metadata": {},
   "source": [
    "<font face=\"XB Zar\" size=4><div dir=rtl>\n",
    "    به دقت بالای 97 درصد رسیدیم! چقدر خوب! <br> <br>\n",
    "    قبل از اینکه خیلی هیجان زده شوید،‌بگذارید تا یک مدل بسیار ساده را آموزش دهیم تا دقت آن را بسنجیم و با مدل خود مقایسه کنیم.\n",
    "    این مدل ساده به گونه ای کار می‌کند که هر تصویری ببیند، آن را در دسته‌ی \"غیر 5\" پیش‌بینی می‌کند:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569cab71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "class Never5Classifier(BaseEstimator):\n",
    "    def fit(self, X, y=None):\n",
    "        pass\n",
    "    def predict(self, X):\n",
    "        return np.zeros((len(X), 1), dtype=bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7d6f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "never_5_clf = Never5Classifier()\n",
    "never_5_clf_predictions = never_5_clf.predict(X_test)\n",
    "accuracy_score(y_test_5, never_5_clf_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff7a49c",
   "metadata": {},
   "source": [
    "<font face=\"XB Zar\" size=4> <div dir=rtl>\n",
    "خب همانطور که مشخص است، این دسته‌بند به دقت بالای ۹۰ درصد رسید! خیلی هم تعجب برانگیز نیست؛ زیرا فقط حدود ۱۰ درصد از داده ها لیبل شان ۵ بوده است، و آن ها نیز اشتباه تشخیص داده شده اند."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59f7d2d",
   "metadata": {},
   "source": [
    "<font face=\"XB Zar\" size=4> <div dir=rtl>\n",
    "<font color=\"red\" size=5> Confusion Matrix </font> <br><br>\n",
    "    یک روش خیلی بهتر برای ارزیابی مدل،‌نگاه به confusion matrix آن است که به ما نشان می‌دهد که هر دسته چند بار در دسته درست و چند بار در دسته های اشتباه طبقه‌بندی شده است. \n",
    "    هر سطر در این ماتریس نشان‌دهنده‌ی کلاس واقعی و هر ستون نشان‌دهنده کلاسی است که مدل ما پیش‌بینی کرده است. <br> <br>\n",
    "    می‎توان با استفاده از تابع <code>confusion_matrix</code> از ماژول <code>sklearn.metrics</code> این ماتریس را نمایش داد:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97ada49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "svm_confusion_matrix = confusion_matrix(y_test_5, svm_clf_predictions)\n",
    "print(svm_confusion_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d3c711",
   "metadata": {},
   "source": [
    "<font face=\"XB Zar\" size=4> <div dir=rtl>\n",
    "اطلاعاتی که confusion matrix از مدل ما می‌دهد عبارت هستند از:\n",
    "          <ul>\n",
    "              <li>\n",
    "                اندیس [0,0]: 15887 عکسی که لیبل آن ها \"غیر 5\" بوده است به درستی \"غیر 5\" تشخیص داده شده اند. (True Negative)\n",
    "              </li>\n",
    "              <li>\n",
    "                اندیس [0,1]: 6 عکسی که لیبل آن ها \"غیر 5\" بوده است به اشتباه \"5\" تشخیص داده شده اند. (False Positive)\n",
    "              </li>\n",
    "              <li>\n",
    "                اندیس [1,0]: 467 عکسی که لیبل آن ها \"5\" بوده است به اشتباه \"غیر 5\" تشخیص داده شده اند. (False Negative)\n",
    "              </li>\n",
    "              <li>\n",
    "                اندیس [1,1]: 1140 عکسی که لیبل آن ها \"5\" بوده است به درستی \"5\" تشخیص داده شده اند. (True Positive)\n",
    "              </li>\n",
    "          </ul>\n",
    "<br> برای یک دسته بند ایده‌آل، خانه‌های غیرقطری confusion matrix همگی 0 هستند. <br><br>\n",
    "همانطور که متوجه شده اید، confusion matrix اطلاعات زیادی به ما می‌دهد. یکی از آن ها، دقت پیش‌بینی های positive است که به آن precision می‌گوییم:\n",
    "    $$ Precision = \\frac{TP}{TP + FP} $$ <br>\n",
    "    اگر یک پیش بینی انجام دهیم که positive باشد، آنگاه precision برابر با ۱ می‌شود. به همین دلیل این متریک را با متریک دیگری به اسم recall  می‌سنجیم:\n",
    "    $$ Recall (Sensitivity) = \\frac{TP}{TP + FN} $$ <br><br>\n",
    "    در کتابخانه سایکیت-لرن نیز می‌توانیم به آن‌ها از طریق ماژول <code>sklearn.metrics</code> و از طریق توابع <code>precision_score</code> و <code>recall_score</code> دسترسی داشته باشیم:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0470885e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "svm_y_train_pred = svm_clf.predict(X_train)\n",
    "print('Precision =', precision_score(y_train_5, svm_y_train_pred))\n",
    "print('Recall =', recall_score(y_train_5, svm_y_train_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2109057",
   "metadata": {},
   "source": [
    "<font face=\"XB Zar\" size=4><div dir=rtl>\n",
    "دسته‌بند ما الان آنقدر هم خوب بنظر نمی‌آید! این دسته‌بند 70 درصد عکس‌هایی که ۵ بوده است را درست دسته‌بندی کرده است. <br> <br>\n",
    "یک معیار دیگر که برای ارزیابی مدل استفاده می‌شود، F1 score نام دارد که به صورت زیر تعریف می‌شود: <br>\n",
    "    $$ F1\\_score = \\frac{2}{\\frac{1}{precision} + \\frac{1}{recall}} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1cd96c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "print(\"F1_score =\", f1_score(y_train_5, svm_y_train_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb90fd69",
   "metadata": {},
   "source": [
    "<font face=\"XB Zar\" size=4><div dir=rtl>\n",
    "این متریک دسته‌بندهایی که precision و recall نزدیکی دارند را دوست دارد. استفاده از این متریک خیلی وابسته به مسئله است. ممکن است در یک مسئله precision برای ما اهمیت زیادی داشته باشد یا در مساله دیگری بخواهیم recall بالایی داشته باشیم."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be962996",
   "metadata": {},
   "source": [
    "<font face=\"XB Zar\" size=4> <div dir=rtl>\n",
    "<font color=\"red\" size=5> Precision/Recall tradeoff </font> <br><br>\n",
    "    دسته‌بندها،‌برای هر داده که دسته‌بندی می‌کنند یک امتیاز (score) مشخص می‌کنند و بر اساس یک آستانه (threshold) آن را به دسته مثبت یا دسته منفی اختصاص می‌دهند. در مثال‌هایی که بالاتر داشتیم، امتیاز ما درواقع همان مقدار تابع طبقه‌بند $f(x)$ بود. مقدار آستانه نیز برابر با 0 بود و دسته‌بندی با توجه به علامت تابع طبقه‌بند برای هر نمونه استفاده می‌شد؛ اما می‌توان این مقدار آستانه را تغییر داد و در نتیجه، پیش‎بینی‌هایی با امتیاز نزدیک به این آستانه تغییر می‎کنند و متناظرا متریک‌های ما نیز تغییر خواهند نمود. این موضوع منجر به یک tradeoff بین این دو متریک خواهد شد که در ادامه آن را نشان می‌دهیم:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe516d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the scores for train set predictions\n",
    "svm_train_scores = svm_clf.decision_function(X_train)\n",
    "\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "precisions, recalls, thresholds = precision_recall_curve(y_train_5, svm_train_scores)\n",
    "\n",
    "thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2d0c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_precision_recall_vs_threshold(precisions, recalls, thresholds):\n",
    "    plt.plot(thresholds, precisions[:-1], \"b--\", label=\"Precision\", linewidth=2)\n",
    "    plt.plot(thresholds, recalls[:-1], \"g-\", label=\"Recall\", linewidth=2)\n",
    "    plt.legend(loc=\"center right\", fontsize=16) \n",
    "    plt.xlabel(\"Threshold\", fontsize=16)        \n",
    "    plt.grid(True)                             \n",
    "    plt.axis([-2.5, 2.5, 0, 1])             \n",
    "\n",
    "\n",
    "\n",
    "recall_90_precision = recalls[np.argmax(precisions >= 0.90)]\n",
    "threshold_90_precision = thresholds[np.argmax(precisions >= 0.90)]\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8, 4))                                                                  \n",
    "plot_precision_recall_vs_threshold(precisions, recalls, thresholds)\n",
    "plt.plot([threshold_90_precision, threshold_90_precision], [0., 0.9], \"r:\")                 \n",
    "plt.plot([-2.5, threshold_90_precision], [0.9, 0.9], \"r:\")                                \n",
    "plt.plot([-2.5, threshold_90_precision], [recall_90_precision, recall_90_precision], \"r:\")\n",
    "plt.plot([threshold_90_precision], [0.9], \"ro\")                                             \n",
    "plt.plot([threshold_90_precision], [recall_90_precision], \"ro\")                             \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "393d7a93",
   "metadata": {},
   "source": [
    "<font face=\"XB Zar\" size=4><div dir=rtl>\n",
    "همانطور که از نمودار بالا مشخص است،‌با جابجا کردن این threshold، نمی‌توان هم precision و هم recall را زیاد کرد. پس اینجا مسئله‌ای مطرح می‌شود که  threshold را در کجا قرار دهیم،‌که جواب این سوال،‌وابسته به مسئله ما متقاوت است و باید با توجه به هدف خود آن را تنظیم کنیم. <br><br>\n",
    "    همچنین می‌توان نمودار precision برحسب recall را کشید و از روی آن تصمیم گیری کرد:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181e652a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_precision_vs_recall(precisions, recalls):\n",
    "    plt.plot(recalls, precisions, \"b-\", linewidth=2)\n",
    "    plt.xlabel(\"Recall\", fontsize=16)\n",
    "    plt.ylabel(\"Precision\", fontsize=16)\n",
    "    plt.axis([0, 1, 0, 1])\n",
    "    plt.grid(True)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plot_precision_vs_recall(precisions, recalls)\n",
    "plt.plot([recall_90_precision, recall_90_precision], [0., 0.9], \"r:\")\n",
    "plt.plot([0.0, recall_90_precision], [0.9, 0.9], \"r:\")\n",
    "plt.plot([recall_90_precision], [0.9], \"ro\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cdb510e",
   "metadata": {},
   "source": [
    "<font face=\"XB Zar\" size=4><div dir=rtl>\n",
    "حال بطور مثال در مساله ای که داریم،‌می‌خواهیم حتما به recall برابر با ۹۰ درصد برسیم. پس برای پیداکردن آستانه موردنظر به صورت زیر عمل می‌کنیم:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb465c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_90_recall = thresholds[np.argmax(recalls >= 0.90)]\n",
    "print(\"Threshold value for 90% recall =\", threshold_90_recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b535b6f",
   "metadata": {},
   "source": [
    "<font face=\"XB Zar\" size=4><div dir=rtl>\n",
    "    می‌توانیم نتیجه بالا را نیز چک کنیم:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d77ef74",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_y_train_pred_90 = (svm_train_scores >= threshold_90_precision)\n",
    "print(\"Precision =\", precision_score(y_train_5, svm_y_train_pred_90))\n",
    "print(\"Recall =\", recall_score(y_train_5, svm_y_train_pred_90))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834d4cd5",
   "metadata": {},
   "source": [
    "<font face=\"XB Zar\" size=4><div dir=rtl>\n",
    "همانطور که مشاهده می‌شود، به ازای این مقدار آستانه معیارهای مورد نظر ما مقادیر به مراتب بهتری نسبت به آستانه اولیه 0 دارند. <br><br>\n",
    "<font color=\"red\" size=5> منحنی ROC (ROC Curve) </font> <br><br>\n",
    "    این ابزار نیز برای دسته‌بند های باینری زیاد استفاده می‌شود و شباهت زیادی به precision-recall curve دارد، با این تفاوت که recall را بر حسب false positive rate نمایش می‌دهد:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7f8fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_train_5, svm_train_scores)\n",
    "\n",
    "def plot_roc_curve(fpr, tpr, label=None):\n",
    "    plt.plot(fpr, tpr, linewidth=2, label=label)\n",
    "    plt.plot([0, 1], [0, 1], 'k--') # dashed diagonal\n",
    "    plt.axis([0, 1, 0, 1])                                    \n",
    "    plt.xlabel('False Positive Rate (Fall-Out)', fontsize=16) \n",
    "    plt.ylabel('True Positive Rate (Recall)', fontsize=16)    \n",
    "    plt.grid(True)                                            \n",
    "\n",
    "plt.figure(figsize=(8, 6))                                    \n",
    "plot_roc_curve(fpr, tpr)\n",
    "fpr_90 = fpr[np.argmax(tpr >= recall_90_precision)]           \n",
    "plt.plot([fpr_90, fpr_90], [0., recall_90_precision], \"r:\")   \n",
    "plt.plot([0.0, fpr_90], [recall_90_precision, recall_90_precision], \"r:\") \n",
    "plt.plot([fpr_90], [recall_90_precision], \"ro\")              \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d376dfc",
   "metadata": {},
   "source": [
    "<font face=\"XB Zar\" size=4> <div dir=rtl>\n",
    "در این جا هم یک tradeoff بین recall و false positive rate داریم.\n",
    "خط‌چین نمایش داده شده برای یک دسته‌بند کاملا رندوم است و یک دسته‌بند خوب،‌ در دورترین حالت نسبت به آن خط چین قرار می‌گیرد."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c057fc1",
   "metadata": {},
   "source": [
    "<font face=\"XB Zar\" size=4> <div dir=rtl>\n",
    "<font color=\"red\" size=5> مساحت زیر منحنی (AUC) </font> <br><br>\n",
    "یک راه دیگر برای ارزیابی و مقایسه مدل‌ها، مقایسه کردن مساحت زیر نمودار ROC Curve است که به این متریک AUC گفته می‌شود.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c1c24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "print(\"AUC =\", roc_auc_score(y_train_5, svm_train_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d278960",
   "metadata": {},
   "source": [
    "<font face=\"XB Zar\" size=4><div dir=rtl>\n",
    "این متریک برای دسته‌بند SVM ما عدد بالایی است! <br> <br>\n",
    "    بنابراین با آشنایی با این معیارها، متوجه می‌شویم که صرفا توجه کردن به دقت مدل شاید کافی نباشد و بهتر است که سایر متریک‌های گفته‌شده را نیز بررسی نماییم."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

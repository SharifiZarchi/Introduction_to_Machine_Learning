{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8cee9c79-ae63-4675-87df-255510403af3",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\" align=\"center\">\n",
    "<font face=\"XB Zar\" size=5>\n",
    "    <font face=\"IranNastaliq\" size=5>\n",
    "      به نام خدا\n",
    "    </font>\n",
    "    <br>\n",
    "    <font size=3>\n",
    "      دانشگاه صنعتی شریف - دانشکده مهندسی کامپیوتر\n",
    "    </font>\n",
    "    <br>\n",
    "    <font color=blue size=5>\n",
    "      مقدمه‌ای بر یادگیری ماشین\n",
    "    </font>\n",
    "    <br>\n",
    "    <hr/>\n",
    "    <font color=red size=6>\n",
    "      فصل دوم:  کاهش بعد داده‌ها\n",
    "    </font>\n",
    "    <br>\n",
    "      نویسنده:‌ سینا مظاهری\n",
    "    <hr>\n",
    "<br>\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba14717-4269-4af7-9901-62b59bd8a751",
   "metadata": {},
   "source": [
    "<span style=\"font-family: XB Zar; font-size: 5; \"><div dir=rtl><meta charset=\"UTF-8\">\n",
    "    <span style=\"color: red; font-size: large; \">1.مقدمه</span>\n",
    "    <br>\n",
    "    در مباحثی از یادگیری نظارت نشده که بیشتر بر روی کاهش بعد داده‌ها تمرکز دارند؛ یک هدف اصلی وجود دارد:  <b> پیدا کردن متغیر‌های تصادفی نهان پیوسته در ساختار مجموعه دادگان داده شده </b>.<br>\n",
    "    این عمل درست بر‌خلاف خوشه‌بندی می‌باشد که در آن ما به دنبال متغیر‌های نهان گسسته می‌باشیم. (پیدا کردن تعداد خوشه‌ها، بررسی رفتار خوشه‌ها، ...)<br>\n",
    "    روش‌های متعددی به منظور کاهش بعد داده‌ها وجود دارند. در این بخش به یکی از پرکاربردترین و معروف‌ترین آن‌ها می پردازیم.<br>\n",
    "    کاهش بعد‌ داده‌ها عموما به سه منظور صورت می‌گیرد:\n",
    "    <ul>\n",
    "        <li>نمایش داده‌ها یا همان Data Visualization برای مجموعه دادگانی که معمولا ابعاد بسیار بالایی دارند یعنی تعداد ابعاد از تعداد داده‌ها بسیار بسیار بیشتر است $(D \\gg N)$</li>\n",
    "        <li>در بعضی از روش‌های یادگیری ماشین نظارت شده که هدف به دست آوردن یک مقدار پیوسته یا گسسته است، کاهش بعد داده‌های ورودی، تاثیر بسزایی برروی عملکرد و نتایج خروجی آن می گذارد.</li> \n",
    "        <li>به منظور فشرده‌سازی داده‌ها استفاده می‌گردد.</li>\n",
    "    </ul>\n",
    "</div></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25574fb-0e37-45aa-99b8-db7fe7ff82e4",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\"><table><tr>\n",
    "<td> <img src=\"https://www.sqlservercentral.com/wp-content/uploads/legacy/c2362193dd68a81c6774bb9668f1178fc2cf4ad4/32947.jpg\" width=\"500\" style=\"vertical-align:middle\">\n",
    " <figcaption><center>شکل1. فشرده‌سازی تصویر</center></figcaption>\n",
    " </td>\n",
    "<td> <img src=\"https://www.researchgate.net/profile/Diego-Peluffo/publication/325363944/figure/fig1/AS:631693463539712@1527618866278/Dimensionality-reduction-effect-over-an-3D-artificial-Swiss-roll-manifold-the-2D.png\" width=\"500\" style=\"vertical-align:middle\">\n",
    " <figcaption><center>شکل2. کاهش بعد داده‌های سه بعدی به دو بعدی</center></figcaption> </td>\n",
    "</tr></table>\n",
    "    </div>\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ebc8de2-1826-41ac-a8a4-ba878c473e49",
   "metadata": {},
   "source": [
    "<span style=\"font-family: XB Zar; font-size: medium; \"><div dir=rtl><meta charset=\"UTF-8\">\n",
    "    <span style=\"color: red; font-size: large; \">2. تحلیل مولفه‌های اصلی یا PCA</span>\n",
    "    <br>\n",
    "    یکی از پرکاربردترین روش‌هایی که در کاهش بعد داده‌های یک مجموعه دادگان استفاده می‌شود\n",
    "    <b>تحلیل مولفه‌های اصلی </b>\n",
    "    می‌باشد. این روش از دو منظر قابل تعریف است که هر دو معادل یکدیگر می‌باشند:\n",
    "    <ul>\n",
    "  <li>نگاشت متعامد مجموعه دادگان به یک فضای برداری خطی با ابعاد کمتر از فضای برداری اصلی خود دادگان به گونه‌ای که واریانس دادگان تبدیل یافته بیشینه شود. به این زیرفضای کاهش یافته،\n",
    "      <b>زیرفضای اصلی (principal subspace)</b>\n",
    "      نیز گفته می‌شود.</li>\n",
    "  <li>نگاشت خطی‌ای می باشد که میانگین مربعات فاصله اقلیدسی بین داده اصلی و داده تبدیل یافته را کمینه می‌کند. </li>\n",
    "    </ul>\n",
    "</div></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fdd67a9-aea0-4d11-ac85-5f58269e2356",
   "metadata": {},
   "source": [
    "<div>\n",
    "<center>\n",
    "<img src=\"http://alexhwilliams.info/itsneuronalblog/img/pca/pca_two_views.png\" width=\"500\">\n",
    "     <figcaption><center>شکل3. کمینه‌سازی خطای مربعات در مقایسه با بیشینه‌سازی واریانس</center></figcaption> </td>\n",
    "</center>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e77070-39ad-4805-bcdd-d2074c89f3b2",
   "metadata": {},
   "source": [
    "<span style=\"font-family: XB Zar; font-size: medium; \"><div dir=rtl><meta charset=\"UTF-8\">\n",
    "    ما در این درسنامه به بررسی تعریف اول یعنی بیشینه سازی واریانس می پردازیم و بررسی درستی تعریف دوم را به خواننده واگذار می کنیم.\n",
    "</div></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5339da8d-2ac7-43aa-8109-cdeaf40ade04",
   "metadata": {},
   "source": [
    "<span style=\"font-family: XB Zar; font-size: medium; \"><div dir=rtl><meta charset=\"UTF-8\">\n",
    "    <span style=\"color: red; font-size: large; \">1.2. بررسی تعریف بیشینه سازی واریانس در تحلیل مولفه های اصلی</span>\n",
    "    <br>\n",
    "    ابتدا فرص کنید که مجموعه دادگان\n",
    "    $ \\mathcal{D} = \\{ \\mathbf{x}_{1}, \\mathbf{x}_{2}, \\dots, \\mathbf{x}_{ِN}\\} $\n",
    "    به ما داده شده است. هر یک از\n",
    "    $ \\mathbf{x}_{i} $\n",
    "    یک بردار داده\n",
    "    $ D $\n",
    "     بعدی می‌باشد. حال ما این بردار‌ها را در درون یک ماتریس قرار می‌دهیم تا ماتریس دادگان یا \n",
    "    <b>Design Matrix</b>\n",
    "    $ \\mathbf{X}_{N \\times D} $\n",
    "    را تشکیل دهیم.\n",
    "    <br>\n",
    "    <br>\n",
    "    $$\\mathbf{X}_{N \\times D}=\n",
    "\\begin{bmatrix}\n",
    "\\mathbf{x}^\\top_{1} \\\\\n",
    "\\mathbf{x}^\\top_{2} \\\\\n",
    "\\vdots \\\\\n",
    "\\mathbf{x}^\\top_n \\\\\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    " x_{1}^{[1]} & x_{1}^{[2]} & \\cdots & x_{1}^{[D]} \\\\\n",
    " x_{2}^{[1]} & x_{2}^{[2]} & \\cdots & x_{2}^{[D]} \\\\\n",
    "\\vdots & \\vdots & \\cdots & \\vdots \\\\\n",
    " x_{N}^{[1]} & x_{N}^{[2]} & \\cdots & x_{N}^{[D]} \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "<br>\n",
    "    ابتدا به بررسی میانگین نمونه‌ای و کوواریانس نمونه‌ای این ماتریس دادگان می‌پردازیم و سپس زیر فضای اصلی را میابیم \n",
    "</div></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bac21f2-1e28-4dd6-b697-94b43704a873",
   "metadata": {},
   "source": [
    "<span style=\"font-family: XB Zar; font-size: medium; \"><div dir=rtl><meta charset=\"UTF-8\">\n",
    "    <span style=\"color: red; font-size: large; \">2.2. بررسی تعریف میانگین نمونه‌ای و واریانس نمونه‌ای ماتریس دادگان</span>\n",
    "    <br>\n",
    "    میانگین نمونه ای ماتریس دادگان را با\n",
    "    $\\bar{\\mathbf{x}}$\n",
    "    نمایش می دهیم و طبق تعریف خواهیم داشت:\n",
    "    <br>\n",
    "    $$\n",
    "    \\begin{split}\n",
    "    \\bar{\\mathbf{x}}_{D \\times 1} = \\frac{1}{N}\\mathbf{X}^\\top \\mathbf{1}\\quad, \\quad \\mathbf{1}_{N \\times 1} = \\begin{bmatrix}\n",
    "1 \\\\\n",
    "1 \\\\\n",
    "\\vdots \\\\\n",
    "1 \\\\\n",
    "\\end{bmatrix}\n",
    "    \\end{split}\n",
    "    $$\n",
    "    <br>\n",
    "    ‌برای ماتریس کوواریانس نمونه‌ای که آن را با\n",
    "    $\\mathbf{S}$\n",
    "    ابتدا تعریف می‌کنیم\n",
    "    $\\mathbf{T} = \\mathbf{X} - \\mathbf{1}\\bar{\\mathbf{x}}^{\\top}$\n",
    "    و\n",
    "    نیز خواهیم داشت:\n",
    "    <br>\n",
    "    <br>\n",
    "     $$\n",
    "    \\mathbf{S}_{D \\times D} =   \\frac{1}{N - 1}\\mathbf{T}^\\top\\mathbf{T} = \\frac{1}{N - 1}(\\mathbf{X} - \\mathbf{1}\\bar{\\mathbf{x}}^{\\top})^\\top(\\mathbf{X} - \\mathbf{1}\\bar{\\mathbf{x}}^{\\top}) = \\frac{1}{N - 1}\\displaystyle\\sum_{i = 1}^{N}(\\mathbf{x}_{i} - \\bar{\\mathbf{x}})(\\mathbf{x}_{i} - \\bar{\\mathbf{x}})^\\top\n",
    "    $$\n",
    "</div></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8a806c-0d35-4945-ae77-ba0ee73af0b2",
   "metadata": {},
   "source": [
    "<span style=\"font-family: XB Zar; font-size: medium; \"><div dir=rtl><meta charset=\"UTF-8\">\n",
    "    چند نکته در مورد ماتریس کوواریانس نمونه‌ای\n",
    "    $ \\mathbf{S} $:\n",
    "    <ul>\n",
    "        <li>این ماتریس به وضوح یک ماتریس\n",
    "            <b> متقارن</b>\n",
    "        می‌باشد زیرا داریم:\n",
    "        $ \\mathbf{S}^{\\top} = \\mathbf{S} $\n",
    "        </li>\n",
    "        <li>\n",
    "            طبق قضیه‌ای در جبر خطی، هر ماتریس متقارن\n",
    "            ،$ D \\times D $،\n",
    "            $D$\n",
    "            مقدار ویژه حقیقی به شکل\n",
    "            $ \\lambda_{1}, \\lambda_{2}, \\dots, \\lambda_{D} $\n",
    "            دارد که متناظر آن‌ها بردار ویژه‌ای وجود دارد به شکل\n",
    "            $\\{ \\mathbf{q}_{1}, \\mathbf{q}_{1}, \\dots, \\mathbf{q}_{D} \\}$\n",
    "که \n",
    "            <b>متعامد نرمال</b>\n",
    "             می‌باشند. نکته‌ای که وجود دارد آن است که بردارهای ویژه متعامد هستند اما لزوما نرم واحد ندارند. ما می‌توانیم آن را نرمال کنیم و با قرار دادن آن‌ها در ستون های ماتریس\n",
    "            $ \\mathbf{Q} $\n",
    "            ، بردار‌های ویژه متعامد نرمال را به دست بیاوریم.\n",
    "        </li>\n",
    "        <li>\n",
    "            طبق قضیه ای در جبر خطی، هر ماتریس متقارن یک ماتریس قطری شدنی است یعنی چنین تجزیه‌ای برای آن وجود دارد:\n",
    "            <br>\n",
    "            <br>\n",
    "            $$\\begin{split}\n",
    "            \\mathbf{S} = \\mathbf{Q}\\mathbf{\\Lambda}\\mathbf{Q}^{-1} = \\mathbf{Q}\\mathbf{\\Lambda}\\mathbf{Q}^{\\top} = \\displaystyle\\sum_{i = 1}^{D} \\lambda_{i}\\mathbf{q}_{i}\\mathbf{q}_{i}^{\\top}, \\quad \\mathbf{\\Lambda} = diag(\\lambda_{1}, \\lambda_{2}, \\dots, \\lambda_{D}), \\quad \\mathbf{Q}= \\begin{bmatrix}\n",
    "\\mathbf{q}_{1} & \\mathbf{q}_{2} & \\dots & \\mathbf{q}_{D}\n",
    "\\end{bmatrix}\n",
    "            \\end{split}\n",
    "            $$\n",
    "        </li>\n",
    "    </ul>\n",
    "</div></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90f5171-29fc-4360-8894-4b26044fd508",
   "metadata": {},
   "source": [
    "<span style=\"font-family: XB Zar; font-size: medium; \"><div dir=rtl><meta charset=\"UTF-8\">\n",
    "    <span style=\"color: red; font-size: large; \">3.2. بررسی زیرفضای اصلی دادگان تبدیل یافته</span>\n",
    "    <br>\n",
    "    ابتدا فرض می‌کنیم که می‌خواهیم بعد داده‌ها را به مقدار\n",
    "    $M$\n",
    "    به گونه ای که\n",
    "    $M < D$\n",
    "    می‌باشد؛ کاهش دهیم. این فضای برداری را\n",
    "    $\\mathbf{W}$\n",
    "    می‌نامیم و داریم\n",
    "    $ \\mathbf{W} \\subseteq \\mathbb{R}^{D}$.\n",
    "    برای این کار ابتدا فرض می‌کنیم که پایه‌های متعامد نرمال تشکیل دهنده این فضا به شکل \n",
    "    $\\mathbf{u}_{1}, \\mathbf{u}_{2}, \\dots, \\mathbf{u}_{M}$\n",
    "    می‌باشند. هر کدام از این پایه‌های برداری\n",
    "    $\\mathbf{u}_{i}$،\n",
    "    $D \\times 1$\n",
    "    ‌بوده که داده‌‌ای چون \n",
    "    $\\mathbf{x}$\n",
    "    را به فضای مقصد می‌نگارند. اگر فرض کنیم که با قرار دادن این پایه‌ها در درون ستون‌های یک ماتریس چون \n",
    "    $ \\mathbf{U} $\n",
    "    ماتریس نگاشت به دست می‌آید؛ در اینصورت بردار داده در فضای تبدیل یافته \n",
    "    $ M $\n",
    "    بعدی را با \n",
    "    $\\mathbf{x}^{*}$\n",
    "    نشان می‌دهیم و خواهیم داشت:\n",
    "    <br>\n",
    "    <br>\n",
    "    $$\\begin{split}\n",
    "    \\mathbf{x}^{*} = \\mathbf{U}^{\\top}\\mathbf{x}, \\quad \\mathbf{U} = \\begin{bmatrix}\n",
    "\\mathbf{u}_{1} & \\mathbf{u}_{2} & \\dots & \\mathbf{u}_{M}\n",
    "\\end{bmatrix}\n",
    "    \\end{split}\n",
    "    $$\n",
    "    <br>\n",
    "    <br>\n",
    "    حال اگر فرض کنیم که می‌خواهیم این موضوع را در مورد کل\n",
    "    $ N $\n",
    "    داده ماتریس دادگان تعمیم دهیم ابتدا این ماتریس دادگان را به ماتریس دادگان \n",
    "    $ \\mathbf{T} $\n",
    "    که میانگین تجربی 0 به ازای تمامی ویژگی‌هایشان دارند تبدیل می‌کنیم به گونه‌ای که\n",
    "    $ \\mathbf{T} = \\mathbf{X} - \\mathbf{1}\\bar{\\mathbf{x}}^{\\top}$ \n",
    "    و \n",
    "    سپس ماتریس دادگان در فضای جدید را به شکل \n",
    "    $ \\mathbf{X}^{*} $\n",
    "    تعریف می‌کنیم و نیز خواهیم داشت:\n",
    "    <br>\n",
    "    <br>\n",
    "    $$\n",
    "    \\mathbf{X}^{*} = \\mathbf{T}\\mathbf{U} $$\n",
    "    حال از آنجایی که نیاز است تا واریانس داده‌ها در هر یک از \n",
    "    $M$\n",
    "     بعد بیشینه شود. باید ابتدا بردار میانگین داده‌های تبدیل یافته و همچنین ماتریس کوواریانس آن‌ها را به دست بیاوریم \n",
    "    این مقادیر را به ترتیب با \n",
    "    $ \\bar{\\mathbf{x}}^{*} $\n",
    "    و\n",
    "    $ \\mathbf{S}^{*} $\n",
    "    نشان می‌دهیم و طبق تعریفی که قبلا ارائه دادیم؛ داریم:\n",
    "    <br>\n",
    "    <br>\n",
    "    $$ \n",
    "    \\bar{\\mathbf{x}}^{*} = \\frac{1}{N} (\\mathbf{X}^{*})^{\\top} \\mathbf{1} = \\frac{1}{N} (\\mathbf{T}\\mathbf{U})^{\\top} \\mathbf{1} = \\frac{1}{N} (\\mathbf{U}^{\\top}\\mathbf{T}^{\\top}) \\mathbf{1} =  \\mathbf{U}^{\\top}(\\frac{1}{N}\\mathbf{T}^{\\top} \\mathbf{1}) = \\mathbf{U}^{\\top}\\mathbf{0} = \\mathbf{0}$$\n",
    "    <br>\n",
    "    $$\n",
    "    \\mathbf{S}^{*} =\n",
    "    \\frac{1}{N - 1}\\big(\\mathbf{X}^{*} - \\mathbf{1}(\\bar{\\mathbf{x}}^{*})^{\\top}\\big)^\\top\\big(\\mathbf{X}^{*} - \\mathbf{1}(\\bar{\\mathbf{x}}^{*})^{\\top}\\big) =\n",
    "    \\frac{1}{N - 1}\\big(\\mathbf{U}^{\\top}\\mathbf{T}^{\\top}\\big)\\big(\\mathbf{T}\\mathbf{U}\\big) =  \n",
    "    \\mathbf{U}^{\\top}\\big(\\frac{1}{N - 1}\\mathbf{T}^{\\top}\\mathbf{T}\\big)\\mathbf{U}\n",
    " = \\mathbf{U}^{\\top}\\mathbf{S}\\mathbf{U}\n",
    "    $$\n",
    "    <br>\n",
    "ما می‌دانیم که واریانس هر بعد (می توان هر بعد را به نوعی محور اعداد حقیقی فرض کرد که واریانس نمونه‌ای \n",
    "    $ S_{i} $\n",
    "    برای آن تعریف می‌شود)\n",
    "    بر روی درایه‌های قطر اصلی ماتریس\n",
    "        $ \\mathbf{S}^{*} $\n",
    "    واقع شده‌است. به منظور بیشینه‌سازی هر واریانس نمونه‌ای، حاصل جمع آن‌ها را بیشینه می‌کنیم:\n",
    "    <br>\n",
    "    <br>\n",
    "    $$\\DeclareMathOperator{\\Tr}{Tr} \\max_{S_1, S_2, \\dots, S_{M}} S_1 + S_2 + \\dots + S_{M} \\equiv  \\max_{S_1, S_2, \\dots, S_{M}} \\Tr(\\mathbf{S^{*}}) \\equiv \\max_{S_1, S_2, \\dots, S_{M}} \\Tr(\\mathbf{U}^{\\top}\\mathbf{S}\\mathbf{U})$$\n",
    "    <br>\n",
    "    چون حاصل جمع، تابعی خطی از تک تک واریانس‌ها می‌باشد؛ بیشینه کردن آن نسبت به واریانس‌ها، بیشینه کردن هر یک از واریانس‌ها را تضمین می‌کند. ما می‌دانیم که \n",
    "    واریانس هر بعد چون \n",
    "    $ S_{i} $\n",
    "    از مولفه‌های تبدیل یافته هر داده توسط پایه متناظر آن بعد یعنی\n",
    "    $ \\mathbf{u}_{i} $\n",
    "    بدست می‌آید\n",
    "    پس در واقع\n",
    "    $S^{*}_{i} = \\mathbf{u}_{i}^{\\top} \\mathbf{S} \\mathbf{u}_{i} $\n",
    "    و از آنجایی که ماتریس \n",
    "    $ \\mathbf{S}$\n",
    "    از روی داده‌ها حساب شده است پس متغیر‌های برداری \n",
    "    $ \\mathbf{u}_{i} $\n",
    "    در مسئله بهینه‌سازی استفاده شده و مقدار حاصل جمع باید نسبت به آن‌ها بهینه شود. چون این بردارها متعامد نرمال هستند پس داریم\n",
    "    $\\mathbf{U}^{\\top} \\mathbf{U} = \\mathbf{I}$\n",
    "    و مسئله بهینه‌سازی به شکل زیر خواهد شد:\n",
    "    </div></span>\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded6de1c-1f9c-4272-920e-9275a6173a78",
   "metadata": {},
   "source": [
    "<span style=\"font-family: XB Zar; font-size: medium; \"><div dir=rtl><meta charset=\"UTF-8\">\n",
    "    <br>\n",
    "    $$\\DeclareMathOperator{\\Tr}{Tr} \\begin{aligned}\n",
    "\\max_{\\mathbf{U}} \\quad & \\Tr(\\mathbf{U}^{\\top}\\mathbf{S}\\mathbf{U})\\\\\n",
    "\\textrm{subject to} \\quad & \\mathbf{U} \\in \\mathbb{R}^{D \\times M},\\\\\n",
    "  & \\mathbf{U}^{\\top}\\mathbf{U} = \\mathbf{I}\\\\\n",
    "\\end{aligned}$$\n",
    "    <br>\n",
    "     ثابت می‌کنیم که\n",
    "    اگر \n",
    "    $\\lambda_{1} \\geq \\lambda_{2} \\geq \\dots \\geq \\lambda_{D}$\n",
    "    مقادیر ویژه ماتریس\n",
    "    $ \\mathbf{S} $\n",
    "    باشند؛ آنگاه بیشینه عبارت بالا برابر\n",
    "    $ \\lambda_{1} + \\lambda_{2} + \\dots + \\lambda_{M} $\n",
    "    می‌باشد یا به عبارتی حاصل جمع \n",
    "    $M$\n",
    "    مقدار ویژه بزرگ \n",
    "    ماتریس\n",
    "    $ \\mathbf{S} $\n",
    "    می شود.\n",
    "    در واقع این مقادیر ویژه همان بیشینه مقدار واریانس در هر یک از \n",
    "    $ M $\n",
    "    بعد می‌باشند\n",
    "    $S_{1}^{max} = \\lambda_{1}, S_{2}^{max} = \\lambda_{2}, \\dots, S_{M}^{max} = \\lambda_{M} $\n",
    "    و این زمانی به دست می‌آید که \n",
    "    ماتریس\n",
    "    $\\mathbf{U}$\n",
    "    ماتریس نگاشت متعامد و شامل بردارهای ویژه \n",
    "    $\\mathbf{U} = \\begin{bmatrix} \\mathbf{q_{1}} & \\mathbf{q_{2}} & \\cdots & \\mathbf{q_{M}} \\end{bmatrix}$\n",
    "    نظیر مقادیر ویژه\n",
    "    $ \\lambda_{1}, \\lambda_{2}, \\dots, \\lambda_{M} $\n",
    "    باشد.\n",
    "    </div></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29fde405-e67c-483d-94ee-1a2067601a18",
   "metadata": {},
   "source": [
    "<span style=\"font-family: XB Zar; font-size: medium; \"><div dir=rtl><meta charset=\"UTF-8\">\n",
    "    اثبات:\n",
    "    <br>\n",
    "    ابتدا فرض می کنیم که\n",
    "    $ \\mathbf{u}_{1}, \\mathbf{u}_{2}, \\dots, \\mathbf{u}_{M} $\n",
    "    بردار های ستونی، ستون های ماتریس\n",
    "    $ \\mathbf{U} $\n",
    "     می باشند. از آنجایی که گفتیم برای هر ماتریس متقارن یک تجزیه به شکل\n",
    "    $ \\sum_{j = 1}^{D} \\lambda_{j}\\mathbf{q}_{j}\\mathbf{q}_{j}^{\\top} $\n",
    "    وجود دارد در نتیجه عبارت\n",
    "    $\\DeclareMathOperator{\\Tr}{Tr} \\Tr(\\mathbf{U}^{\\top}\\mathbf{S}\\mathbf{U})$\n",
    "    به شکل زیر می شود:\n",
    "    <br>\n",
    "    <br>\n",
    "    $$\n",
    "    \\DeclareMathOperator{\\Tr}{Tr} \\Tr(\\mathbf{U}^{\\top}\\mathbf{S}\\mathbf{U}) = \\displaystyle\\sum_{i = 1}^{M} \\mathbf{u}_{i}^{\\top}\\mathbf{S}\\mathbf{u}_{i} =\n",
    "    \\displaystyle\\sum_{i = 1}^{M} \\mathbf{u}_{i}^{\\top}\\Big(\\displaystyle\\sum_{j = 1}^{D} \\lambda_{j}\\mathbf{q}_{j}\\mathbf{q}_{j}^{\\top}\\Big)\\mathbf{u}_{i} = \n",
    "    \\displaystyle\\sum_{i = 1}^{M}\\displaystyle\\sum_{j = 1}^{D} \\lambda_{j} \\mathbf{u}_{i}^{\\top}\\mathbf{q}_{j}\\mathbf{q}_{j}^{\\top}\\mathbf{u}_{i} = \n",
    "    \\displaystyle\\sum_{i = 1}^{M}\\displaystyle\\sum_{j = 1}^{D} \\lambda_{j} \\big(\\mathbf{q}_{j}^{\\top}\\mathbf{u}_{i}\\big)^{2} =\n",
    "    \\displaystyle\\sum_{j = 1}^{D} \\lambda_{j}\\displaystyle\\sum_{i = 1}^{M} \\big(\\mathbf{q}_{j}^{\\top}\\mathbf{u}_{i}\\big)^{2} =\n",
    "    \\displaystyle\\sum_{j = 1}^{D} \\lambda_{j}c_{j}\n",
    "    $$\n",
    "    <br>\n",
    "    در عبارت بالا مقدار\n",
    "    $c_{j}$\n",
    "    برابر\n",
    "    $c_{j} = \\sum_{i = 1}^{M} \\big(\\mathbf{q}_{j}^{\\top}\\mathbf{u}_{i}\\big)^{2}$\n",
    "    می‌باشد. ابتدا ثابت می‌کنیم که مقدار \n",
    "    $c_{j}$\n",
    "    بین 0 و 1 می‌باشد یعنی \n",
    "    $0 \\leq c_{j} \\leq 1$\n",
    "    و ثانیا ثابت می‌کنیم که \n",
    "    $\\sum_{j = 1}^{D} c_{j} = M$.\n",
    "    <br>\n",
    "    می‌دانیم که به وضوح\n",
    "    $ c_{j} \\geq 0 $\n",
    "    برقرار است زیرا مجموع مقادیر نامنفی می‌باشد\n",
    "    حال برای اثبات \n",
    "    $ c_{j} \\leq 1 $\n",
    "    مقادیر برداری \n",
    "    $\\mathbf{u}_{1}, \\mathbf{u}_{2}, \\dots,  \\mathbf{u}_{M}$\n",
    "    را به پایه‌های متعامد نرمال\n",
    "    $\\mathbf{u}_{1}, \\mathbf{u}_{2}, \\dots,  \\mathbf{u}_{D}$\n",
    "    فضای برداری\n",
    "    $\\mathbb{R}^{D} $\n",
    "    گسترده می‌کنیم و داریم:\n",
    "    <br>\n",
    "    <br>\n",
    "    $$\n",
    "    c_{j} = \\displaystyle\\sum_{i = 1}^{M}\\big(\\mathbf{q}_{j}^{\\top}\\mathbf{u}_{i}\\big)^{2} \\leq \\displaystyle\\sum_{i = 1}^{D}\\big(\\mathbf{q}_{j}^{\\top}\\mathbf{u}_{i}\\big)^{2} = \n",
    "    \\| \\mathbf{q}_{j}^{\\top} \\mathbf{U} \\|_{2}^{2} =\n",
    "    (\\mathbf{q}_{j}^{\\top} \\mathbf{U})(\\mathbf{q}_{j}^{\\top} \\mathbf{U})^{\\top}\n",
    "    = \\| \\mathbf{q}_{j} \\|_{2}^{2} = 1\n",
    "    $$\n",
    "    <br>\n",
    "    و چون \n",
    "    $\\mathbf{q}_{1}, \\mathbf{q}_{2}, \\dots,  \\mathbf{q}_{D}$\n",
    "    پایه‌های متعامد نرمالی برای فضای برداری \n",
    "    $\\mathbb{R}^{D} $\n",
    "    هستند؛ نیز داریم:\n",
    "    <br>\n",
    "    <br>\n",
    "    $$\\displaystyle\\sum_{j = 1}^{D} c_{j} = \\displaystyle\\sum_{j = 1}^{D}\\displaystyle\\sum_{i = 1}^{M}\\big(\\mathbf{q}_{j}^{\\top}\\mathbf{u}_{i}\\big)^{2} =\n",
    "    \\displaystyle\\sum_{i = 1}^{M}\\displaystyle\\sum_{j = 1}^{D}\\big(\\mathbf{q}_{j}^{\\top}\\mathbf{u}_{i}\\big)^{2} =\n",
    "    \\displaystyle\\sum_{i = 1}^{M}\\| \\mathbf{u}_{i} \\|_{2}^{2} =\n",
    "    \\displaystyle\\sum_{i = 1}^{M}1 = M$$\n",
    "    <br> \n",
    "     بنابراین \n",
    "    عبارت \n",
    "    $ \\sum_{j = 1}^{D} \\lambda_{j}c_{j}$\n",
    "    زمانی بیشینه میشود که با داده شدن مقادیر ویژه، هر یک از ضرایب آن‌ها بیشینه شود و مقدار 1 را به خود بگیرد از طرفی چون رابطه تساوی\n",
    "    $ \\sum_{j = 1}^{D} c_{j} = M$\n",
    "    باید برقرار باشد خواهیم داشت:\n",
    "    <br>\n",
    "    <br>\n",
    "    $$\n",
    "    \\begin{split}\n",
    "    c_{1} = c_{2} =  \\dots = c_{M} = 1, \\quad c_{M+1} = c_{M+2} =  \\dots = c_{D} = 0\n",
    "    \\end{split}\n",
    "    $$\n",
    "    <br>\n",
    "    در نتیجه تساوی \n",
    "    $Span[\\mathbf{q}_{1}, \\mathbf{q}_{2}, \\dots,  \\mathbf{q}_{M}] = Span[\\mathbf{u}_{1}, \\mathbf{u}_{2}, \\dots,  \\mathbf{u}_{M}]$\n",
    "    باید برقرار باشد \n",
    "    و در نتیجه مقادیر برداری \n",
    "    $\\mathbf{q}_{1}, \\mathbf{q}_{2}, \\dots,  \\mathbf{q}_{M}$\n",
    "    پایه‌هایی برای فضای مقصد می‌باشند که در ستون‌های ماتریس \n",
    "    $\\mathbf{U}$\n",
    "    قرار گرفته‌اند.\n",
    "    <br>\n",
    "    <br>\n",
    "    خلاصه و جمع‌بندی:\n",
    "    <br>\n",
    "     برای پیداکردن مجموعه دادگان تبدیل یافته با بعد کمتر به کمک روش تحلیل مولفه‌های اصلی باید مراحل زیر را طی کنیم. فرض می‌کنیم که مجموعه دادگان\n",
    "    $ \\mathbf{X}_{N \\times D}$\n",
    "    که \n",
    "    $D$\n",
    "    بعدی است داده شده و می‌خواهیم آن را به \n",
    "    $\\mathbf{X}^{*}_{N \\times M}$\n",
    "    تبدیل کنیم که \n",
    "    $ M $\n",
    "    بعدی است:\n",
    "    <ol>\n",
    "  <li>ابتدا ماتریس کوواریانس نمونه‌ای \n",
    "        $ S $\n",
    "      را طبق فرمول \n",
    "      $\\mathbf{S}_{D \\times D} = \\frac{1}{N - 1}\\mathbf{T}^\\top \\mathbf{T} $\n",
    "        بدست می‌آوریم که در آن\n",
    "      $\\mathbf{T} = \\mathbf{X} - \\mathbf{1}\\bar{\\mathbf{x}}^{\\top}$\n",
    "      و \n",
    "      $\\bar{\\mathbf{x}}_{D \\times 1} = \\frac{1}{N}\\mathbf{X}^\\top \\mathbf{1} $\n",
    "      می‌باشد\n",
    "        </li>\n",
    "  <li>مقادیر ویژه\n",
    "      $\\lambda_{1}, \\lambda_{2}, \\dots, \\lambda_{M}$\n",
    "       و بردار‌های ویژه نظیر آن‌ها \n",
    "        $\\mathbf{q}_{1}, \\mathbf{q}_{2}, \\dots,  \\mathbf{q}_{M}$\n",
    "       را برای ماتریس\n",
    "      $\\mathbf{S}$\n",
    "       می‌یابیم\n",
    "        </li>\n",
    "  <li>برای کاهش بعد داده‌ها به \n",
    "        $M$\n",
    "        بعد، ابتدا این مقادیر ویژه را به شکل نزولی مرتب می‌کنیم\n",
    "        $\\lambda_{1} \\geq \\lambda_{2} \\geq \\dots \\geq \\lambda_{D}$\n",
    "        سپس \n",
    "        $M$\n",
    "        تا از بزرگترین آن‌ها را به شکل \n",
    "        $\\lambda_{1} \\geq \\lambda_{2} \\geq \\dots \\geq \\lambda_{M}$\n",
    "        بر می‌گزینیم و بردار‌های ویژه نظیر آن‌ها را انتخاب می‌کنیم\n",
    "        $\\mathbf{q}_{1}, \\mathbf{q}_{2}, \\dots,\\mathbf{q}_{M} $\n",
    "        </li>\n",
    "        <li>\n",
    "        مقادیر ویژه \n",
    "        $\\lambda_{1}, \\lambda_{2}, \\dots, \\lambda_{M}$\n",
    "        به ترتیب بیشینه  واریانس نمونه‌ای دادگان تبدیل یافته در هر یک از \n",
    "            $ M $\n",
    "            بعد جدید خواهند بود و با قرار دادن بردار‌های ویژه \n",
    "            $\\mathbf{q}_{1}, \\mathbf{q}_{2}, \\dots,\\mathbf{q}_{M}$\n",
    "            در ستون‌های ماتریس \n",
    "            $ \\mathbf{U}_{D \\times M} $\n",
    "            به شکل \n",
    "            $\\mathbf{U}_{D \\times M} = \\begin{bmatrix} \\mathbf{q}_{1}& \\mathbf{q}_{2}& \\cdots&\\mathbf{q}_{M}\\end{bmatrix}$\n",
    "            ؛ ماتریس نگاشت را می‌سازیم.\n",
    "        </li>\n",
    "        <li>\n",
    "          در نهایت با اعمال نگاشت به ماتریس دادگان طبق رابطه\n",
    "        $\\mathbf{X}^{*} = \\mathbf{T}\\mathbf{U}$\n",
    "        ماتریس \n",
    "            $\\mathbf{X}^{*}$\n",
    "        را بدست می‌اوریم.\n",
    "        </li>\n",
    "    </ol>\n",
    "    </div></span>\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2d8ee5-66f8-4c74-a765-b418923e6089",
   "metadata": {},
   "source": [
    "<span style=\"font-family: XB Zar; font-size: medium; \"><div dir=rtl><meta charset=\"UTF-8\">\n",
    "    <span style=\"color: red; font-size: large; \"> 4.2. بررسی مجموعه دادگان  Iris </span>\n",
    "    <br>\n",
    "    برای مشاهده عملکرد PCA به منظور کاهش بعد داده‌ها ، مجموعه دادگان Iris را در نظر می‌گیریم. از آنجایی که داده‌های این مجموعه 4 بعدی هستند؛ قصد داریم تا آن‌ها را به 2 و 3 بعد کاهش دهیم.\n",
    "    ابتدا توصیفی از ویژگی‌های این مجموعه دادگان ارائه می‌دهیم:\n",
    "    <ul>\n",
    "        <li><b>Sepal Length: </b>\n",
    "      این مقدار طول کاسبرگ را نشان می‌دهد.</li>\n",
    "        <li><b> Sepal Width:</b>\n",
    "      این مقدار عرض کاسبرگ را نشان می‌دهد.</li>\n",
    "        <li><b>Petal Length:</b>\n",
    "      این مقدار طول گلبرگ را نشان می‌دهد.</li>\n",
    "        <li><b>Petal Width:</b>\n",
    "      این مقدار عرض گلبرگ را نشان می‌دهد.</li>\n",
    "        <li><b>Species:</b>\n",
    "      این مقدار نوع گونه را نشان می‌دهد که می تواند از 3 نوع \n",
    "           Setosa, Virginica یا Versicolor باشد</li>\n",
    "</ul>   \n",
    "    </div></span>\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32b9b44-671f-4c7c-a9b7-8c7984cb78c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7a6c21-09b3-45e4-b177-5dac7b643c6c",
   "metadata": {},
   "source": [
    "<span style=\"font-family: XB Zar; font-size: medium; \"><div dir=rtl><meta charset=\"UTF-8\">\n",
    "    <br>\n",
    "    بعد از آنکه کتابخانه‌های مورد نظر را import کردیم مجموعه دادگان را ابتدا بارگذاری می‌کنیم. \n",
    "    </div></span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8525cc5-7416-4669-9f93-c645f6c3700c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"iris.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea43cb40-a74a-4391-ac7e-adc8c7d5be14",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:, 0:4]\n",
    "print(X)\n",
    "X=X.to_numpy()\n",
    "N, D = X.shape\n",
    "y = df.iloc[:, 4].to_numpy().reshape((N,))\n",
    "print(\"The Design Matrix Shape is\", X.shape, \"That is N =\", N, \"and D =\", D)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e659b1-1330-46cb-94ac-af25470584bc",
   "metadata": {},
   "source": [
    "<span style=\"font-family: XB Zar; font-size: medium; \"><div dir=rtl><meta charset=\"UTF-8\">\n",
    "    <br>\n",
    "    طبق آنچه که گفته شد ابتدا میانگین نمونه‌ای یعنی \n",
    "    $\\bar{\\mathbf{x}}$\n",
    "    و سپس ماتریس کوواریانس نمونه‌ای \n",
    "    $\\mathbf{S}$\n",
    "    را\n",
    "    حساب می‌کنیم\n",
    "    </div></span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24195c0c-c653-4869-ade2-a603c1be0b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ones = np.ones((N, 1), dtype=X.dtype)\n",
    "x_bar = 1 / N * (X.transpose().dot(ones))\n",
    "T = X - ones.dot(x_bar.transpose())\n",
    "S = 1 / (N - 1) * T.transpose().dot(T)\n",
    "print(\"The Sample Covariance Matrix is:\\n\", S)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed095bf0-219d-4f61-b6c0-48495580c87e",
   "metadata": {},
   "source": [
    "<span style=\"font-family: XB Zar; font-size: medium; \"><div dir=rtl><meta charset=\"UTF-8\">\n",
    "    <br>\n",
    "    حال مقادیر ویژه و بردار‌های ویژه را حساب می‌کنیم. این کار را با استفاده از تابع \n",
    "   np.linalg.eig()\n",
    "    حساب می‌کنیم. دقت داریم که این تابع خود مقادیر ویژه را به شکل نزولی مرتب می‌کند\n",
    "    </div></span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc9acdf-93a8-4e82-a6e3-7a198d36eb77",
   "metadata": {},
   "outputs": [],
   "source": [
    "eigenvalues, eigenvectors = np.linalg.eig(S)\n",
    "print(\"The Eigenvalues are: \", eigenvalues)\n",
    "print(\"The Eigenvectors Matrix which each of them is a column-vector:\\n\", eigenvectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb35d4d-fad7-41ed-88b9-005e8c9673ed",
   "metadata": {},
   "source": [
    "<span style=\"font-family: XB Zar; font-size: medium; \"><div dir=rtl><meta charset=\"UTF-8\">\n",
    "    <br>\n",
    "    با انتخاب 3 مقدار ویژه بزرگتر، 3 بردار‌ ویژه متناظر با آن را به عنوان پایه‌ای برای فضای مقصد درنظر می‌گیریم و با انجام تبدیل مورد نظر داده‌های ما 3 بعدی شده و در شکل زیر نتایج را می‌بینیم:\n",
    "    </div></span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac60432-89fe-4f31-a7fa-f14594795fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "U = eigenvectors[:, 0:3]\n",
    "X_star_3D = T.dot(U)\n",
    "df_3D = pd.DataFrame({'PC1': X_star_3D[:, 0], 'PC2': X_star_3D[:, 1], 'PC3': X_star_3D[:, 2], 'species': y})\n",
    "fig = px.scatter_3d(df_3D, x='PC1', y='PC2', z='PC3',\n",
    "              color='species')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb65d41c-d9b4-496e-8cd8-924a3409108c",
   "metadata": {},
   "source": [
    "<span style=\"font-family: XB Zar; font-size: medium; \"><div dir=rtl><meta charset=\"UTF-8\">\n",
    "    <br>\n",
    "    در این قسمت با انتخاب 2 بردار ویژه، فضای مقصد را 2 بعدی در نظر گرفته و تبدیل را انجام می‌دهیم:  \n",
    "    </div></span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d009a8c4-51db-4ec1-87f0-9f04d0367e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "U = eigenvectors[:, 0:2]\n",
    "X_star_2D = T.dot(U)\n",
    "df_2D = pd.DataFrame({'PC1': X_star_2D[:, 0], 'PC2': X_star_2D[:, 1], 'species': y})\n",
    "fig = px.scatter(df_2D, x='PC1', y='PC2',\n",
    "              color='species')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48da5802-d45d-4c0f-ab32-5297c96f95da",
   "metadata": {},
   "source": [
    "<span style=\"font-family: XB Zar; font-size: medium; \"><div dir=rtl><meta charset=\"UTF-8\">\n",
    "    <span style=\"color: red; font-size: large; \"> 5.2. تحلیل مولفه‌های اصلی افزایشی یا Incremental PCA </span>\n",
    "    <br>\n",
    "    یکی از مشکلات بزرگ روش قبلی در PCA این آست که کل مجموعه دادگان را به عنوان ورودی خود می‌گیرد. این موضوع در عمل برای مجموعه دادگان حجیم با تعداد داده‌های بسیار زیاد با مشکلاتی همچون زمان محاسبات زیاد همراه می‌شود. یکی از راه حل‌های این مشکل تقسیم‌بندی کردن مجموعه دادگان به دسته‌های کوچک و اعمال PCA به هریک از این دسته‌هاست.\n",
    "    این روش\n",
    "    فقط تخمین‌های واریانس‌های مولفه و نویز را ذخیره می‌کند، . \n",
    "    این عمل به شکل فزاینده، نرخ واریانس را بروزرسانی می‌کند \n",
    "    به همین دلیل است که استفاده از حافظه به تعداد نمونه‌ها در هر دسته بستگی دارد، نه به تعداد نمونه‌هایی که در مجموعه داده پردازش می‌شوند. \n",
    "   در این قسمت از کتابخانه sklearn برای بررسی یک نمونه مسئله در این مورد استفاده می‌کنیم.\n",
    "    </div></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6571d3-a561-4d85-b10c-c2fa75692d82",
   "metadata": {},
   "source": [
    "<span style=\"font-family: XB Zar; font-size: medium; \"><div dir=rtl><meta charset=\"UTF-8\">\n",
    "    <br>\n",
    "    تعداد دسته‌های خود را 10تایی در نظر می‌گیریم و طبق آن خواهیم داشت:\n",
    "    </div></span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2b0674-5ce7-4a07-9f25-6b22a5c1e34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn import IncrementalPCA\n",
    "\n",
    "batch_number = 10\n",
    "incremental_pca = IncrementalPCA(n_components=2)\n",
    "for X_batch in np.array_split(X, batch_number):\n",
    "    incremental_pca.partial_fit(X_batch)\n",
    "X_reduced = incremental_pca.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c081c93d-bb7c-4319-9c28-25831bf6a380",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2D = pd.DataFrame({'PC1': X_reduced[:, 0], 'PC2': X_reduced[:, 1], 'species': y})\n",
    "fig = px.scatter(df_2D, x='PC1', y='PC2',\n",
    "              color='species')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ac25c4-d7d6-40d2-a947-8a11f6042440",
   "metadata": {},
   "source": [
    "<span style=\"font-family: XB Zar; font-size: medium; \"><div dir=rtl><meta charset=\"UTF-8\">\n",
    "    <span style=\"color: red; font-size: large; \"> 6.2. تحلیل مولفه‌های اصلی تصادفی یا Randomized PCA </span>\n",
    "    <br>\n",
    "    نوعی از الگوریتم PCA است که در آن به شکل تصادفی تقریبی از \n",
    "    $d$\n",
    "    مولفه اصلی را پیدا می‌کند\n",
    "    اگر فرض کنیم که مجموعه دادگان ما \n",
    "    $ N \\times D $\n",
    "    باشد؛ به گونه‌ای که \n",
    "    $ N $\n",
    "    تعداد دادگان و \n",
    "    $ D $\n",
    "    بعد داده‌ها باشد.\n",
    "    و داشته باشیم\n",
    "    $max = \\max(N, D)$\n",
    "    و \n",
    "    $min = \\min(N, D) $\n",
    "    پیچیدگی زمانی الگوریتم تحلیل مولفه‌های اصلی تصادفی به شکل\n",
    "    $O(max^{2}d)$\n",
    "    خواهد بود این درحالی است که خود الگوریتم اصلی پیچیدگی زمانی \n",
    "    $O(max^{2}min)$\n",
    "    دارد\n",
    "    حال یک مثال و کاربرد آن را به وسیله sklearn می بینیم:\n",
    "    </div></span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8088f7-5a9a-407b-9451-81b5d067a6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.datasets import fetch_lfw_people\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.utils.fixes import loguniform\n",
    "\n",
    "\n",
    "# %%\n",
    "# Download the data, if not already on disk and load it as numpy arrays\n",
    "\n",
    "lfw_people = fetch_lfw_people(min_faces_per_person=70, resize=0.4)\n",
    "\n",
    "# introspect the images arrays to find the shapes (for plotting)\n",
    "n_samples, h, w = lfw_people.images.shape\n",
    "\n",
    "# for machine learning we use the 2 data directly (as relative pixel\n",
    "# positions info is ignored by this model)\n",
    "X = lfw_people.data\n",
    "n_features = X.shape[1]\n",
    "\n",
    "# the label to predict is the id of the person\n",
    "target_names = lfw_people.target_names\n",
    "n_classes = target_names.shape[0]\n",
    "print(\"Total dataset size:\")\n",
    "print(\"n_samples: %d\" % n_samples)\n",
    "print(\"n_features: %d\" % n_features)\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "# %%\n",
    "# Compute a PCA (eigenfaces) on the face dataset (treated as unlabeled\n",
    "# dataset): unsupervised feature extraction / dimensionality reduction\n",
    "\n",
    "n_components = 150\n",
    "\n",
    "print(\n",
    "    \"Extracting the top %d eigenfaces from %d faces\" % (n_components, X.shape[0])\n",
    ")\n",
    "t0 = time()\n",
    "pca = PCA(n_components=n_components, svd_solver=\"randomized\", whiten=True).fit(X)\n",
    "print(\"done in %0.3fs\" % (time() - t0))\n",
    "\n",
    "eigenfaces = pca.components_.reshape((n_components, h, w))\n",
    "\n",
    "print(\"Projecting the input data on the eigenfaces orthonormal basis\")\n",
    "t0 = time()\n",
    "X_pca = pca.transform(X)\n",
    "print(\"done in %0.3fs\" % (time() - t0))\n",
    "\n",
    "\n",
    "\n",
    "# %%\n",
    "# Qualitative evaluation of the predictions using matplotlib\n",
    "\n",
    "\n",
    "def plot_gallery(images, titles, h, w, n_row=3, n_col=4):\n",
    "    \"\"\"Helper function to plot a gallery of portraits\"\"\"\n",
    "    plt.figure(figsize=(1.8 * n_col, 2.4 * n_row))\n",
    "    plt.subplots_adjust(bottom=0, left=0.01, right=0.99, top=0.90, hspace=0.35)\n",
    "    for i in range(n_row * n_col):\n",
    "        plt.subplot(n_row, n_col, i + 1)\n",
    "        plt.imshow(images[i].reshape((h, w)), cmap=plt.cm.gray)\n",
    "        plt.title(titles[i], size=12)\n",
    "        plt.xticks(())\n",
    "        plt.yticks(())\n",
    "\n",
    "\n",
    "# %%\n",
    "# plot the gallery of the most significative eigenfaces\n",
    "\n",
    "eigenface_titles = [\"eigenface %d\" % i for i in range(eigenfaces.shape[0])]\n",
    "plot_gallery(eigenfaces, eigenface_titles, h, w)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1b5f2c-dc36-48e6-9cf6-986c9c2b975f",
   "metadata": {},
   "source": [
    "<span style=\"font-family: XB Zar; font-size: medium; \"><div dir=rtl><meta charset=\"UTF-8\">\n",
    "    <span style=\"color: red; font-size: large; \"> 7.2. تحلیل مولفه‌های اصلی به کمک هسته یا Kernel PCA </span>\n",
    "    <br>\n",
    "     این روش بر خلاف PCA اصلی ،از طریق استفاده از هسته‌ها به کاهش ابعاد غیرخطی می‌رسد \n",
    "    </div></span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e699856b-df47-4188-b64a-7bc1188c5971",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import KernelPCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3f9daf-65d4-4add-81c7-6e07424c60d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "rbf_pca = KernelPCA(n_components=2, kernel=\"rbf\", gamma=0.04)\n",
    "X_reduced = rbf_pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f16f4b-1344-4c7e-b3fe-d062d7f78548",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2D = pd.DataFrame({'PC1': X_reduced[:, 0], 'PC2': X_reduced[:, 1], 'species': y})\n",
    "fig = px.scatter(df_2D, x='PC1', y='PC2',\n",
    "              color='species')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460823a8-80dd-451d-a8bd-79901cee7240",
   "metadata": {},
   "source": [
    "<span style=\"font-family: XB Zar; font-size: medium; \"><div dir=rtl><meta charset=\"UTF-8\">\n",
    "    <span style=\"color: red; font-size: large; \"> 3. تجزیه مقادیر منفرد یا SVD و ارتباط آن با تحلیل مولفه‌های اصلی </span>\n",
    "    <br>\n",
    "    فرض کنید \n",
    "    ماتریس دادگان\n",
    "    $\\mathbf{X}_{N \\times D}$\n",
    "    به ما داده شده‌است.\n",
    "    حال ماتریس جدید\n",
    "    $\\mathbf{T} = \\mathbf{X} - \\mathbf{1}\\bar{\\mathbf{x}} $\n",
    "    را در نظر بگیرید \n",
    "    که میانگین تجربی آن برای هر ویژگی صفر است حال ما \n",
    "    تجزیه مقادیر منفرد را برای این ماتریس میابیم \n",
    "    <br>\n",
    "    <br>\n",
    "    $$\n",
    "    \\mathbf{T} = \\mathbf{U}\\mathbf{\\Sigma}\\mathbf{V}^{\\top} \n",
    "    $$\n",
    "    <br>\n",
    "    اگر فرض کنیم \n",
    "    $k$\n",
    "    مقدار منفرد مثبت وجود داشته‌باشد\n",
    "    دراینصورت \n",
    "    $\\begin{split}m, (m<d) \\end{split}$\n",
    "    مولفه اصلی فضای ابعاد کاهش یافته در واقع \n",
    "    $ m $\n",
    "    بردار ویژه راست \n",
    "    یعنی \n",
    "    $ m $\n",
    "    بردار \n",
    "    $ \\mathbf{V}$\n",
    "    است که پایه‌های فضای برداری مقصد را می‌سازند.\n",
    "    حال با استفاده از نامپای همین فرآیند را یکبار دیگر برای کاهش بعد تکرار می‌کنیم تا مطمئن شویم رویکردمان درست است:\n",
    "    </div></span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27de2a0a-1836-4b32-9c0c-f434dffbc23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ua, Sigma, V = np.linalg.svd(T)\n",
    "U = V.transpose()[:, 0:2]\n",
    "X_star_2D = T.dot(U)\n",
    "df_2D = pd.DataFrame({'PC1': X_star_2D[:, 0], 'PC2': X_star_2D[:, 1], 'species': y})\n",
    "fig = px.scatter(df_2D, x='PC1', y='PC2',\n",
    "              color='species')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2969de42-3a9c-4476-a93e-1576352e3c8b",
   "metadata": {},
   "source": [
    "<span style=\"font-family: XB Zar; font-size: medium; \"><div dir=rtl><meta charset=\"UTF-8\">\n",
    "    <span style=\"color: red; font-size: large; \">4. منابع</span>\n",
    "    <br>\n",
    "    <ul>\n",
    "        <li><a href=\"https://www.sqlservercentral.com/wp-content/uploads/legacy/c2362193dd68a81c6774bb9668f1178fc2cf4ad4/32947.jpg\">شکل 1</a> </li>\n",
    "        <li><a href=\"https://www.researchgate.net/profile/Diego-Peluffo/publication/325363944/figure/fig1/AS:631693463539712@1527618866278/Dimensionality-reduction-effect-over-an-3D-artificial-Swiss-roll-manifold-the-2D.png\">شکل 2</a> </li>\n",
    "        <li><a href=\"http://alexhwilliams.info/itsneuronalblog/img/pca/pca_two_views.png\">شکل 3</a> </li>\n",
    "        <li><a href=\"https://scikit-learn.org/stable/auto_examples/decomposition/plot_faces_decomposition.html#sphx-glr-auto-examples-decomposition-plot-faces-decomposition-py\">scikitlearn tutorials</a> </li>\n",
    "    </ul>\n",
    "    </div></span>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\" align=\"center\">\n",
    "<font face=\"XB Zar\" size=5>\n",
    "    <font face=\"IranNastaliq\" size=5>\n",
    "      به نام خدا\n",
    "    </font>\n",
    "    <br>\n",
    "    <font size=3>\n",
    "      دانشگاه صنعتی شریف - دانشکده مهندسی کامپیوتر\n",
    "    </font>\n",
    "    <br>\n",
    "    <font color=blue size=5>\n",
    "      مقدمه‌ای بر یادگیری ماشین\n",
    "    </font>\n",
    "    <br>\n",
    "    <hr/>\n",
    "    <font color=red size=6>\n",
    "      فصل دوم: رگرسیون خطی\n",
    "    </font>\n",
    "    <br>\n",
    "      نویسندگان:‌ سید امیرمحمد عیسی زاده\n",
    "    <hr>\n",
    "<br>\n",
    "  <div align=\"right\">\n",
    "  <font color=\"red\" size=5>فهرست مطالب</font>\n",
    "\t<ul>\n",
    "        <li>\n",
    "        <a href=\"#lr_from_scratch\">\n",
    "            رگرسیون خطی از پایه\n",
    "        </a>\n",
    "        <ul>\n",
    "            <br>\n",
    "            <li>\n",
    "                <a href=\"#lr_closed_form\">\n",
    "                  فرم بسته رگرسیون خطی\n",
    "                </a>\n",
    "            </li>\n",
    "            <br>\n",
    "            <li>\n",
    "                <a href=\"#lr_optimization\">\n",
    "                  رگرسیون خطی با استفاده از optimization function\n",
    "                </a>\n",
    "            </li>\n",
    "            <br>\n",
    "            <li>\n",
    "                <a href=\"#gradient_descent\">\n",
    "                  Gradient Descent\n",
    "                </a>\n",
    "            </li>\n",
    "            <br>\n",
    "            <li>\n",
    "                <a href=\"#model_training\">\n",
    "                  آموزش مدل\n",
    "                </a>\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <br>\n",
    "    <li>\n",
    "        <a href=\"#lr_sklearn\">\n",
    "            رگرسیون خطی با استفاده از sklearn\n",
    "        </a>\n",
    "    </li>\n",
    "    <br>\n",
    "    <li>\n",
    "      <a href=\"#beyond_linear\">\n",
    "        فراتر از linear regression\n",
    "      </a>\n",
    "        <ul>\n",
    "            <br>\n",
    "            <li>\n",
    "                <a href=\"#lasso\">\n",
    "            Lasso Regression\n",
    "                </a>\n",
    "            </li>\n",
    "            <br>\n",
    "            <li>\n",
    "                <a href=\"#ridge\">\n",
    "            Ridge Regression\n",
    "                </a>\n",
    "            </li>\n",
    "            <br>\n",
    "            <li>\n",
    "                <a href=\"#elasticnet\">\n",
    "                    Elastic Net\n",
    "                </a>\n",
    "            </li>\n",
    "            <br>\n",
    "            <li>\n",
    "                <a href=\"#ridge_vs_lasso\">\n",
    "                    Ridge vs Lasso\n",
    "                </a>\n",
    "            </li>\n",
    "        </ul>\n",
    "\t</li>\n",
    "    <br>\n",
    "    <br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sIGyifOmnOYc"
   },
   "source": [
    "\n",
    "  <div dir=rtl id=\"lr_from_scratch\">\n",
    "  <font face=\"XB Zar\" size=4>\n",
    "      <font color=\"red\" size=5>\n",
    "        رگرسیون خطی از پایه\n",
    "      </font>\n",
    "      <hr />\n",
    "در این قسمت ما از یک دیتاست ساده برای تخمین میزان محصول دو میوه(سیب  و پرتقال) با توجه به پارامترهای دما، میزان بارش و رطوبت در چندین منطقه استفاده میکنیم.\n",
    "    <br>\n",
    "      به این منظور دیتاست ساده‌ای را میسازیم:\n",
    "    </font>\n",
    "      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use numpy to define our inputs and targets\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset (temp, rainfall, humidity, apples, oranges)\n",
    "df = np.array([[73, 67, 43, 56, 70], \n",
    "                   [91, 88, 64, 81, 101], \n",
    "                   [87, 134, 58, 119, 133], \n",
    "                   [102, 43, 37, 22, 37], \n",
    "                   [69, 96, 70, 103, 119]], dtype='float32')\n",
    "\n",
    "# reigons which are used as indexes\n",
    "regions = [\"Semnan\", \"Golestan\", \"Gilan\", \"Ghazvin\", \"Mazandaran\"]\n",
    "\n",
    "columns = [\"Temp(F)\", \"Rainfall(mm)\", \"Humidity(%)\", \"Apples(ton)\", \"Oranges(ton)\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "  <div dir=rtl>\n",
    "    <font face=\"XB Zar\" size=4>\n",
    "برای نمایش دیتاست از کتابخانه pandas استفاده میکنیم:\n",
    "    </font>\n",
    "      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating dataframe using variable columns as columns and regions as index\n",
    "df = pd.DataFrame(df, columns=columns, index=regions)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=rtl id=\"dataset\">\n",
    "  <font face=\"XB Zar\" size=4>\n",
    "در رگرسیون خطی، هر مقدار خروجی(ستون‌های سیب و پرتقال)، به صورت جمع وزن‌داری از ستون‌های ورودی به اضافه یک مقدار ثابت در نظر گرفته میشوند:\n",
    "      <br>\n",
    "    </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "apple  = w11 * temp + w12 * rainfall + w13 * humidity + b1\n",
    "orange = w21 * temp + w22 * rainfall + w23 * humidity + b2\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=rtl id=\"dataset\">\n",
    "  <font face=\"XB Zar\" size=4>\n",
    "که به‌این معناست که خروجی‌های ما یک تابع خطی یا یک صفحه از ورودی‌ها است:\n",
    "      <br>\n",
    "    </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"center\">\n",
    "<img src=\"linear-regression-graph.png\" width=\"500\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "execution": {
     "iopub.execute_input": "2022-09-08T17:23:58.094178Z",
     "iopub.status.busy": "2022-09-08T17:23:58.093871Z",
     "iopub.status.idle": "2022-09-08T17:23:58.223769Z",
     "shell.execute_reply": "2022-09-08T17:23:58.222728Z",
     "shell.execute_reply.started": "2022-09-08T17:23:58.094152Z"
    },
    "id": "FWaKYE6Zego-",
    "outputId": "33aced47-2337-49da-b453-5f17ef3e90d8"
   },
   "source": [
    "<div dir=rtl id=\"dataset\">\n",
    "  <font face=\"XB Zar\" size=4>\n",
    "عملا فرم ماتریس خروجی(Y) به این شکل است:\n",
    "      <br>\n",
    "      <br>\n",
    "      <font color=\"black\" size=5>\n",
    "          $$X \\qquad * \\qquad W \\qquad + \\qquad b \\\\$$\n",
    "          $$\n",
    "\\begin{bmatrix}\n",
    "      73 & 67 & 43 \\\\\n",
    "      91 & 88 & 64 \\\\\n",
    "      \\vdots & \\vdots & \\vdots \\\\\n",
    "      69 & 96 & 70 \\\\\n",
    "\\end{bmatrix}\n",
    "*\n",
    "\\begin{bmatrix}\n",
    "      w_{11} & w_{21} \\\\\n",
    "      w_{12} & w_{22} \\\\\n",
    "      w_{13} & w_{23} \\\\\n",
    "\\end{bmatrix}\n",
    "+\n",
    "\\begin{bmatrix}\n",
    "      b_1 & b_2 \\\\\n",
    "      b_1 & b_2 \\\\\n",
    "      \\vdots & \\vdots \\\\\n",
    "      b_1 & b_2 \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "          </font>\n",
    "    </font>\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=rtl id=\"dataset\">\n",
    "  <font face=\"XB Zar\" size=4>\n",
    "      دیتاست را به دو بخش X و Y قسمت میکنیم که X همان ورودی‌های ما و Y میزان بهره‌برداری از میوه‌ها یا همان خروجی‌ها هستند.\n",
    "      <br>\n",
    "    </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:, :3].to_numpy()\n",
    "Y = df.iloc[:, 3:].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=rtl id=\"dataset\">\n",
    "  <font face=\"XB Zar\" size=4>\n",
    "نکته شایان توجه این است که b فقط مقداری ثابت به حاصل ضرب اضافه می‌کند، پس اگر به ابتدای تمامی سطرهای X مقدار 1 را اضافه کنیم و به ابتدای هر ستون ماتریس W مقدار بایاس متناظر با آن را اضافه کنیم خروجی هیچ تغییری نخواهد کرد.\n",
    "      <br>\n",
    "    </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.append(np.ones((5, 1)), X , axis=1)\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=rtl id=\"dataset\">\n",
    "  <font face=\"XB Zar\" size=4>\n",
    "بدست آوردن بهترین وزن در رگرسیون خطی، از دو راه ممکن است:\n",
    "      <ol>\n",
    "            <li>فرم بسته: اگر مشتق جزئی بر حسب وزن را مساوی صفر قرار دهیم، با بدست آوردن ماتریس وزن متناظر، به بهترین پاسخ می‌رسیم.</li>\n",
    "            <li>استفاده از optimization function: همچنین می‌توانیم بجای استفاده از فرم بسته، مانند اکثر مدل‌ها، مرحله مرحله جلو رفته و به بهترین وزن، نزدیک شویم.\n",
    "          دقت کنید که بر خلاف رگرسیون خطی، در اکثر مدل‌ها نمی‌توان فرم بسته‌ای از پاسخ پیدا کرد.</li>\n",
    "        </ol>\n",
    "      <br>\n",
    "    </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "  <div dir=rtl id=\"lr_closed_form\" align=\"justify\">\n",
    "  <font face=\"XB Zar\" size=4>\n",
    "      <font color=\"red\" size=5>\n",
    "        فرم بسته رگرسیون خطی\n",
    "      </font>\n",
    "      <hr />\n",
    "      همانطور که در کلاس درس گفته شد، پاسخ فرم بسته بدین شکل است:\n",
    "      <br>\n",
    "      </font>\n",
    "  </div>\n",
    "      <div class=\"cmath\"> W = (X<sup>T</sup>X)<sup>-1</sup>X<sup>T</sup>Y</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal_equation(X, Y):\n",
    "    W = np.dot((np.linalg.inv(np.dot(X.T,X))), np.dot(X.T,Y))\n",
    "    return W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=rtl id=\"dataset\">\n",
    "  <font face=\"XB Zar\" size=4>\n",
    "این تابع همان معادله بالاست که با گرفتن ورودی X و Y،خروجی W را به ما می‌دهد.\n",
    "      <br>\n",
    "    </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .dot() represent matrix multiplication in numpy\n",
    "def lr_predict(X, W):\n",
    "    return np.dot(X, W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=rtl id=\"dataset\">\n",
    "  <font face=\"XB Zar\" size=4>\n",
    "این تابع برای محاسبه مقادیر Y با استفاده از X و وزن‌های فعلی است.\n",
    "      <br>\n",
    "      حال دو تابع بالا را در یک مدل تعریف می‌کنیم. ابتدا بهترین وزن را بدست می‌آوریم و سپس مقدار Y را پیش‌بینی می‌کنیم.\n",
    "      <br>\n",
    "    </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def closed_form_lr(X, Y):\n",
    "    W = normal_equation(X, Y)\n",
    "    predictions = lr_predict(X, W)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = closed_form_lr(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2022-09-08T17:23:58.275887Z",
     "iopub.status.busy": "2022-09-08T17:23:58.275573Z",
     "iopub.status.idle": "2022-09-08T17:23:58.309198Z",
     "shell.execute_reply": "2022-09-08T17:23:58.308143Z",
     "shell.execute_reply.started": "2022-09-08T17:23:58.275853Z"
    },
    "id": "-rUt-7j7egpD",
    "outputId": "a4f8f030-0e66-4a95-92a5-79d01e0460d5"
   },
   "source": [
    "<div dir=rtl id=\"dataset\">\n",
    "  <font face=\"XB Zar\" size=4>\n",
    "حال معیاری برای میزان خطای مدل، تعریف میکنیم.\n",
    "      <br>\n",
    "      ما از معیار MSE(mean squared error) استفاده میکنیم که اخلاف توان دو پیش بینی‌ها نسبت به مقدار واقعی خروجی‌ها است.\n",
    "      <br>\n",
    "    </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSE loss\n",
    "def mse_loss(pred, real):\n",
    "    diff = pred - real\n",
    "    return np.sum(diff * diff) / diff.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=rtl id=\"dataset\">\n",
    "  <font face=\"XB Zar\" size=4>\n",
    "تابع sum تمامی عناصر آن آرایه را با هم جمع میکند.\n",
    "      در تابع mse_loss در ابتدا اختلاف‌ها را بین دو ماتریس پیدا میکنیم، سپس تمامی این اختلاف‌ها را با هم جمع میکنیم و در انتها تقسیم بر تعداد عناصر میکنیم.\n",
    "      <br>\n",
    "    </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_loss(predictions, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BmZv9X3O0cxg"
   },
   "source": [
    "\n",
    "  <div dir=rtl id=\"lr_optimization\" align=\"justify\">\n",
    "  <font face=\"XB Zar\" size=4>\n",
    "      <font color=\"red\" size=5>\n",
    "        رگرسیون خطی با استفاده از optimization function\n",
    "      </font>\n",
    "      <hr />\n",
    "      آموزش مدل رگرسیون خطی، عملا همان پیدا کردن مقدار بهینه برای وزن‌ها(w11 و w12 و ... ) است. وزن‌های آموزش دیده شده، برای پیشبینی داده‌های جدید استفاده خواهند شد. سپس این کار را تکرار می‌کنیم تا رفته رفته نتایج ما بهتر شوند. برای بهتر کردن مدل، که همان پیدا کردن وزن‌های بهتر باشد، از optimizer استفاده میکنیم. optimizer مورد استفاده در این تمرین، gradient descent است.\n",
    "  </font>\n",
    "  </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=rtl id=\"dataset\">\n",
    "  <font face=\"XB Zar\" size=4>\n",
    "حال باید به وزن‌ها مقدار اولیه بدهیم. روش‌های مختلفی برای این کار وجود دارند که ما از مقداردهی رندوم استفاده میکنیم:\n",
    "      <br>\n",
    "    </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weights\n",
    "W = np.random.rand(4, 2)\n",
    "W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=rtl id=\"dataset\">\n",
    "  <font face=\"XB Zar\" size=4>\n",
    "از تابع lr_predict که بالا تعریف کردیم، استفاده می‌کنیم.\n",
    "      <br>\n",
    "      این مدل را روی داده‌های ورودی، که همان X و W هستند تست میکنیم:\n",
    "      <br>\n",
    "    </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = lr_predict(X, W)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=rtl id=\"dataset\">\n",
    "  <font face=\"XB Zar\" size=4>\n",
    "از تابع mse_loss که بالا تعریف کردیم، استفاده می‌کنیم.\n",
    "      <br>\n",
    "    </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = mse_loss(predictions, Y)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=rtl id=\"dataset\">\n",
    "  <font face=\"XB Zar\" size=4>\n",
    "همانطور که از خود مقادیر عناصر پیش‌بینی شده واضح بود، اختلاف خیلی زیاد است. این اختلاف به این دلیل است که هیچ بهینه سازی‌ای در وزن‌ها انجام ندادیم و صرفا یک پیش‌بینی رندوم کردیم.\n",
    "در بخش Gradient Descent نحوه بروزرسانی وزن‌ها توضیح داده شده است.\n",
    "      <br>\n",
    "    </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7u-9szEQ1UST"
   },
   "source": [
    "\n",
    "  <div dir=rtl id=\"gradient_descent\" align=\"justify\">\n",
    "    <font face=\"XB Zar\" size=4>\n",
    "      <font color=\"red\" size=5>Gradient Descent</font>\n",
    "      <hr />\n",
    "      مقدار loss ما یک تابع درجه دو از وزن‌ها است. و وظیفه ما این است که آن را به کمترین مقدار خودش برسانیم.\n",
    "        اگر به شکل پایین دقت کنیم، و با توجه به حساب دیفرانسیل، یک نقطه را در نظر بگیرید. دو حالت برای این نقطه ممکن است:\n",
    "        <br>\n",
    "         شیب یا همان گرادیان مثبت باشد:\n",
    "        <br>\n",
    "        1) اگر مقدار کمی عقب برویم، مقدار تابع کمتر میشود.\n",
    "        <br>\n",
    "        2)  اگر مقدار کمی جلو برویم، مقدار تابع بیشتر میشود.\n",
    "        <br>\n",
    "  </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"center\">\n",
    "<img src=\"positive gradient.png\" width=\"500\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=rtl id=\"dataset\">\n",
    "  <font face=\"XB Zar\" size=4>\n",
    "         گرادیان منفی باشد:\n",
    "      <br>\n",
    "      1) اگر مقدار کمی عقب برویم، مقدار تابع بیشتر میشود.\n",
    "      <br>\n",
    "      2) اگر مقدار کمی جلو برویم، مقدار تابع کمتر میشود.\n",
    "      <br>\n",
    "    </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"center\">\n",
    "<img src=\"negative gradient.png\" width=\"500\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=rtl id=\"dataset\">\n",
    "  <font face=\"XB Zar\" size=4>\n",
    "        بنابراین، در هر مرحله مقدار گرادیان را در عدد کوچکی ضرب میکنیم و وزن‌ها را از این مقدار کم میکنیم.\n",
    "      این مقدار کوچک همان learning rate است که یکی از مهم ترین hyperparameter های هر مدل ماشین لرنینگ است.\n",
    "      دو حالت نامطلوب برای learning rate محتمل است:\n",
    "      <br>\n",
    "      <ul>\n",
    "  <li>مقدار کوچک‌تر از حد مطلوب: در این صورت مدل دیر همگرا میشود و مراحل بیشتری نیاز است، چون در هر مرحله مقدار بسیار کمی تغییر میکند.</li>\n",
    "  <li>مقدار بیشتر از حد مطلوب: در این صورت مدل در نهایت با حالت مینیمم فاصله زیادی خواهد داشت.</li>\n",
    "</ul>\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"center\">\n",
    "<img src=\"gradient_descent.avif\" width=\"500\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=rtl id=\"dataset\">\n",
    "  <font face=\"XB Zar\" size=4>\n",
    "      حال که مقدار خطا را بدست آوردیم و وزن‌های فعلی را داریم، باید مقدار گرادیان را حساب کنیم.\n",
    "    </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_gradient(X, error):\n",
    "    gradient = np.dot(X.T, error)\n",
    "    return gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=rtl id=\"dataset\">\n",
    "  <font face=\"XB Zar\" size=4>\n",
    "رابطه نوشته شده همان مقدار گرادیان است. پیشنهاد می‌شود که صحت این رابطه را بررسی کنید.\n",
    "    </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=rtl id=\"dataset\">\n",
    "  <font face=\"XB Zar\" size=4>\n",
    "بعد از بدست آوردن گرادیان، باید وزن‌ها را بروزرسانی کنیم.\n",
    "    </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_weights(W, lr, gradient):\n",
    "    new_weights = W - lr * gradient\n",
    "    return new_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1ElZ0H5Fg2yw"
   },
   "source": [
    "\n",
    "  <div dir=\"rtl\" id=\"model_training\">\n",
    "    <font face=\"XB Zar\" size=4>\n",
    "      <font color=\"red\" size=5>\n",
    "        آموزش مدل\n",
    "      </font>\n",
    "      <hr />\n",
    "       حال که همه بخش‌های مدل را بررسی کردیم، نوبت آزمایش آن است.\n",
    "        یک بار تمامی مراحل را مرور میکنیم:\n",
    "        <ol>\n",
    "            <li>تولید پیش بینی با استفاده از وزن‌های فعلی</li>\n",
    "            <li>محاسبه مقدار loss</li>\n",
    "            <li>محاسبه گرادیان‌ها با توجه به وزن‌ها</li>\n",
    "            <li>آپدیت کردن وزن‌ها</li>\n",
    "        </ol>\n",
    "        <br>\n",
    "        این کار‌ها را در چندین مرحله(epoch) انجام میدهیم.\n",
    "        <br>\n",
    "        حال باید تابعی را تعریف کنیم که این مراحل را انجام دهد.\n",
    "       </font>\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_lr_gd(X, Y, W, n_epoches, lr):\n",
    "    losses = []\n",
    "    for i in range(n_epoches):\n",
    "        # generating the predictions \n",
    "        predictions = lr_predict(X, W)\n",
    "        error = predictions - Y\n",
    "        # calculating the loss\n",
    "        loss = mse_loss(predictions, Y)\n",
    "        # adding the loss to our loss list \n",
    "        losses.append(loss)\n",
    "        # calculating gradients\n",
    "        gradient = calc_gradient(X, error) / Y.size\n",
    "        # updating weights and biases\n",
    "        W = update_weights(W, lr, gradient)\n",
    "    return W, losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=rtl id=\"dataset\">\n",
    "  <font face=\"XB Zar\" size=4>\n",
    "         مدلمان را در 10000 مرحله تست میکنیم و مقدار learning rate را  1e-5 میگذاریم.\n",
    "      <br>\n",
    "      همچنین loss ها را در هر مرحله ذخیره میکنیم.\n",
    "    </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-5\n",
    "n_epoches = 10000\n",
    "W, losses = train_model_lr_gd(X, Y, W, n_epoches, lr)\n",
    "print(losses[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=rtl id=\"dataset\">\n",
    "  <font face=\"XB Zar\" size=4>\n",
    "         همانطور که مشاهده می‌کنید مقدار loss نهایی خیلی نزدیک به مقدار بدست آمده در حالت بسته است.\n",
    "    </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "  \n",
    "# x axis values\n",
    "x = [i for i in range(n_epoches)]\n",
    "# corresponding y axis values\n",
    "y = losses\n",
    "  \n",
    "# plotting the points \n",
    "plt.plot(x, y)\n",
    "  \n",
    "# naming the x axis\n",
    "plt.xlabel('number of epoches')\n",
    "# naming the y axis\n",
    "plt.ylabel('loss')\n",
    "  \n",
    "# function to show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=rtl id=\"dataset\">\n",
    "  <font face=\"XB Zar\" size=4>\n",
    "         همانطور که مشاهده میکنید، در ابتدا به مقدار زیادی از خطا کم میشود و رفته رفته اختلاف خطاها در هر دو مرحله متوالی کمتر میشود تا در نهایت در همسایگی کوچکی از مینیمم، تغییر کند. هر چه learning rate را کمتر کنیم طول این همسایگی کمتر میشود. (البته در مراحل بیشتری باید کار را انجام دهیم تا نتیجه مطلوبی کسب کنیم)\n",
    "    </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   <div dir=\"rtl\" id=\"lr_sklearn\">\n",
    "    <font face=\"XB Zar\" size=4>\n",
    "      <font color=\"red\" size=5>\n",
    "        رگرسیون خطی با استفاده از sklearn\n",
    "      </font>\n",
    "        <hr />\n",
    "       <br>\n",
    "      حال دیتاست داده شده را با استفاده از مدل آماده linear regression در sklearn یاد میگیریم.\n",
    "        <br>\n",
    "        این کتابخانه تابع آماده برای محاسبه خطا دارد.\n",
    "       </font>\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:, :3].to_numpy()\n",
    "Y = df.iloc[:, 3:].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=rtl id=\"dataset\">\n",
    "  <font face=\"XB Zar\" size=4>\n",
    "         یک مدل رگرسیون خطی از ماژول linear_model این کتابخانه تعریف می‌کنیم.\n",
    "    </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create linear regression object\n",
    "regr = linear_model.LinearRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=rtl id=\"dataset\">\n",
    "  <font face=\"XB Zar\" size=4>\n",
    "با تابع fit، مدل را بر روی داده‌ها آموزش می‌دهیم.    </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model using the training sets\n",
    "regr.fit(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=rtl id=\"dataset\">\n",
    "  <font face=\"XB Zar\" size=4>\n",
    "با تابع predict، از ورودی‌ها مقدار خروجی‌ها را بدست آورده و سپس میزان خطا را محاسبه می‌کنیم.    </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = regr.predict(X)\n",
    "predictions\n",
    "mean_squared_error(Y, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=rtl id=\"dataset\">\n",
    "  <font face=\"XB Zar\" size=4>\n",
    "همانطور که مشاهده می‌کنید میزان خطای بدست آمده بسیار نزدیک به میزان خطا در فرم بسته است!!    </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   <div dir=\"rtl\" id=\"beyond_linear\" align=\"justify\">\n",
    "    <font face=\"XB Zar\" size=4>\n",
    "      <font color=\"red\" size=5>\n",
    "        فراتر از linear regression\n",
    "      </font>\n",
    "        <hr />\n",
    "      در رگرسیون خطی‌ای که معرفی کردیم، هیچ قانون محدود کننده‌ای روی وزن ها قرار ندادیم. این کار باعث پیچیده تر شدن مدل می‌شود که به overfitting بر روی داده train منجر می‌شود. به گذاشتن محدودیت روی وزن‌ها regularization می‌گوییم. به عبارتی وظیفه ما مینیمم کردن مجموع loss و regularization term است.\n",
    "        <br>\n",
    "        حال مدل‌هایی را معرفی می‌کنیم که بخش regularization را به linear regression اضافه کرده‌اند.\n",
    "       </font>\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   <div dir=\"rtl\" id=\"lasso\" align=\"justify\">\n",
    "    <font face=\"XB Zar\" size=4>\n",
    "      <font color=\"red\" size=5>\n",
    "        Lasso Regression\n",
    "      </font>\n",
    "        <hr />\n",
    "      در این مدل، regularization term به صورت جمع قدر مطلق وزن‌ها تعریف می‌شود.\n",
    "        به عبارتی در این مدل باید تابع زیر را کمینه کنیم:\n",
    "        <br>\n",
    "        <br>\n",
    "        <font color=\"black\" size=5>\n",
    "        $$\\frac{1}{2m} \\Sigma_{i=1}^{m}{(y-Xw)^2} + \\alpha \\Sigma_{j=1}^{p}{\\mid w_j\\mid}$$\n",
    "            </font>\n",
    "       </font>\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=rtl id=\"dataset\">\n",
    "  <font face=\"XB Zar\" size=4>\n",
    "در lasso، به علت پنالتی‌ای که روی وزن ها قرار دارد، تمایل به کم کردن وزن ها به صورت برابر وجود دارد، به همین دلیل ماتریس وزن، sparsity بالایی دارد و تعداد زیادی از وزن ها به صفر میل می‌کنند.    \n",
    "    </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   <div dir=\"rtl\" id=\"ridge\" align=\"justify\">\n",
    "    <font face=\"XB Zar\" size=4>\n",
    "      <font color=\"red\" size=5>\n",
    "        Ridge Regression\n",
    "      </font>\n",
    "        <hr />\n",
    "      در این مدل، regularization term به صورت جمع توان دو وزن‌ها تعریف می‌شود.\n",
    "        به عبارتی در این مدل باید تابع زیر را کمینه کنیم:\n",
    "        <br>\n",
    "        <br>\n",
    "        <font color=\"black\" size=5>\n",
    "        $$\\Sigma_{i=1}^{m}{(y-Xw)^2} + \\alpha \\Sigma_{j=1}^{p}{w_j^2}$$\n",
    "            </font>\n",
    "       </font>\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=rtl id=\"dataset\">\n",
    "  <font face=\"XB Zar\" size=4>\n",
    "در ridge، بر خلاف lasso چون پنالتی بر روی توان دو وزن ها قرار دارد، وزن ها صفر نمی‌شوند و با همدیگر کم می‌شوند، چون به عنوان مثال اختلاف توان دو 4 و 2، بیشتر از اختلاف توان دو 2 و صفر هست، به همین علت همه با هم کم می‌شوند.\n",
    "    </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   <div dir=\"rtl\" id=\"elasticnet\" align=\"justify\">\n",
    "    <font face=\"XB Zar\" size=4>\n",
    "      <font color=\"red\" size=5>\n",
    "        Elastic Net\n",
    "      </font>\n",
    "        <hr />\n",
    "      در این مدل، regularization term به صورت جمع توان دو و توان یک وزن‌ها تعریف می‌شود. به عبارتی خصوصیت های ridge و lasso را در خود جمع کرده است.\n",
    "        در این مدل باید تابع زیر را کمینه کنیم:\n",
    "        <br>\n",
    "        <br>\n",
    "        <font color=\"black\" size=5>\n",
    "        $$\\frac{1}{2m} \\Sigma_{i=1}^{m}{(y-Xw)^2} + \\alpha * ratio * \\Sigma_{j=1}^{p}{\\mid w_j\\mid} + 0.5 * \\alpha * (1-ratio) * \\Sigma_{j=1}^{p}{w_j^2}$$\n",
    "            </font>\n",
    "       </font>\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=rtl id=\"dataset\">\n",
    "  <font face=\"XB Zar\" size=4>\n",
    "با تابع predict، از ورودی‌ها مقدار خروجی‌ها را بدست آورده و سپس میزان خطا را محاسبه می‌کنیم.\n",
    "    <br>\n",
    "      حال نمونه ای از پیاده‌سازی ElasticNet در پایتون را مشاهده می‌کنیم.\n",
    "    </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Regression:\n",
    "    \n",
    "    def __init__(self, regularization, lr, epoch):\n",
    "        self.m = None #samples\n",
    "        self.n = None #features\n",
    "        self.w = None #weight\n",
    "        self.b = None #bias\n",
    "        self.regularization = regularization #penalty object\n",
    "        self.lr = lr #learning rate\n",
    "        self.epoch = epoch #iteration\n",
    "        \n",
    "    def __calculate_cost(self, y, y_pred):\n",
    "        return (1 / (2*self.m)) * np.sum(np.square(y_pred-y)) + self.regularization(self.w)\n",
    "    \n",
    "    def __hypothesis(self, w, X):\n",
    "        return np.dot(X, w) \n",
    "    \n",
    "    def __initialization(self, X):\n",
    "        X = np.insert(X, 0, 1, axis=1)\n",
    "        self.m, self.n = X.shape\n",
    "        self.w = np.zeros((self.n,1))\n",
    "        return X\n",
    "    \n",
    "    def __update_parameters(self, X, y, y_pred):\n",
    "        dw = (1/self.m) * np.dot(X.T, (y_pred - y)) + self.regularization.derivation(self.w)\n",
    "        self.w = self.w - self.lr * dw\n",
    "        return True\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        X = self.__initialization(X)\n",
    "        for e in range(1, self.epoch+1):\n",
    "            y_pred = self.__hypothesis(self.w, X)\n",
    "            cost = self.__calculate_cost(y, y_pred)\n",
    "            self.__update_parameters(X, y, y_pred)\n",
    "            if e % 5000 == 0:\n",
    "#                 print(f\"The Cost in iteration {e}----->{cost} :)\")\n",
    "                pass\n",
    "\n",
    "        return True\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        X_test = np.insert(X_test, 0 , 1, axis= 1)\n",
    "        y_pred = self.__hypothesis(self.w, X_test)\n",
    "        return y_pred\n",
    "\n",
    "class ElasticPenalty:\n",
    "    \n",
    "    def __init__(self, l = 0.1, l_ratio = 0.5):\n",
    "        self.l = l \n",
    "        self.l_ratio = l_ratio\n",
    "\n",
    "    def __call__(self, w):\n",
    "        l1_contribution = self.l_ratio * self.l * np.sum(np.abs(w))\n",
    "        l2_contribution = (1 - self.l_ratio) * self.l * 0.5 * np.sum(np.square(w))\n",
    "        return (l1_contribution + l2_contribution)\n",
    "\n",
    "    def derivation(self, w):\n",
    "        l1_derivation = self.l * self.l_ratio * np.sign(w)\n",
    "        l2_derivation = self.l * (1 - self.l_ratio) * w\n",
    "        return (l1_derivation + l2_derivation)\n",
    "\n",
    "class ElasticNet(Regression):\n",
    "    \n",
    "    def __init__(self, l, l_ratio, lr, epoch):\n",
    "        self.regularization = ElasticPenalty(l,l_ratio)\n",
    "        super().__init__(self.regularization, lr, epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=rtl id=\"dataset\">\n",
    "  <font face=\"XB Zar\" size=4>\n",
    "نکته بسیار مهم این است که اگر مقدار l_ratio در پارامتر ها برابر با صفر باشد، مدل تبدیل به Ridge می‌شود و اگر برابر با یک باشد مدل تبدیل به Lasso می‌شود. (چرا؟)\n",
    "    <br>\n",
    "    </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   <div dir=\"rtl\" id=\"ridge_vs_lasso\" align=\"justify\">\n",
    "    <font face=\"XB Zar\" size=4>\n",
    "      <font color=\"red\" size=5>\n",
    "        Ridge vs Lasso\n",
    "      </font>\n",
    "        <hr />\n",
    "            به ازای l_ratio های مختلف(از Ridge تا Lasso) مقدار mse_loss را برای مدلمان بدست می‌آوریم.\n",
    "        <br>\n",
    "        برای ساخت دیتاست از تابع make_regression کتابخانه sklearn استفاده می‌کنیم. در این تابع می‌توانیم میزان نویز را به دلخواه تعیین کنیم.\n",
    "       </font>\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "X, y = make_regression(n_samples=50000, n_features=8, noise=5)\n",
    "y = y[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "for i in range(11):\n",
    "    parameters = {\n",
    "        \"l_ratio\" : i * 0.1,\n",
    "        \"l\" : 0.1,\n",
    "        \"lr\" : 0.001,\n",
    "        \"epoch\" : 10000\n",
    "    }\n",
    "    model = ElasticNet(**parameters)\n",
    "    model.fit(X, y) \n",
    "    y_pred = model.predict(X)\n",
    "    losses.append(mse_loss(y_pred, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [i * 0.1 for i in range(11)]\n",
    "y = losses\n",
    "plt.plot(x, y)\n",
    "plt.xlabel('l ratio')\n",
    "plt.ylabel('loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=rtl id=\"dataset\">\n",
    "  <font face=\"XB Zar\" size=4>\n",
    "در این دیتاست، lasso نتیجه بهتری داده‌است.\n",
    "    </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_regression(n_samples=5000, n_features=3, noise=3)\n",
    "y = y[:, np.newaxis]\n",
    "\n",
    "for i in range (5000):\n",
    "    if i % 4 == 0:\n",
    "        X[i] = X[i] * 20\n",
    "        y[i] = y[i] * 20\n",
    "losses = []\n",
    "for i in range(11):\n",
    "    parameters = {\n",
    "        \"l_ratio\" : i * 0.1,\n",
    "        \"l\" : 0.1,\n",
    "        \"lr\" : 0.0001,\n",
    "        \"epoch\" : 10000\n",
    "    }\n",
    "    model = ElasticNet(**parameters)\n",
    "    model.fit(X, y) \n",
    "    y_pred = model.predict(X)\n",
    "    losses.append(mse_loss(y_pred, y))\n",
    "x = [i * 0.1 for i in range(11)]\n",
    "y = losses\n",
    "plt.plot(x, y)\n",
    "plt.xlabel('l ratio')\n",
    "plt.ylabel('loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=rtl id=\"dataset\">\n",
    "  <font face=\"XB Zar\" size=4>\n",
    "همانطور که مشاهده می‌کنید با توجه به نیازی که داریم و نحوه توزیع دیتاست، استفاده از هر یک از این مدل ها می‌تواند مقدار خطا را کمتر می‌کند.\n",
    "    </font>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\" align=\"center\">\n",
    "<font face=\"XB Zar\" size=5>\n",
    "    <font face=\"IranNastaliq\" size=5>\n",
    "      به نام خدا\n",
    "    </font>\n",
    "    <br>\n",
    "    <font size=3>\n",
    "      دانشگاه صنعتی شریف - دانشکده مهندسی کامپیوتر\n",
    "    </font>\n",
    "    <br>\n",
    "    <font color=blue size=5>\n",
    "      مقدمه‌ای بر یادگیری ماشین\n",
    "    </font>\n",
    "    <br>\n",
    "    <hr/>\n",
    "    <font color=red size=6>\n",
    "      مباحث تکمیلی:‌ Federated Learning\n",
    "    </font>\n",
    "    <br>\n",
    "      نویسندگان:‌ آرین احدی نیا\n",
    "    <hr>\n",
    "<br>\n",
    "  <div align=\"right\">\n",
    "\n",
    "  <div>\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=rtl id=\"\">\n",
    "<font face=\"XB Zar\" size=4>\n",
    "<font color=\"red\" size=5>\n",
    "مقدمه\n",
    "</font>\n",
    "<hr />\n",
    "\n",
    "Federated Learning که از این پس برای اختصار آن را FL خواهیم نامید، راهکاری برای یادگیری توزیع شده بدون به اشتراک گذاشته شدن داده ها است. در این روش به جای آنکه مستقیما داده را به اشتراک بگذاریم،‌ بردار گرادیانی به سرور ارسال می‌کنیم که سرور بر اساس آن بردار گرادیانی، مدل را به روز می‌کند. این روش به دلیل اینکه داده ها را به اشتراک نمی‌گذارد، می‌تواند برای داده های حساس مانند داده های پزشکی و داده های مربوط به اطلاعات شخصی مفید باشد. این روش در این دفترچه به صورت مختصر بررسی خواهد شد و در نهایت چالش‌ها و مشکلات آنها و برخی از راهکار‌های برای فائق آمدن بر این مشکلات ارائه خواهد شد.\n",
    "\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=rtl id=\"\">\n",
    "<font face=\"XB Zar\" size=4>\n",
    "<font color=\"red\" size=5>\n",
    "ساختار مدل\n",
    "</font>\n",
    "<hr />\n",
    "\n",
    "ایده FL اولین بار توسط McMahan در سال ۲۰‍۱۷ مطرح شد. در ابتدا لازم است که برای معرفی FL بر روی برخی تعاریف با هم به توافق برسیم.\n",
    "\n",
    "- کاربر یا Client که مدل مدل را از سرور دریافت می‌کند، آن را به روز رسانی می‌کند و بروز رسانی خود را به سرور ارسال می‌کند.\n",
    "\n",
    "- Server که مدل را در خود نگه‌داری می‌کند و با جمع‌آوری بروزرسانی ارسال شده توسط کاربران مدل را به روز می‌کند.\n",
    "\n",
    "- بردار گرادیان: در این‌جا منظور از بردار گرادیان،‌ بردار بروزرسانی‌ای است که وزن‌های مدل را بروزرسانی می‌کند.\n",
    "\n",
    "شکل زیر شمای کلی این سیستم را نشان می‌دهد.\n",
    "\n",
    "<center><img src=\"./img1.png\"></img></center>\n",
    "\n",
    "الگوریتم اولیه‌ای که برای FL ارائه شده است، FedAvg نام دارد. این الگوریتم به در هر مرحله به این صورت کار می‌کند.\n",
    "\n",
    "- سرور گروهی از کاربران را انتخاب می‌کند.\n",
    "\n",
    "- سرور مدل را به گروهی از کاربران ارسال می‌کند.\n",
    "\n",
    "- کاربران با استفاده از داده‌های خودشان مدل را آموزش می‌دهند و بروز‌رسانی اعمال شده بر روی مدل را در قالب یک بردار گرادیان به سرور ارسال می‌کنند.\n",
    "\n",
    "- سرور بردار‌های گرادیان را میانگین می‌گیرد.\n",
    "\n",
    "- سرور مدل اصلی را بروز می‌کند.\n",
    "\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=rtl id=\"\">\n",
    "<font face=\"XB Zar\" size=4>\n",
    "<font color=\"red\" size=5>\n",
    "\n",
    "پیاده‌سازی و ریاضیات\n",
    "\n",
    "</font>\n",
    "<hr />\n",
    "\n",
    "در این قسمت به پیاده‌سازی FL از پایه و توضیح قسمت‌های مختلف آن پرداخته می‌شود. این پیاده‌سازی تماما مبتنی بر PyTorch است و از هیچ کتابخانه‌ دیگری در آن استفاده نشده است.\n",
    "\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import tqdm\n",
    "import numpy as np\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=rtl id=\"\">\n",
    "<font face=\"XB Zar\" size=4>\n",
    "\n",
    "تنظیمات سیستم ما به این صورت خواهد بود که ۸ کاربر خواهیم داشت که در ۳۰ راند مدل را Train خواهند کرد. هر یک از کاربران در سه ایپاک مدل را ترین خواهند کرد و نرخ یادگیری آنها برابر 0.02 خواهد بود.\n",
    "\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_clients = 8\n",
    "rounds = 10\n",
    "epochs_per_client = 3\n",
    "learning_rate = 2e-2\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=rtl id=\"\">\n",
    "<font face=\"XB Zar\" size=4>\n",
    "<font color=\"red\" size=5>\n",
    "\n",
    "دادگان مورد استفاده\n",
    "\n",
    "</font>\n",
    "<hr />\n",
    "\n",
    "در این نوت‌بوک ما به پیاده‌سازی یک مدل برای طبقه‌بندی دادگان MNIST می‌پردازیم. در سلول زیر به بارگیری دادگان این مجموعه داده می‌پردازیم.\n",
    "\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = torchvision.datasets.MNIST(\n",
    "    \"./working\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=torchvision.transforms.transforms.ToTensor(),\n",
    ")\n",
    "test_dataset = torchvision.datasets.MNIST(\n",
    "    \"./working\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=torchvision.transforms.transforms.ToTensor(),\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=rtl id=\"\">\n",
    "<font face=\"XB Zar\" size=4>\n",
    "\n",
    "حال مجموعه داده آموزش را به نسبت ۴ به ۱ به داده آموزش و داده صحت‌سنجی برای تنظیم ابرپارامتر‌ها تقسیم می‌کنیم.\n",
    "\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "    train_dataset,\n",
    "    [\n",
    "        int(len(train_dataset) * 0.80),\n",
    "        int(len(train_dataset) * 0.20),\n",
    "    ],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_train_size = len(train_dataset)\n",
    "total_test_size = len(test_dataset)\n",
    "total_val_size = len(val_dataset)\n",
    "\n",
    "total_train_size, total_val_size, total_test_size\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=rtl id=\"\">\n",
    "<font face=\"XB Zar\" size=4>\n",
    "\n",
    "لطفا توجه بفرمایید که دادگان MNIST از ۱۰ کلاس هستند که هر یک از آنها عکسی به ابعاد ۲۸ در ۲۸ پیکسل است. هر یک از batchهای مورد استفاده شامل ۱۲۸ داده خواهد بود.\n",
    "\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = 10\n",
    "input_dim = 784\n",
    "batch_size = 128\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=rtl id=\"\">\n",
    "<font face=\"XB Zar\" size=4>\n",
    "<font color=\"red\" size=5>\n",
    "\n",
    "استفاده از سخت‌افزار مناسب\n",
    "\n",
    "</font>\n",
    "<hr />\n",
    "\n",
    "از آنجایی که مدل مورد استفاده یک مدل عمیق است، ترجیح می‌دهیم که از GPU برای آموزش مدل استفاده کنیم. بنابرین در سلول زیر بررسی می‌کنیم که اگر GPU در دسترس باشد، از آن و اگر نباشد از CPU استفاده کنیم.\n",
    "\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device():\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "        torch.device: 'cuda' if GPU is available, else 'cpu'\n",
    "    \"\"\"\n",
    "    return torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "def to_device(object, device):\n",
    "    \"\"\"\n",
    "    Recursively moves tensors to chosen device.\n",
    "    Args:\n",
    "        object: Object to move to device.\n",
    "        device (torch.device): Device to move to.\n",
    "    Returns:\n",
    "        Object: Object on chosen device.\n",
    "    \"\"\"\n",
    "    if isinstance(object, (list, tuple)):\n",
    "        return [to_device(x, device) for x in object]\n",
    "    return object.to(device, non_blocking=True)\n",
    "\n",
    "\n",
    "device = get_device()\n",
    "device\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=rtl id=\"\">\n",
    "<font face=\"XB Zar\" size=4>\n",
    "\n",
    "حال DataLoader زیر را به عنوان یک Wrapper برای دیتاست‌های عادی تولید می‌کنیم که داده را به دستگاه مورد نظر انتقال دهد.\n",
    "\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeviceDataLoader(torch.utils.data.DataLoader):\n",
    "    \"\"\"\n",
    "    Wrap a dataloader to move data to a device\n",
    "\n",
    "    Args:\n",
    "        dl (torch.utils.data.DataLoader): dataloader to wrap\n",
    "        device (torch.device): device to move data to\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dl, device):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dl (torch.utils.data.DataLoader): dataloader to wrap\n",
    "            device (torch.device): device to move data to\n",
    "        \"\"\"\n",
    "        self.dl = dl\n",
    "        self.device = device\n",
    "\n",
    "    def __iter__(self):\n",
    "        \"\"\"\n",
    "        Yields:\n",
    "            object: batch of data on chosen device\n",
    "        \"\"\"\n",
    "        for batch in self.dl:\n",
    "            yield to_device(batch, self.device)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dl)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=rtl id=\"\">\n",
    "<font face=\"XB Zar\" size=4>\n",
    "\n",
    "تابع زیر برای این منظور است که دقت خروجی بر روی یک batch محاسبه شود.\n",
    "\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_accuracy(outputs, labels):\n",
    "    \"\"\"\n",
    "    Calculate accuracy of batch of predictions.\n",
    "    Args:\n",
    "        outputs (torch.Tensor): Predictions.\n",
    "        labels (torch.Tensor): Labels.\n",
    "    Returns:\n",
    "        torch.Tensor: Accuracy.\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        _, predictions = torch.max(outputs, dim=1)\n",
    "        return torch.tensor(torch.sum(predictions == labels).item() / len(predictions))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=rtl id=\"\">\n",
    "<font face=\"XB Zar\" size=4>\n",
    "<font color=\"red\" size=5>\n",
    "\n",
    "طراحی مدل\n",
    "\n",
    "</font>\n",
    "<hr />\n",
    "\n",
    "طراحی مدل برای FL هیج تفاوتی با طراحی مدل عادی ندارد. در این مساله ما یک مدل Feed Forward ساده تعریف می‌کنیم. تنها تغییری که در این مدل ایجاد کرده‌ایم، این است که ما تعدادی از لایه‌ها را به اصطلاح ترک‌ می‌کنیم تا تغییرات آنها را برای ارسال به سرور داشته باشیم.\n",
    "\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FederatedNet(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Simple CNN for MNIST classification adapted for federated learning with tracking of layers.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Tracks layers for later use in federated learning.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.conv1 = torch.nn.Conv2d(1, 20, 7)\n",
    "        self.conv2 = torch.nn.Conv2d(20, 40, 7)\n",
    "        self.maxpool = torch.nn.MaxPool2d(2, 2)\n",
    "        self.flatten = torch.nn.Flatten()\n",
    "        self.linear = torch.nn.Linear(2560, 10)\n",
    "        self.non_linearity = torch.nn.ReLU()\n",
    "        self.track_layers = {\n",
    "            \"conv1\": self.conv1,\n",
    "            \"conv2\": self.conv2,\n",
    "            \"linear\": self.linear,\n",
    "        }\n",
    "\n",
    "    def forward(self, x_batch):\n",
    "        \"\"\"\n",
    "        Forward pass of the network.\n",
    "        Args:\n",
    "            x_batch (torch.Tensor): Input batch.\n",
    "        Returns:\n",
    "            torch.Tensor: Output of the network.\n",
    "        \"\"\"\n",
    "        return torch.nn.Sequential(\n",
    "            self.conv1,\n",
    "            self.non_linearity,\n",
    "            self.conv2,\n",
    "            self.non_linearity,\n",
    "            self.maxpool,\n",
    "            self.flatten,\n",
    "            self.linear,\n",
    "        )(x_batch)\n",
    "\n",
    "    def get_track_layers(self):\n",
    "        return self.track_layers\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=rtl id=\"\">\n",
    "<font face=\"XB Zar\" size=4>\n",
    "\n",
    "تابع زیر یک batch را به عنوان ورودی به مدل می‌دهد و دقت و loss را بر روی آن محاسبه می‌کند.\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_batch(model, batch):\n",
    "    \"\"\"\n",
    "    Process a batch of data.\n",
    "    Args:\n",
    "        model (torch.nn.Module): Model to use for processing.\n",
    "        batch (torch.Tensor): Batch of data.\n",
    "    Returns:\n",
    "        torch.Tensor: Loss.\n",
    "        torch.Tensor: Accuracy.\n",
    "    \"\"\"\n",
    "    images, labels = batch\n",
    "    outputs = model(images)\n",
    "    loss = torch.nn.functional.cross_entropy(outputs, labels)\n",
    "    accuracy = batch_accuracy(outputs, labels)\n",
    "    return (loss, accuracy)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=rtl id=\"\">\n",
    "<font face=\"XB Zar\" size=4>\n",
    "\n",
    "تابع زیر وظیفه آموزش مدل بر روی یک مجموعه داده را دارد. این تابع با آموزش عادی مدل عمیق تفاوتی ندارد.\n",
    "\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model, dataset, epochs, lr, batch_size=batch_size, opt=torch.optim.SGD):\n",
    "    \"\"\"\n",
    "    Fit a model to a dataset.\n",
    "    Args:\n",
    "        model (torch.nn.Module): Model to fit.\n",
    "        dataset (torch.utils.data.Dataset): Dataset to fit to.\n",
    "        epochs (int): Number of epochs to train for.\n",
    "        lr (float): Learning rate.\n",
    "        batch_size (int): Batch size.\n",
    "        opt (torch.optim.Optimizer): Optimizer to use.\n",
    "    Returns:\n",
    "        list: List of tuples of (loss, accuracy) for each epoch.\n",
    "    \"\"\"\n",
    "    dataloader = DeviceDataLoader(\n",
    "        torch.utils.data.DataLoader(dataset, batch_size, shuffle=True), device\n",
    "    )\n",
    "    optimizer = opt(model.parameters(), lr)\n",
    "    history = []\n",
    "    for epoch in range(epochs):\n",
    "        losses = []\n",
    "        accs = []\n",
    "        for batch in dataloader:\n",
    "            loss, acc = process_batch(model, batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            loss.detach()\n",
    "            losses.append(loss)\n",
    "            accs.append(acc)\n",
    "        avg_loss = torch.stack(losses).mean().item()\n",
    "        avg_acc = torch.stack(accs).mean().item()\n",
    "        history.append((avg_loss, avg_acc))\n",
    "    return history\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=rtl id=\"\">\n",
    "<font face=\"XB Zar\" size=4>\n",
    "\n",
    "همان‌گونه که می‌دانید در FL کاربران در هر مرحله مدل را از سرور دریافت می‌کنند و آن مدل را آموزش می‌دهد. تابع زیر برای این منظور است. این تابع با دریافت پارامتر‌های یک مدل، آن ها را با پارامتر‌های فعلی مدل جایگزین می‌کند.\n",
    "\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_parameters(model, parameters_dict):\n",
    "    \"\"\"\n",
    "    Apply parameters to model.\n",
    "    Args:\n",
    "        model (torch.nn.Module): Model to apply parameters to.\n",
    "        parameters_dict (dict): Dictionary of parameters.\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        for layer_name in parameters_dict:\n",
    "            model.track_layers[layer_name].weight.data *= 0\n",
    "            model.track_layers[layer_name].bias.data *= 0\n",
    "            model.track_layers[layer_name].weight.data += parameters_dict[layer_name][\n",
    "                \"weight\"\n",
    "            ]\n",
    "            model.track_layers[layer_name].bias.data += parameters_dict[layer_name][\n",
    "                \"bias\"\n",
    "            ]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=rtl id=\"\">\n",
    "<font face=\"XB Zar\" size=4>\n",
    "\n",
    "تابع زیر به این منظور است که پارامتر‌های مدل را استخراج کند و آنها را در قالب یک دیکشنری در بیاورد که بتوان آنها بین کاربر و سرور جابجا کرد.\n",
    "\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parameters(model):\n",
    "    \"\"\"\n",
    "    Get parameters from model.\n",
    "    Args:\n",
    "        model (torch.nn.Module): Model to get parameters from.\n",
    "    Returns:\n",
    "        dict: Dictionary of parameters.\n",
    "    \"\"\"\n",
    "    parameters_dict = dict()\n",
    "    for layer_name in model.track_layers:\n",
    "        parameters_dict[layer_name] = {\n",
    "            \"weight\": model.track_layers[layer_name].weight.data,\n",
    "            \"bias\": model.track_layers[layer_name].bias.data,\n",
    "        }\n",
    "    return parameters_dict\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=rtl id=\"\">\n",
    "<font face=\"XB Zar\" size=4>\n",
    "\n",
    "این تابع برای بررسی دقت مدل است که به عنوان خروجی loss و دقت مدل را بر روی داده تست بر‌میگرداند.\n",
    "\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataset, batch_size=batch_size):\n",
    "    \"\"\"\n",
    "    Evaluate a model on a dataset.\n",
    "    Args:\n",
    "        model (torch.nn.Module): Model to evaluate.\n",
    "        dataset (torch.utils.data.Dataset): Dataset to evaluate on.\n",
    "        batch_size (int): Batch size.\n",
    "    Returns:\n",
    "        tuple: Tuple of (loss, accuracy).\n",
    "    \"\"\"\n",
    "    dataloader = DeviceDataLoader(\n",
    "        torch.utils.data.DataLoader(dataset, batch_size), device\n",
    "    )\n",
    "    losses = []\n",
    "    accs = []\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            loss, acc = process_batch(model, batch)\n",
    "            losses.append(loss)\n",
    "            accs.append(acc)\n",
    "    avg_loss = torch.stack(losses).mean().item()\n",
    "    avg_acc = torch.stack(accs).mean().item()\n",
    "    return (avg_loss, avg_acc)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=rtl id=\"\">\n",
    "<font face=\"XB Zar\" size=4>\n",
    "<font color=\"red\" size=5>\n",
    "\n",
    "مدل کاربر\n",
    "\n",
    "</font>\n",
    "<hr />\n",
    "\n",
    "حال مدل مورد استفاده برای هر کاربر را پیاده‌سازی می‌کنیم. هر کاربر یک مدل و یک مجموعه داده دارد. فرآیند آموزش هر کاربر به این صورت است که ابتدا وزن‌ها را به عنوان ورودی از سرور دریافت می‌کند، سپس این وزن‌ها به بر روی شبکه اعمال می‌شود و بر روی آن وزن‌ها مدل آموزش داده می‌شود. در نهایت خروجی مرحله آموزش به عنوان وزن‌های بروز شده به سرور ارسال می‌شود.\n",
    "\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Client:\n",
    "    \"\"\"\n",
    "    Client class.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, client_id, model, dataset):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            client_id (int): Client ID.\n",
    "            model (torch.nn.Module): Model.\n",
    "            dataset (torch.utils.data.Dataset): Dataset.\n",
    "        \"\"\"\n",
    "        self.client_id = client_id\n",
    "        self.model = to_device(model, device)\n",
    "        self.dataset = dataset\n",
    "        self.history = []\n",
    "\n",
    "    def get_dataset_size(self):\n",
    "        \"\"\"\n",
    "        Get dataset size.\n",
    "        Returns:\n",
    "            int: Dataset size.\n",
    "        \"\"\"\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def get_client_id(self):\n",
    "        \"\"\"\n",
    "        Get client ID.\n",
    "        Returns:\n",
    "            int: Client ID.\n",
    "        \"\"\"\n",
    "        return self.client_id\n",
    "\n",
    "    def train(self, parameters_dict):\n",
    "        \"\"\"\n",
    "        Train the model.\n",
    "        Args:\n",
    "            parameters_dict (dict): Dictionary of parameters.\n",
    "        Returns:\n",
    "            dict: Dictionary of parameters.\n",
    "        \"\"\"\n",
    "        apply_parameters(self.model, parameters_dict)\n",
    "        train_history = fit(\n",
    "            self.model,\n",
    "            self.dataset,\n",
    "            epochs_per_client,\n",
    "            learning_rate,\n",
    "            batch_size,\n",
    "        )\n",
    "        self.history.extend(train_history)\n",
    "        print(\n",
    "            f\"{self.client_id}: Loss = {round(train_history[-1][0], 4)}, Accuracy = {round(train_history[-1][1], 4)}\"\n",
    "        )\n",
    "        return get_parameters(self.model)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=rtl id=\"\">\n",
    "<font face=\"XB Zar\" size=4>\n",
    "<font color=\"red\" size=5>\n",
    "\n",
    "مجموعه دادگاه کاربر\n",
    "\n",
    "</font>\n",
    "<hr />\n",
    "\n",
    "حال در این سلول مجموعه دادگان در هم ریخته را تقسیم‌بندی می‌کنیم و به هر کاربر یک بخش از آن را می‌دهیم.\n",
    "\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples_per_client = total_train_size // num_clients\n",
    "client_datasets = torch.utils.data.random_split(\n",
    "    train_dataset,\n",
    "    [\n",
    "        min(i + examples_per_client, total_train_size) - i\n",
    "        for i in range(0, total_train_size, examples_per_client)\n",
    "    ],\n",
    ")\n",
    "clients = [\n",
    "    Client(\"client_\" + str(i), FederatedNet(), client_datasets[i])\n",
    "    for i in range(num_clients)\n",
    "]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=rtl id=\"\">\n",
    "<font face=\"XB Zar\" size=4>\n",
    "<font color=\"red\" size=5>\n",
    "\n",
    "\n",
    "\n",
    "</font>\n",
    "<hr />\n",
    "\n",
    "\n",
    "\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_net = to_device(FederatedNet(), device)\n",
    "history = []\n",
    "for i in range(rounds):\n",
    "    print(\"Start Round {} ...\".format(i + 1))\n",
    "    curr_parameters = get_parameters(global_net)\n",
    "    new_parameters = dict(\n",
    "        [(layer_name, {\"weight\": 0, \"bias\": 0}) for layer_name in curr_parameters]\n",
    "    )\n",
    "    for client in clients:\n",
    "        client_parameters = client.train(curr_parameters)\n",
    "        fraction = client.get_dataset_size() / total_train_size\n",
    "        for layer_name in client_parameters:\n",
    "            new_parameters[layer_name][\"weight\"] += (\n",
    "                fraction * client_parameters[layer_name][\"weight\"]\n",
    "            )\n",
    "            new_parameters[layer_name][\"bias\"] += (\n",
    "                fraction * client_parameters[layer_name][\"bias\"]\n",
    "            )\n",
    "    apply_parameters(global_net, new_parameters)\n",
    "\n",
    "    train_loss, train_acc = evaluate(global_net, train_dataset)\n",
    "    dev_loss, dev_acc = evaluate(global_net, val_dataset)\n",
    "    print(\n",
    "        \"After round {}, train_loss = {}, dev_loss = {}, dev_acc = {}\\n\".format(\n",
    "            i + 1, round(train_loss, 4), round(dev_loss, 4), round(dev_acc, 4)\n",
    "        )\n",
    "    )\n",
    "    history.append((train_loss, dev_loss))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=rtl id=\"\">\n",
    "<font face=\"XB Zar\" size=4>\n",
    "<font color=\"red\" size=5>\n",
    "\n",
    "نمودار خطا\n",
    "\n",
    "</font>\n",
    "<hr />\n",
    "\n",
    "همان‌طور که ملاحظه می‌کنید، این روش در مرور زمان همگرا می‌شود.\n",
    "\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "plt.plot(\n",
    "    [i + 1 for i in range(len(history))],\n",
    "    [history[i][0] for i in range(len(history))],\n",
    "    color=\"r\",\n",
    "    label=\"train loss\",\n",
    ")\n",
    "plt.plot(\n",
    "    [i + 1 for i in range(len(history))],\n",
    "    [history[i][1] for i in range(len(history))],\n",
    "    color=\"b\",\n",
    "    label=\"val loss\",\n",
    ")\n",
    "plt.legend()\n",
    "plt.title(\"Training history\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT THE TRAINING HISTORY OF CLIENTS\n",
    "fig, axs = plt.subplots(2, 4, figsize=(20, 10))\n",
    "for i in range(num_clients):\n",
    "    axs[i // 4, i % 4].plot(\n",
    "        [j + 1 for j in range(len(clients[i].history))],\n",
    "        [clients[i].history[j][0] for j in range(len(clients[i].history))],\n",
    "        color=\"r\",\n",
    "        label=\"train loss\",\n",
    "    )\n",
    "    axs[i // 4, i % 4].plot(\n",
    "        [j + 1 for j in range(len(clients[i].history))],\n",
    "        [clients[i].history[j][1] for j in range(len(clients[i].history))],\n",
    "        color=\"b\",\n",
    "        label=\"train acc\",\n",
    "    )\n",
    "    axs[i // 4, i % 4].set_title(\"Client {}\".format(i + 1))\n",
    "    axs[i // 4, i % 4].legend()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=rtl id=\"\">\n",
    "<font face=\"XB Zar\" size=4>\n",
    "<font color=\"red\" size=5>\n",
    "\n",
    "ناهمگنی داده\n",
    "\n",
    "</font>\n",
    "<hr />\n",
    "\n",
    "در این محیط بسیار محتمل است که داده کاربران مختلف از یک توزیع مشابه نباشند. این مورد می‌تواند برای دقت و محرمانگی مدل چالش ایجاد کند.\n",
    "\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=rtl id=\"\">\n",
    "<font face=\"XB Zar\" size=4>\n",
    "\n",
    "برای این سناریو ابتدا داده ناهمگن سنتز می‌کنیم. این داده به این صورت است که هر کاربر داده‌ای از یک کلاس به خصوص دارد.\n",
    "\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_with_labels = []\n",
    "for i in tqdm.tqdm(range(10)):\n",
    "    data_with_labels.append(\n",
    "        torch.utils.data.Subset(\n",
    "            train_dataset,\n",
    "            [j for j in range(len(train_dataset)) if train_dataset[j][1] == i],\n",
    "        )\n",
    "    )\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=rtl id=\"\">\n",
    "<font face=\"XB Zar\" size=4>\n",
    "\n",
    "حال کلاسی را تعریف می‌کنیم تا این داده ناهمگن را به عنوان دیتاست لفاف‌بندی کند.\n",
    "\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeteroDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    Heterogeneous dataset.\n",
    "    Args:\n",
    "        dataset (torch.utils.data.Dataset): Dataset.\n",
    "        label (int): Label.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataset, label):\n",
    "        \"\"\"\n",
    "        Heterogeneous dataset.\n",
    "        Args:\n",
    "            dataset (torch.utils.data.Dataset): Dataset.\n",
    "            label (int): Label.\n",
    "        \"\"\"\n",
    "        self.dataset = dataset\n",
    "        self.label = label\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Get item.\n",
    "        Args:\n",
    "            index (int): Index.\n",
    "        Returns:\n",
    "            tuple: Tuple of (data, label).\n",
    "        \"\"\"\n",
    "        return self.dataset[index][0], self.label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=rtl id=\"\">\n",
    "<font face=\"XB Zar\" size=4>\n",
    "\n",
    "کاربران ساخته شده هر یک داده‌ای از سه کلاس به خصوص خواهند داشت.\n",
    "\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clients = [\n",
    "    Client(\"client_\" + str(i), FederatedNet(), HeteroDataset(data_with_labels[i] + data_with_labels[i + 1] + data_with_labels[i + 2], i))\n",
    "    for i in range(8)\n",
    "]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=rtl id=\"\">\n",
    "<font face=\"XB Zar\" size=4>\n",
    "\n",
    "حال مجددا آموزش مدل را با این دادگان جدید انجام خواهیم داد.\n",
    "\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_net = to_device(FederatedNet(), device)\n",
    "history = []\n",
    "for i in range(10):\n",
    "    print(\"Start Round {} ...\".format(i + 1))\n",
    "    curr_parameters = get_parameters(global_net)\n",
    "    new_parameters = dict(\n",
    "        [(layer_name, {\"weight\": 0, \"bias\": 0}) for layer_name in curr_parameters]\n",
    "    )\n",
    "    for client in clients:\n",
    "        client_parameters = client.train(curr_parameters)\n",
    "        fraction = client.get_dataset_size() / total_train_size\n",
    "        for layer_name in client_parameters:\n",
    "            new_parameters[layer_name][\"weight\"] += (\n",
    "                fraction * client_parameters[layer_name][\"weight\"]\n",
    "            )\n",
    "            new_parameters[layer_name][\"bias\"] += (\n",
    "                fraction * client_parameters[layer_name][\"bias\"]\n",
    "            )\n",
    "    apply_parameters(global_net, new_parameters)\n",
    "\n",
    "    train_loss, train_acc = evaluate(global_net, train_dataset)\n",
    "    dev_loss, dev_acc = evaluate(global_net, val_dataset)\n",
    "    print(\n",
    "        \"After round {}, train_loss = {}, dev_loss = {}, dev_acc = {}\\n\".format(\n",
    "            i + 1, round(train_loss, 4), round(dev_loss, 4), round(dev_acc, 4)\n",
    "        )\n",
    "    )\n",
    "    history.append((train_loss, dev_loss))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=rtl id=\"\">\n",
    "<font face=\"XB Zar\" size=4>\n",
    "\n",
    "همان‌طور که ملاحظه می‌کنید دقت و عملکرد مدل دچار کاهش چشم‌گیری شده است. این به این دلیل است که آپدیت‌های مدل که از سمت کاربران ارسال می‌شود، بدون در نظر گرفتن ناهمگنی مدل بلکه تنها با در نظر گرفتن بهینه کردن مدل برای داده خودشان است.\n",
    "\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=rtl id=\"\">\n",
    "<font face=\"XB Zar\" size=4>\n",
    "<font color=\"red\" size=5>\n",
    "\n",
    "حمله بازسازی ورودی\n",
    "\n",
    "</font>\n",
    "<hr />\n",
    "\n",
    "حال می‌خواهیم حمله بازسازی ورودی را مطرح کنیم. همان‌طور که مستحضر هستید هدف اصلی یادگیری فدرال حفظ حریم خصوصی با عدم ارسال داده به سرور بود. اما در این حمله نشان داده می‌شود که امکان بازسازی تصویر ورودی وجود دارد.\n",
    "\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=rtl id=\"\">\n",
    "<font face=\"XB Zar\" size=4>\n",
    "\n",
    "در این قسمت نت‌ورکی که حمله بر روی آن صورت می‌گیرد را پیاده‌سازی می‌کنیم. این نتورک بسیار ساده‌ است و از سه لایه خطی تشکیل شده است.\n",
    "\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TargetNetwork(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TargetNetwork, self).__init__()\n",
    "        self.first_part = torch.nn.Sequential(\n",
    "            torch.nn.Linear(784, 500),\n",
    "            torch.nn.ReLU(),\n",
    "        )\n",
    "        self.second_part = torch.nn.Sequential(\n",
    "            torch.nn.Linear(500, 500),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(500, 10),\n",
    "            torch.nn.Softmax(dim=-1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.second_part(self.first_part(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_network = to_device(TargetNetwork(), device)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=rtl id=\"\">\n",
    "<font face=\"XB Zar\" size=4>\n",
    "\n",
    "در این قسمت مدل حمله‌کننده را پیاده‌سازی می‌کنیم. این مدل خروجی لایه اول را دریافت می‌کند و با استفاده از دو لایه‌ای که خود دارد، خروجی مدل را بازسازی می‌کند.\n",
    "\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attacker(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Attacker.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Attacker, self).__init__()\n",
    "        self.layers = torch.nn.Sequential(\n",
    "            torch.nn.Linear(500, 1000),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(1000, 784),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward.\n",
    "        Args:\n",
    "            x (torch.Tensor): Input.\n",
    "        Returns:\n",
    "            torch.Tensor: Output.\n",
    "        \"\"\"\n",
    "        return self.layers(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attacker = Attacker().to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=rtl id=\"\">\n",
    "<font face=\"XB Zar\" size=4>\n",
    "\n",
    "حال مجموعه داده را بارگیری می‌کنیم.\n",
    "\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with flatten transform and to tensor\n",
    "train_dataset = torchvision.datasets.MNIST(\n",
    "    root=\"./working/\",\n",
    "    train=True,\n",
    "    transform=torchvision.transforms.Compose(\n",
    "        [\n",
    "            torchvision.transforms.ToTensor(),\n",
    "            torchvision.transforms.Lambda(lambda x: x.flatten()),\n",
    "        ]\n",
    "    ),\n",
    "    download=True,\n",
    ")\n",
    "test_dataset = torchvision.datasets.MNIST(\n",
    "    root=\"./working/\",\n",
    "    train=False,\n",
    "    transform=torchvision.transforms.Compose(\n",
    "        [\n",
    "            torchvision.transforms.ToTensor(),\n",
    "            torchvision.transforms.Lambda(lambda x: x.flatten()),\n",
    "        ]\n",
    "    ),\n",
    "    download=True,\n",
    ")\n",
    "\n",
    "train_dataloader = DeviceDataLoader(\n",
    "    torch.utils.data.DataLoader(train_dataset, batch_size), device\n",
    ")\n",
    "test_dataloader = DeviceDataLoader(\n",
    "    torch.utils.data.DataLoader(test_dataset, batch_size), device\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=rtl id=\"\">\n",
    "<font face=\"XB Zar\" size=4>\n",
    "\n",
    "حال مدل اصلی رو آموزش می‌دهیم.\n",
    "\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(target_network.parameters(), lr=0.001)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "for epoch in range(10):\n",
    "    losses = []\n",
    "    for batch in train_dataloader:\n",
    "        images, labels = batch\n",
    "        optimizer.zero_grad()\n",
    "        outputs = target_network(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "    print(\"Epoch {}, loss = {}\".format(epoch + 1, round(np.mean(losses), 4)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the accuracy\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for batch in test_dataloader:\n",
    "        images, labels = batch\n",
    "        outputs = target_network(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "print(\"Accuracy = {}\".format(round(correct / total, 4)))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=rtl id=\"\">\n",
    "<font face=\"XB Zar\" size=4>\n",
    "\n",
    "حال مدل حمله‌کننده را با استفاده از لایه اول مدل آموزش می‌دهیم.\n",
    "\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(attacker.parameters(), lr=1e-4)\n",
    "\n",
    "for data, targets in tqdm.tqdm(train_dataloader):\n",
    "    data = data.to(device)\n",
    "    targets = targets.to(device)\n",
    "    optimizer.zero_grad()\n",
    "    target_outputs = target_network.first_part(data)\n",
    "    attack_outputs = attacker(target_outputs)\n",
    "    loss = ((data - attack_outputs) ** 2).mean()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=rtl id=\"\">\n",
    "<font face=\"XB Zar\" size=4>\n",
    "\n",
    "حال میخواهیم داده‌های ذخیره شده در مدل را بازسازی کنیم.\n",
    "\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data, targets in test_dataloader:\n",
    "    target_outputs = target_network.first_part(data)\n",
    "    recreated_data = attacker(target_outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot 10 reconstructed images\n",
    "plt.figure(figsize=(10, 10))\n",
    "for i in range(10):\n",
    "    plt.subplot(5, 5, i + 1)\n",
    "    plt.imshow(recreated_data[i].cpu().detach().numpy().reshape(28, 28))\n",
    "    plt.axis(\"off\")\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=rtl id=\"\">\n",
    "<font face=\"XB Zar\" size=4>\n",
    "\n",
    "همان‌طور که ملاحظه می‌کنید داده‌های ذخیره شده در مدل با دقت بالایی بازسازی شده‌اند.\n",
    "\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[1] Li, Q., Wen, Z., Wu, Z., Hu, S., Wang, N., Li, Y., ... & He, B. (2021). A survey on federated learning systems: vision, hype and reality for data privacy and protection. IEEE Transactions on Knowledge and Data Engineering.\n",
    "\n",
    "[2] Lyu, L., Yu, H., & Yang, Q. (2020). Threats to federated learning: A survey. arXiv preprint arXiv:2003.02133."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "287ee184d78959fade4213e06aa8453c71d17316fcbc60aa682b94ecd5afe758"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

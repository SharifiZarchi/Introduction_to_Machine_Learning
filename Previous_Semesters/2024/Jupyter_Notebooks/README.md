This is an overview of notebooks from all chapters.

----

# [Chapter 1: Supervised Learning](https://github.com/SharifiZarchi/Introduction_to_Machine_Learning/tree/main/Jupyter_Notebooks/Chapter_01_Supervised_Learning)
In this chapter we over the basics of Supervised Learning, where we train models using labeled data.
To learn more about notebooks from this chapter check out [here](https://github.com/SharifiZarchi/Introduction_to_Machine_Learning/blob/main/Jupyter_Notebooks/Chapter_01_Supervised_Learning/README.md).

## [1. Linear Regression](https://github.com/SharifiZarchi/Introduction_to_Machine_Learning/blob/main/Jupyter_Notebooks/Chapter_01_Supervised_Learning/01-Linear%20Regression/01-Linear_Regression.ipynb)
In this notebook we solve the problem of Regression from scratch using only numpy. We also experiment wih Polynomial Regression and use sklearn to solve real-world problems.

## [2. Linear Classification](https://github.com/SharifiZarchi/Introduction_to_Machine_Learning/blob/main/Jupyter_Notebooks/Chapter_01_Supervised_Learning/02-Linear%20Classification/02-Linear_Classification.ipynb)
In this notebook we use Regression to solve binary classification problems and understand their limitations.

## [3. Logistic Regression](https://github.com/SharifiZarchi/Introduction_to_Machine_Learning/blob/main/Jupyter_Notebooks/Chapter_01_Supervised_Learning/03-Logistic%20Regression/03-Logistic_Regression.ipynb)
In this notebook we learn about Logistic Regression and repurpose Regeression to classify data from small datasets.

## [4. K-Nearest Neighbors](https://github.com/SharifiZarchi/Introduction_to_Machine_Learning/blob/main/Jupyter_Notebooks/Chapter_01_Supervised_Learning/04-kNN/04-kNN.ipynb)
In this notebook we use implement the kNN algorithm from scratch and use it for classification. 

## [5. Ensemble Learning](https://github.com/SharifiZarchi/Introduction_to_Machine_Learning/blob/main/Jupyter_Notebooks/Chapter_01_Supervised_Learning/05-Ensemble%20Learning/05-Ensemble_Learning.ipynb)
In this notebook we use implement the Decision Tree Classifiers from scratch and explore how ensembling different models improve classification. 
We also work with Random Forests and XGBoost and compare their results.

----

# [Chapter 2: Unsupervised Learning](https://github.com/SharifiZarchi/Introduction_to_Machine_Learning/tree/main/Jupyter_Notebooks/Chapter_02_Unsupervised_Learning)
In this chapter we over the basics of Unsupervised Learning, where we train models using unlabeled data.
To learn more about notebooks from this chapter check out [here](https://github.com/SharifiZarchi/Introduction_to_Machine_Learning/blob/main/Jupyter_Notebooks/Chapter_02_Unsupervised_Learning/README.md).

## [1. Clustering](https://github.com/SharifiZarchi/Introduction_to_Machine_Learning/blob/main/Jupyter_Notebooks/Chapter_02_Unsupervised_Learning/Clustering.ipynb)
In this notebook we implement the K-Means algorithm from scratch and see how clustering can be applied to unlabeled datasets.

## [2. Dimensionality Reduction](https://github.com/SharifiZarchi/Introduction_to_Machine_Learning/blob/main/Jupyter_Notebooks/Chapter_02_Unsupervised_Learning/DimensionalityReduction.ipynb)
In this notebook we go over different Dimensionality Reduction algorithms such as Principal Component Analysis (PCA), t-Distributed Stochastic Neighbor Embedding (t-SNE) and Uniform Manifold Approximation and Projection (UMAP).

----

# [Chapter 3: Neural Networks](https://github.com/SharifiZarchi/Introduction_to_Machine_Learning/tree/main/Jupyter_Notebooks/Chapter_03_Neural_Networks)
In this chapter we over the basics of Neural Networks, which are the building blocks of modern Deep Learning.
To learn more about notebooks from this chapter check out [here](https://github.com/SharifiZarchi/Introduction_to_Machine_Learning/blob/main/Jupyter_Notebooks/Chapter_03_Neural_Networks/README.md).

## [1. Neural Networks from Scratch](https://github.com/SharifiZarchi/Introduction_to_Machine_Learning/blob/main/Jupyter_Notebooks/Chapter_03_Neural_Networks/NNs_from_scratch.ipynb)
In this notebook we implement and train a neural network from scratch using only numpy!
Additionally we experiment with different batch sizes.

## [2. Neural Networks with PyTorch](https://github.com/SharifiZarchi/Introduction_to_Machine_Learning/blob/main/Jupyter_Notebooks/Chapter_03_Neural_Networks/NNs_with_torch.ipynb)
In this notebook we are going to implement and train a neural network with PyTorch!
Our goal is to learn the basics of PyTorch.
Additionally we experiment with different optimizers such as `SGD`, `AdaGrad`, `RMSProp`, and `Adam`.

## [3. More on Neural Networks with PyTorch](https://github.com/SharifiZarchi/Introduction_to_Machine_Learning/blob/main/Jupyter_Notebooks/Chapter_03_Neural_Networks/More_on_NNs_with_Torch.ipynb)
In this notebook we are going to explore some of the more advanced features of PyTorch. 
This includes working with `Dataset`s, `DataLoader`s, saving and loading models and more.

## [4. Improving Neural Networks](https://github.com/SharifiZarchi/Introduction_to_Machine_Learning/blob/main/Jupyter_Notebooks/Chapter_03_Neural_Networks/Improving_NNs.ipynb)
In this notebook we are going to start with a dataset, and step by step, try and improve our models. 
We will build on what we have learned from the previous lectures and notebooks and use these lessons to obtain better results!
We try to get better results using different optimizers, bigger networks, batch normalization layers, dropout layers and hyperparameter tuning.

----

# [Chapter 4: Compuer Vision](https://github.com/SharifiZarchi/Introduction_to_Machine_Learning/blob/main/Jupyter_Notebooks/Chapter_04_Compuer_Vision)
In this chapter we over the basics of Computer Vision, exploring how neural networks can be modified to solve different problems in Computer Vision.
To learn more about notebooks from this chapter check out [here](https://github.com/SharifiZarchi/Introduction_to_Machine_Learning/blob/main/Jupyter_Notebooks/Chapter_04_Compuer_Vision/README.md).

## [1. Convolutional Neural Networks from Scratch](https://github.com/SharifiZarchi/Introduction_to_Machine_Learning/blob/main/Jupyter_Notebooks/Chapter_04_Computer_Vision/CNNs_from_scratch.ipynb)
In this notebook we implement and train a convolutional neural network from scratch using only numpy, based on [our implementation from the previous chapter](https://github.com/SharifiZarchi/Introduction_to_Machine_Learning/blob/main/Jupyter_Notebooks/Chapter_03_Neural_Networks/NNs_from_scratch.ipynb).
Additionally we implement the same model using PyTorch.

## [2. AlexNet Architecture](https://github.com/SharifiZarchi/Introduction_to_Machine_Learning/blob/main/Jupyter_Notebooks/Chapter_04_Computer_Vision/AlexNet.ipynb)
In this notebook we implement the [AlexNet](http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks) architecture in PyTorch.
Moreover we go over different augmentation techniques that are widely used on image datasets.

## [3. Residual Networks and Transfer Learning](https://github.com/SharifiZarchi/Introduction_to_Machine_Learning/blob/main/Jupyter_Notebooks/Chapter_04_Computer_Vision/ResNets_Transfer_Learning.ipynb)
In this notebook we will explore the effect of residual connections in deep convolutional neural networks and experiment with the ResNet architecture.
Additionally we observe how transfer learning can help us improve the training process by using pretrained models.

## [4. AutoEncoders](https://github.com/SharifiZarchi/Introduction_to_Machine_Learning/blob/main/Jupyter_Notebooks/Chapter_04_Computer_Vision/AutoEncoders.ipynb)
In this notebook we experiment with AutoEncoders and observe how they can reduce the dimensionality of the data by visualizing their latent space.
We also work with Denoising AutoEncoders and see how AutoEncoders can be used for getting rid of unwanted noise.

## [5. U-Net](https://github.com/SharifiZarchi/Introduction_to_Machine_Learning/blob/main/Jupyter_Notebooks/Chapter_04_Computer_Vision/UNet.ipynb)
In this notebook we introduce the U-Net architecture and investigate its use cases in different tasks of computer vision.

----

# [Chapter 5: Natural Language Processing](https://github.com/SharifiZarchi/Introduction_to_Machine_Learning/blob/main/Jupyter_Notebooks/Chapter_05_Natural_Language_Processing)
In this chapter we over the basics of Natural Language Processing, exploring how neural networks can be modified to solve different problems in NLP.
To learn more about notebooks from this chapter check out [here](https://github.com/SharifiZarchi/Introduction_to_Machine_Learning/blob/main/Jupyter_Notebooks/Chapter_05_Natural_Language_Processing/README.md).

## [1. Word Embeddings](https://github.com/SharifiZarchi/Introduction_to_Machine_Learning/blob/main/Jupyter_Notebooks/Chapter_05_Natural_Language_Processing/01-Word%20Embedding/Word%20Embedding.ipynb)
In this notebook we explore how words can be embedded as a dense representation in the form of numeric vectors.

## [2. Attention and Transformers](https://github.com/SharifiZarchi/Introduction_to_Machine_Learning/blob/main/Jupyter_Notebooks/Chapter_05_Natural_Language_Processing/Attention.ipynb)
In this notebook, we are going to implement the Attention layers from scratch using PyTorch and learn more about Transformers.

----

# [Chapter 6: Contrastive Learning](https://github.com/SharifiZarchi/Introduction_to_Machine_Learning/tree/main/Jupyter_Notebooks/Chapter_06_Contrastive_Learning)
In this chapter we over the basics of Contrastive Learning, exploring how models trained on images and languages can be used together to enhance the performance of our models.
To learn more about notebooks from this chapter check out [here](https://github.com/SharifiZarchi/Introduction_to_Machine_Learning/blob/main/Jupyter_Notebooks/Chapter_06_Contrastive_Learning/README.md).

## [1. Vision Transformers](https://github.com/SharifiZarchi/Introduction_to_Machine_Learning/blob/main/Jupyter_Notebooks/Chapter_06_Contrastive_Learning/ViT.ipynb)
In this notebook we implement and train a [Vision Transformer (ViT)](https://arxiv.org/abs/2010.11929) using PyTorch.

## [2. Contrastive Language-Image Pre-training](https://github.com/SharifiZarchi/Introduction_to_Machine_Learning/blob/main/Jupyter_Notebooks/Chapter_06_Contrastive_Learning/CLIP.ipynb)
In this notebook we explore how using pre-trained models on images and languages can help improve our performance of a variety of different tasks according to the [CLIP](https://openai.com/index/clip/) model.

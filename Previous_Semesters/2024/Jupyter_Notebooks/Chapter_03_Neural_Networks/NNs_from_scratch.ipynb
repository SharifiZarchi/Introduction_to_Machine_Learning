{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ecaqr9YkWpbL"
   },
   "source": [
    "# Neural Networks from Scratch\n",
    "\n",
    "**[CE477: Machine Learning](https://www.sharifml.ir/)**\n",
    "\n",
    "__Course Instructor__: Dr. Sharifi-Zarchi\n",
    "\n",
    "__Notebook Author__: Ramtin Moslemi\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/SharifiZarchi/Introduction_to_Machine_Learning/blob/main/Jupyter_Notebooks/Chapter_03_Neural_Networks/NNs_from_scratch.ipynb)\n",
    "[![Open In kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://raw.githubusercontent.com/SharifiZarchi/Introduction_to_Machine_Learning/main/Jupyter_Notebooks/Chapter_03_Neural_Networks/NNs_from_scratch.ipynb)\n",
    "\n",
    "---\n",
    "## Notebook Objectives\n",
    "\n",
    "\n",
    "In this notebook we are going to implement and train a neural network from scratch using only numpy!\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "PqFOwWLZVPI0"
   },
   "outputs": [],
   "source": [
    "# @title setup and imports\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tqdm import trange\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "9AKM0RgiVSOQ"
   },
   "outputs": [],
   "source": [
    "# @title helper functions\n",
    "\n",
    "def plot_training(losses):\n",
    "    # Plot the loss\n",
    "    plt.plot(losses)\n",
    "    plt.title(\"Training loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, class_names, kept_classes):\n",
    "    dim = len(kept_classes)\n",
    "    labels = [class_names[i] for i in kept_classes]\n",
    "    # Plot the confusion matrix\n",
    "    conf_mat = confusion_matrix(y_true, y_pred)\n",
    "    norm_conf_mat = conf_mat / np.sum(conf_mat, axis=1)\n",
    "    # plot the matrix\n",
    "    fig, ax = plt.subplots()\n",
    "    plt.imshow(norm_conf_mat)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.xlabel('Predictions')\n",
    "    plt.ylabel('Labels')\n",
    "    plt.xticks(range(dim), labels, rotation=45)\n",
    "    plt.yticks(range(dim), labels)\n",
    "    plt.colorbar()\n",
    "    # Put number of each cell in plot\n",
    "    for i in range(dim):\n",
    "        for j in range(dim):\n",
    "            c = conf_mat[j, i]\n",
    "            color = 'black' if c > 500 else 'white'\n",
    "            ax.text(i, j, str(int(c)), va='center', ha='center', color=color)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def get_data(filter_classes):\n",
    "    fashion_mnist = fetch_openml(\"Fashion-MNIST\", parser='auto')\n",
    "    x, y = fashion_mnist['data'], fashion_mnist['target'].astype(int)\n",
    "    # Remove classes\n",
    "    filtered_indices = np.isin(y, filter_classes)\n",
    "    x, y = x[filtered_indices].to_numpy(), y[filtered_indices]\n",
    "    # Normalize the pixels to be in [-1, +1] range\n",
    "    x = ((x / 255.) - .5) * 2\n",
    "    removed_class_count = 0\n",
    "    for i in range(10):  # Fix the labels\n",
    "        if i in filter_classes and removed_class_count != 0:\n",
    "            y[y == i] = i - removed_class_count\n",
    "        elif i not in filter_classes:\n",
    "            removed_class_count += 1\n",
    "    # Do the train-test split\n",
    "    return train_test_split(x, y, test_size=10_000)\n",
    "\n",
    "\n",
    "def onehot_encoder(y, num_labels):\n",
    "    one_hot = np.zeros(shape=(y.size, num_labels), dtype=int)\n",
    "    one_hot[np.arange(y.size), y] = 1\n",
    "    return one_hot\n",
    "\n",
    "\n",
    "def plot_batch_size(vanila, stochastic, mini_batch):\n",
    "    fig, axes = plt.subplots(2, 2)\n",
    "    # Plot the loss\n",
    "    axes[0, 0].plot(vanila[0], label='Gradient Descent')\n",
    "    axes[0, 0].plot(stochastic[0], label='Stochastic Gradient Descent')\n",
    "    axes[0, 0].plot(mini_batch[0], label='Mini-Batch Gradient Descent')\n",
    "    axes[0, 0].set_xlabel('Epoch'), axes[0, 0].set_ylabel('Loss')\n",
    "    axes[0, 0].set_title('Training Loss'), axes[0, 0].legend()\n",
    "    # Plot the accuracy\n",
    "    axes[0, 1].plot(vanila[2], label='Gradient Descent')\n",
    "    axes[0, 1].plot(stochastic[2], label='Stochastic Gradient Descent')\n",
    "    axes[0, 1].plot(mini_batch[2], label='Mini-Batch Gradient Descent')\n",
    "    axes[0, 1].set_xlabel('Epoch'), axes[0, 1].set_ylabel('Accuracy')\n",
    "    axes[0, 1].set_title('Test Accuracy'), axes[0, 1].legend()\n",
    "    # Plot SGD batch loss\n",
    "    axes[1, 0].plot(stochastic[1], label='Stochastic Gradient Descent')\n",
    "    axes[1, 0].set_xlabel('Batch'), axes[1, 0].set_ylabel('Loss')\n",
    "    axes[1, 0].set_title('Stochastic Gradient Descent')\n",
    "    # Plot MBGD batch loss\n",
    "    axes[1, 1].plot(mini_batch[1], label='Mini-Batch Gradient Descent')\n",
    "    axes[1, 1].set_xlabel('Batch'), axes[1, 1].set_ylabel('Loss')\n",
    "    axes[1, 1].set_title('Mini-Batch Gradient Descent')\n",
    "\n",
    "    fig.set_size_inches(16, 12)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GvFUeXMNadnZ"
   },
   "source": [
    "# Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KThKuK5DJHAH"
   },
   "source": [
    "## Abstract Layer Class\n",
    "\n",
    "The `Layer` class serves as an abstract base class for all layers in the network. It includes placeholder methods:\n",
    "- `forward`: The forward pass computes the output of the layer given an input.\n",
    "- `backward`: The backward pass computes the gradients with respect to the input and parameters.\n",
    "- `step`: Updates the layer parameters (weights and biases)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5HmznEXldEaI"
   },
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self):\n",
    "        self.inp = None\n",
    "        self.out = None\n",
    "\n",
    "    def __call__(self, inp: np.ndarray) -> np.ndarray:\n",
    "        return self.forward(inp)\n",
    "\n",
    "    def forward(self, inp: np.ndarray) -> np.ndarray:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def backward(self, up_grad: np.ndarray) -> np.ndarray:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def step(self, lr: float) -> None:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eB1SjTeaJztY"
   },
   "source": [
    "## Linear Layers\n",
    "\n",
    "The `Linear` class implements the fully connected (or dense) layer of a neural network, which performs a linear transformation on the input:\n",
    "\n",
    "$$\\mathbf{y} = \\mathbf{x} \\cdot \\mathbf{W} + \\mathbf{b}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "phTfqRsVIZKA"
   },
   "source": [
    "**Initialization**\n",
    "- `self.w`: Represents the weight matrix of shape `(in_dim, out_dim)`, initialized using small random values.\n",
    "- `self.b`: Bias vector of shape `(1, out_dim)`, initialized to zeros.\n",
    "- `self.dw` and `self.db`: These store the computed gradients of weights and biases during backpropagation.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Forward Pass**\n",
    "- The forward pass computes:\n",
    "$$\\mathbf{out} = \\mathbf{inp} \\cdot \\mathbf{W} + \\mathbf{b}$$\n",
    "where:\n",
    "  - `inp`: Input matrix of shape `(batch_size, in_dim)`\n",
    "  - `self.w`: Weight matrix of shape `(in_dim, out_dim)`\n",
    "  -\t`self.b`: Bias matrix of shape `(1, out_dim)`\n",
    "-\tThe result is a matrix out of shape `(batch_size, out_dim)`.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Backward Pass**\n",
    "- The backward pass computes gradients needed for updating the weights and biases. Given the upstream gradient `up_grad` (from the loss with respect to the output of this layer), we calculate:\n",
    "  - Gradient w.r.t. weights (`self.dw`):\n",
    "    $$ \\frac{\\partial L}{\\partial W} = \\mathbf{inp}^T \\cdot \\text{up_grad} $$\n",
    "  - Gradient w.r.t. biases (`self.db`):\n",
    "    $$\\frac{\\partial L}{\\partial b} = \\sum \\text{up_grad} \\text{ (summed across batch)}$$\n",
    "  - Gradient to propagate to the previous layer (`down_grad`):\n",
    "    $$\\text{down_grad} = \\text{up_grad} \\cdot W^T$$\n",
    "- This allows the gradient to flow backward to earlier layers.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Step Method**\n",
    "- Updates the weights and biases using the computed gradients and learning rate (`lr`):\n",
    "    $$W = W - lr \\cdot \\frac{\\partial L}{\\partial W}$$\n",
    "    $$b = b - lr \\cdot \\frac{\\partial L}{\\partial b}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pF0DKJV_JRnh"
   },
   "outputs": [],
   "source": [
    "class Linear(Layer):\n",
    "    def __init__(self, in_dim: int, out_dim: int):\n",
    "        super().__init__()\n",
    "        # He initialization: better scaling for deep networks\n",
    "        self.w = 0.1 * np.random.randn(in_dim, out_dim)\n",
    "        self.b = np.zeros((1, out_dim))\n",
    "        self.dw = np.zeros_like(self.w)\n",
    "        self.db = np.zeros_like(self.b)\n",
    "\n",
    "    def forward(self, inp: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Perform the linear transformation: output = inp * W + b\"\"\"\n",
    "        self.inp = inp\n",
    "        self.out = np.dot(inp, self.w) + self.b\n",
    "        return self.out\n",
    "\n",
    "    def backward(self, up_grad: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Backpropagate the gradients through this layer.\"\"\"\n",
    "        # Compute gradients for weights and biases\n",
    "        self.dw = np.dot(self.inp.T, up_grad)  # Gradient wrt weights\n",
    "        self.db = np.sum(up_grad, axis=0, keepdims=True)  # Gradient wrt biases\n",
    "        # Compute gradient to propagate back (downstream)\n",
    "        down_grad = np.dot(up_grad, self.w.T)\n",
    "        return down_grad\n",
    "\n",
    "    def step(self, lr: float) -> None:\n",
    "        \"\"\"Update the weights and biases using the gradients.\"\"\"\n",
    "        self.w -= lr * self.dw\n",
    "        self.b -= lr * self.db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yr6hBcY4N00B"
   },
   "source": [
    "## Activation Functions\n",
    "\n",
    "We can implement activation functions as layers. This will simplify the training process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zsuMui_fLCWf"
   },
   "source": [
    "### Sigmoid\n",
    "\n",
    "- The Sigmoid function is defined as follows:\n",
    "\n",
    "$$f(x) = \\frac{1}{1 + e^{-x}}$$\n",
    "\n",
    "- Sigmoid squashes the input into the range [0, 1], making it useful for binary classification tasks.\n",
    "- It converts any real-valued number into a probability-like output.\n",
    "- However, in deeper networks, it may cause vanishing gradients due to its flat slope for extreme values.\n",
    "- The derivative of Sigmoid is convenient to compute using its output  $f(x)$:\n",
    "$$f'(x) = \\frac{-e^{-x}}{(1 + e^{-x})^2} = \\frac{1}{1 + e^{-x}} \\cdot \\frac{e^{-x}}{1 + e^{-x}} = f(x) \\cdot (1-f(x))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cxGzqMybEGt-"
   },
   "outputs": [],
   "source": [
    "class Sigmoid(Layer):\n",
    "    def forward(self, inp: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Sigmoid Activation: f(x) = 1 / (1 + exp(-x))\"\"\"\n",
    "        self.out = 1 / (1 + np.exp(-inp))\n",
    "        return self.out\n",
    "\n",
    "    def backward(self, up_grad: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Backward pass for Sigmoid: f'(x) = f(x) * (1 - f(x))\"\"\"\n",
    "        down_grad = self.out * (1 - self.out) * up_grad\n",
    "        return down_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "08aibrACJ_kr"
   },
   "source": [
    "### ReLU (Rectified Linear Unit)\n",
    "\n",
    "- The ReLU function outputs 0 if the input is less than zero, otherwise it will return the input itself:\n",
    "$$f(x) = \\max(0,x) $$\n",
    "\n",
    "- ReLU helps introduce non-linearity into the model, which is essential for learning complex patterns.\n",
    "- It also helps avoid the vanishing gradient problem common in deep networks with the Sigmoid activation.\n",
    "- During backpropagation, only the gradients for inputs greater than 0 pass through:\n",
    "\n",
    "$$ f'(x) = \\begin{cases} 1 & \\text{if } x > 0 \\\\ 0 & \\text{otherwise} \\end{cases}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dHaTiE10HART"
   },
   "outputs": [],
   "source": [
    "class ReLU(Layer):\n",
    "    def forward(self, inp: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"ReLU Activation: f(x) = max(0, x)\"\"\"\n",
    "        self.inp = inp\n",
    "        self.out = np.maximum(0, inp)\n",
    "        return self.out\n",
    "\n",
    "    def backward(self, up_grad: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Backward pass for ReLU: derivative is 1 where input > 0, else 0.\"\"\"\n",
    "        down_grad = up_grad * (self.inp > 0)  # Efficient boolean indexing\n",
    "        return down_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CaQZUpp1coQ1"
   },
   "source": [
    "### Softmax\n",
    "\n",
    "- The Softmax function is defined as follows:\n",
    "$$f(x_i) = \\frac{e^{x_i}}{\\sum_j e^{x_j}}$$\n",
    "\n",
    "- Softmax normalizes the input values into probabilities that sum to 1.\n",
    "- It's typically used in the final layer of a neural network for multi-class classification.\n",
    "- It converts raw scores into probabilities, where each class has a non-negative probability between 0 and 1.\n",
    "- Subtracting the maximum input value (`np.max(inp)`) from all inputs before applying `np.exp` helps prevent overflow errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wrq1L5ZCO0vM"
   },
   "outputs": [],
   "source": [
    "class Softmax(Layer):\n",
    "    def forward(self, inp: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Softmax Activation: f(x) = exp(x) / sum(exp(x))\"\"\"\n",
    "        # Subtract max for numerical stability\n",
    "        exp_values = np.exp(inp - np.max(inp, axis=1, keepdims=True))\n",
    "        self.out = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "        return self.out\n",
    "\n",
    "    def backward(self, up_grad: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Backward pass for Softmax using the Jacobian matrix.\"\"\"\n",
    "        down_grad = np.empty_like(up_grad)\n",
    "        for i in range(up_grad.shape[0]):\n",
    "            single_output = self.out[i].reshape(-1, 1)\n",
    "            jacobian = np.diagflat(single_output) - np.dot(single_output, single_output.T)\n",
    "            down_grad[i] = np.dot(jacobian, up_grad[i])\n",
    "        return down_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sl2nW4KnSgYs"
   },
   "source": [
    "# Loss Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oyEmTPR-TLDk"
   },
   "source": [
    "## Abstract Loss Class\n",
    "\n",
    "The `Loss` class serves as an abstract base class for all layers in the network. It includes placeholder methods:\n",
    "- `forward`: To compute the loss given predictions and targets.\n",
    "- `backward`: To compute the loss given predictions and targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qMb2DsJNSp3f"
   },
   "outputs": [],
   "source": [
    "class Loss:\n",
    "    def __init__(self):\n",
    "        self.prediction = None\n",
    "        self.target = None\n",
    "        self.loss = None\n",
    "\n",
    "    def __call__(self, prediction: np.ndarray, target: np.ndarray) -> float:\n",
    "        return self.forward(prediction, target)\n",
    "\n",
    "    def forward(self, prediction: np.ndarray, target: np.ndarray) -> float:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def backward(self) -> np.ndarray:\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "__nX-zlJTo3B"
   },
   "source": [
    "## Cross-Entropy Loss\n",
    "\n",
    "Cross-entropy loss is typically used in classification tasks since it measures the dissimilarity between the true distribution (target) and the predicted probability distribution (prediction):\n",
    "\n",
    "$$L = - \\frac{1}{N} \\sum_{i} \\sum_{c} y_{ic} \\log(p_{ic})$$\n",
    "\n",
    "where $y_{ic}$ is the one-hot encoded true label (target), $p_{ic}$ is the predicted probability (output from Softmax) and $N$ is the batch size.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lyF-rhVvSy1q"
   },
   "outputs": [],
   "source": [
    "class CrossEntropy(Loss):\n",
    "    def forward(self, prediction: np.ndarray, target: np.ndarray) -> float:\n",
    "        \"\"\"Cross-Entropy Loss for classification.\"\"\"\n",
    "        self.prediction = prediction\n",
    "        self.target = target\n",
    "        # Clip predictions to avoid log(0)\n",
    "        clipped_pred = np.clip(prediction, 1e-12, 1.0)\n",
    "        # Compute and return the loss\n",
    "        self.loss = -np.mean(np.sum(target * np.log(clipped_pred), axis=1))\n",
    "        return self.loss\n",
    "\n",
    "    def backward(self) -> np.ndarray:\n",
    "        \"\"\"Gradient of Cross-Entropy Loss.\"\"\"\n",
    "        # Gradient wrt prediction (assuming softmax and one-hot targets)\n",
    "        grad = -self.target / self.prediction / self.target.shape[0]\n",
    "        return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-i8pHfR9TrYa"
   },
   "source": [
    "## Mean Squared Error (MSE) Loss\n",
    "\n",
    "MSE is used primarily for regression tasks, where you need to measure the distance between the predicted continuous values and the true values:\n",
    "\n",
    "$$L = \\frac{1}{N} \\sum_{i} (p_i - y_i)^2$$\n",
    "\n",
    "where $p_i$ is the predicted value, $y_i$ is the true value (target) and $N$ is the batch size.\n",
    "\n",
    "The gradient measures the difference between the prediction and the target, scaled by the batch size:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial p_i} = \\frac{2}{N} (p_i - y_i)$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0ZmkwcRCSzRd"
   },
   "outputs": [],
   "source": [
    "class MSE(Loss):\n",
    "    def forward(self, prediction: np.ndarray, target: np.ndarray) -> float:\n",
    "        \"\"\"Mean Squared Error Loss for regression.\"\"\"\n",
    "        self.prediction = prediction\n",
    "        self.target = target\n",
    "        # Compute and return the loss\n",
    "        self.loss = np.mean((prediction - target) ** 2)\n",
    "        return self.loss\n",
    "\n",
    "    def backward(self) -> np.ndarray:\n",
    "        \"\"\"Gradient of MSE Loss.\"\"\"\n",
    "        grad = 2 * (self.prediction - self.target) / self.target.size\n",
    "        return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p1KKCeDWUNJc"
   },
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MJno_CD7hK1u"
   },
   "source": [
    "Now we can combine everything we've done earlier to build a neural network class called `MLP` with the following methods:\n",
    "\n",
    "- `forward`: Sequentially passes input through each layer in the network to compute the output.\n",
    "- `loss`: Computes the loss between the predicted output and the true target using the specified loss function.\n",
    "- `backward`: Propagates the gradient from the loss function through each layer, updating the gradients of the parameters in each layer.\n",
    "- `update`: Updates each layer's parameters (e.g., weights and biases) using the gradients computed during backpropagation.\n",
    "- `train`: Executes the training loop for a specified number of epochs, iterating over the dataset in mini-batches, performing the forward pass, computing the loss, backpropagating the gradients, and updating the parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nd1HjMLMUOAs"
   },
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    def __init__(self, layers: list[Layer], loss_fn: Loss, lr: float) -> None:\n",
    "        \"\"\"\n",
    "        Multi-Layer Perceptron (MLP) class.\n",
    "        Arguments:\n",
    "        - layers: List of layers (e.g., Linear, ReLU, etc.).\n",
    "        - loss_fn: Loss function object (e.g., CrossEntropy, MSE).\n",
    "        - lr: Learning rate.\n",
    "        \"\"\"\n",
    "        self.layers = layers\n",
    "        self.loss_fn = loss_fn\n",
    "        self.lr = lr\n",
    "\n",
    "    def __call__(self, inp: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Makes the model callable, equivalent to forward pass.\"\"\"\n",
    "        return self.forward(inp)\n",
    "\n",
    "    def forward(self, inp: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Pass input through each layer sequentially.\"\"\"\n",
    "        for layer in self.layers:\n",
    "            inp = layer.forward(inp)\n",
    "        return inp\n",
    "\n",
    "    def loss(self, prediction: np.ndarray, target: np.ndarray) -> float:\n",
    "        \"\"\"Calculate the loss.\"\"\"\n",
    "        return self.loss_fn(prediction, target)\n",
    "\n",
    "    def backward(self) -> None:\n",
    "        \"\"\"Perform backpropagation by propagating the gradient backwards through the layers.\"\"\"\n",
    "        up_grad = self.loss_fn.backward()\n",
    "        for layer in reversed(self.layers):\n",
    "            up_grad = layer.backward(up_grad)\n",
    "\n",
    "    def update(self) -> None:\n",
    "        \"\"\"Update the parameters of each layer using the gradients and the learning rate.\"\"\"\n",
    "        for layer in self.layers:\n",
    "            layer.step(self.lr)\n",
    "\n",
    "    def train(self, x_train: np.ndarray, y_train: np.ndarray, epochs: int, batch_size: int) -> np.ndarray:\n",
    "        \"\"\"Train the MLP over the given dataset for a number of epochs.\"\"\"\n",
    "        losses = np.empty(epochs)\n",
    "        for epoch in (pbar := trange(epochs)):\n",
    "            running_loss = 0.0\n",
    "            for i in range(0, len(x_train), batch_size):\n",
    "                x_batch = x_train[i:i + batch_size]\n",
    "                y_batch = y_train[i:i + batch_size]\n",
    "\n",
    "                # Forward pass\n",
    "                prediction = self.forward(x_batch)\n",
    "\n",
    "                # Compute loss\n",
    "                running_loss += self.loss(prediction, y_batch) * batch_size\n",
    "\n",
    "                # Backward pass\n",
    "                self.backward()\n",
    "\n",
    "                # Update parameters\n",
    "                self.update()\n",
    "\n",
    "            # Normalize running loss by total number of samples\n",
    "            running_loss /= len(x_train)\n",
    "            pbar.set_description(f\"Loss: {running_loss:.3f}\")\n",
    "            losses[epoch] = running_loss\n",
    "\n",
    "        return losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AzONLtl5kc8W"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r93pdZdrZ77B"
   },
   "source": [
    "## Loading the Fashion-MNIST Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bOCW3Kh9azY6"
   },
   "source": [
    "For simplicity you can use `get_data` to load the Fashion-MNIST dataset. Since we aren't using GPUs, in order to save time and get better results, we are only going to include 3 classes in our training. However you can easily modify this cell to include different classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xmRyKM-oZ-hh"
   },
   "outputs": [],
   "source": [
    "class_names = {0: 'T-shirt/top', 1: 'Trouser', 2: 'Pullover',\n",
    "               3: 'Dress', 4: 'Coat', 5:  'Sandal', 6: 'Shirt',\n",
    "               7: 'Sneaker', 8: 'Bag', 9: 'Ankle boot'}\n",
    "\n",
    "# Include all the classes you want to see in training\n",
    "kept_classes = [0, 1, 7]  # T-shirt/top, Trouser, Sneaker\n",
    "\n",
    "# Download the dataset and split it into training and testing sets\n",
    "x_train, x_test, y_train, y_test = get_data(kept_classes)\n",
    "\n",
    "# One-hot encode the target labels of the training set\n",
    "y_train = onehot_encoder(y_train, num_labels=len(kept_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sZeNgQXybi4P"
   },
   "source": [
    "## Training the Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2qfmP-hYkpfz"
   },
   "source": [
    "Now we can define the network and train it on the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "id": "eaIADoc8W7CS",
    "outputId": "7d762b34-7b46-461d-d183-7ee9ec1862e9"
   },
   "outputs": [],
   "source": [
    "# Define the layers of the neural network\n",
    "layers = [Linear(784, 50),\n",
    "          ReLU(),\n",
    "          Linear(50, 50),\n",
    "          ReLU(),\n",
    "          Linear(50, len(kept_classes)),\n",
    "          Softmax()]\n",
    "\n",
    "# Create the model\n",
    "model = MLP(layers, CrossEntropy(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "losses = model.train(x_train, y_train, epochs=30, batch_size=64)\n",
    "\n",
    "# Plot the training loss curve\n",
    "plot_training(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H4Uh-ddyk2mR"
   },
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sgO7eZ9Uk5X0"
   },
   "source": [
    "We can measure the models accuracy on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I3M4fWNaZoHW",
    "outputId": "d8289ad2-2775-4293-9d8c-36f32ab0764a"
   },
   "outputs": [],
   "source": [
    "# Test the model\n",
    "y_prediction = np.argmax(model(x_test), axis=1)\n",
    "acc = 100 * np.mean(y_prediction == y_test)\n",
    "print(f'Test accuracy with {len(y_train)} training examples on {len(y_test)} test samples is {acc:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I2DrYQPilBlF"
   },
   "source": [
    "The confusion matrix can also be observed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "id": "49LwgNLMj72R",
    "outputId": "a1ae708d-967a-4526-ccb1-83041fb594bb"
   },
   "outputs": [],
   "source": [
    "# Plot the confusion matrix\n",
    "plot_confusion_matrix(y_test, y_prediction, class_names, kept_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1KIRyFvYDkvd"
   },
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X-jahzvWEVV_"
   },
   "source": [
    "In this section we are going to run some experiments to better understand the different hyperparameters of our neural network.\n",
    "We will slightly modify the `MLP` class we wrote before to access different metrics during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "I9pWMq0yHnpV"
   },
   "outputs": [],
   "source": [
    "# @title Modified MLP\n",
    "\n",
    "class NN(MLP):\n",
    "    def test(self, x_test: np.ndarray, y_test: np.ndarray) -> float:\n",
    "        \"\"\"Calculate the test accuracy and return it.\"\"\"\n",
    "        y_pred = np.argmax(self.forward(x_test), axis=1)\n",
    "        return 100 * np.mean(y_pred == y_test)\n",
    "\n",
    "    def train(self, x_train: np.ndarray, y_train: np.ndarray, epochs: int,\n",
    "              batch_size: int, x_test: np.ndarray, y_test: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Train the MLP over the given dataset for a number of epochs.\"\"\"\n",
    "        losses = np.zeros(epochs)\n",
    "        batch_count = len(x_train) // batch_size\n",
    "        batch_losses = np.empty(epochs * batch_count + 1)\n",
    "        accuracies = np.empty(epochs)\n",
    "\n",
    "        for epoch in (pbar := trange(epochs)):\n",
    "            correct = 0\n",
    "            for i in range(0, len(x_train), batch_size):\n",
    "                x_batch = x_train[i:i + batch_size]\n",
    "                y_batch = y_train[i:i + batch_size]\n",
    "                # Forward pass\n",
    "                prediction = self.forward(x_batch)\n",
    "                # Compute loss\n",
    "                batch_losses[i // batch_size + epoch * batch_count] = self.loss(prediction, y_batch)\n",
    "                losses[epoch] += batch_losses[i // batch_size + epoch * batch_count]\n",
    "                # Backward pass\n",
    "                self.backward()\n",
    "                # Update parameters\n",
    "                self.update()\n",
    "\n",
    "            # Display and update the metrics\n",
    "            losses[epoch] /= batch_count\n",
    "            accuracies[epoch] = self.test(x_test, y_test)\n",
    "            pbar.set_description(f\"Train Loss = {losses[epoch]:.3f} | Test Accuracy = {accuracies[epoch]:.2f}% \")\n",
    "\n",
    "        return losses, batch_losses, accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1KcAcyeuDuO7"
   },
   "source": [
    "## Batch Size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k2A_sjspETKn"
   },
   "source": [
    "Here we will take a look at different batch sizes and how they effect training and convergence. Run the widget bellow to train the model for different batch sizes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "1179f322ebf643fcbc389efa1e80f0d9",
      "b9ab8195f2d94405a822ea67ea1fd094",
      "48a192d8f8754cac918a4ea1fa60e23a"
     ]
    },
    "id": "i9JQZLyCFiG6",
    "outputId": "2c68cb95-cb03-46ce-b31a-643c9e922f02"
   },
   "outputs": [],
   "source": [
    "# @markdown Batch Size Experimentation Widget\n",
    "\n",
    "class_names = {0: 'T-shirt/top', 1: 'Trouser', 2: 'Pullover',\n",
    "               3: 'Dress', 4: 'Coat', 5:  'Sandal', 6: 'Shirt',\n",
    "               7: 'Sneaker', 8: 'Bag', 9: 'Ankle boot'}\n",
    "\n",
    "# Include all the classes you want to see in training\n",
    "kept_classes = [0, 1, 7]  # T-shirt/top, Trouser, Sneaker\n",
    "\n",
    "# Download the dataset and split it into training and testing sets\n",
    "x_train, x_test, y_train, y_test = get_data(kept_classes)\n",
    "\n",
    "# One-hot encode the target labels of the training set\n",
    "y_train = onehot_encoder(y_train, num_labels=len(kept_classes))\n",
    "\n",
    "# Create a list of values\n",
    "options = [16, 32, 64, 128, 256]\n",
    "\n",
    "# Create a dropdown widget with custom layout\n",
    "dropdown = widgets.Dropdown(\n",
    "    options=options,\n",
    "    description='Batch Size:',\n",
    "    layout={'width': '200px'},\n",
    ")\n",
    "\n",
    "# Define a function to run based on selected value\n",
    "def on_value_change(change):\n",
    "    mini_batch_size = change['new']\n",
    "    global first_run, gd, sgd, mbgd\n",
    "    if first_run:\n",
    "        # Gradient Descent\n",
    "        layers = [Linear(784, 50), ReLU(), Linear(50, 50), ReLU(), Linear(50, len(kept_classes)), Softmax()]\n",
    "        model = NN(layers, CrossEntropy(), lr=0.001)\n",
    "        print('Gradient Descent:', end='\\n\\t')\n",
    "        gd = model.train(x_train, y_train, epochs=30, batch_size=len(x_train), x_test=x_test, y_test=y_test)\n",
    "        # Mini-Batch Gradient Descent\n",
    "        layers = [Linear(784, 50), ReLU(), Linear(50, 50), ReLU(), Linear(50, len(kept_classes)), Softmax()]\n",
    "        model = NN(layers, CrossEntropy(), lr=0.001)\n",
    "        print('Stochastic Gradient Descent:', end='\\n\\t')\n",
    "        sgd = model.train(x_train, y_train, epochs=30, batch_size=1, x_test=x_test, y_test=y_test)\n",
    "        first_run = False\n",
    "    else:\n",
    "        print('\\n')\n",
    "    # Stochastic Gradient Descent\n",
    "    layers = [Linear(784, 50), ReLU(), Linear(50, 50), ReLU(), Linear(50, len(kept_classes)), Softmax()]\n",
    "    model = NN(layers, CrossEntropy(), lr=0.001)\n",
    "    print('Mini-Batch Gradient Descent:', end='\\n\\t')\n",
    "    mbgd = model.train(x_train, y_train, epochs=30, batch_size=mini_batch_size, x_test=x_test, y_test=y_test)\n",
    "    print()\n",
    "    plot_batch_size(gd, sgd, mbgd)\n",
    "\n",
    "\n",
    "\n",
    "# Observe changes in the dropdown value\n",
    "dropdown.observe(on_value_change, names='value')\n",
    "\n",
    "# Run Vanila Gradient Descent and Stochastic Gradient Descent once\n",
    "first_run = True\n",
    "\n",
    "gd, sgd, mbgd = None, None, None\n",
    "\n",
    "# Display the widget\n",
    "display(dropdown)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fpnbFt4zkgu9"
   },
   "source": [
    "As you can see, increasing the batch size improves the speed of our algorithm, while reducing the batch size allows us to achieve higher accuracies."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "Ecaqr9YkWpbL"
   ],
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "1179f322ebf643fcbc389efa1e80f0d9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DropdownModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DropdownModel",
      "_options_labels": [
       "16",
       "32",
       "64",
       "128",
       "256"
      ],
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "DropdownView",
      "description": "Batch Size:",
      "description_tooltip": null,
      "disabled": false,
      "index": 4,
      "layout": "IPY_MODEL_b9ab8195f2d94405a822ea67ea1fd094",
      "style": "IPY_MODEL_48a192d8f8754cac918a4ea1fa60e23a"
     }
    },
    "48a192d8f8754cac918a4ea1fa60e23a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b9ab8195f2d94405a822ea67ea1fd094": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "200px"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

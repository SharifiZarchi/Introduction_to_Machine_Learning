{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N3q3WK-FCcNJ"
   },
   "source": [
    "# Improving Neural Networks\n",
    "\n",
    "\n",
    "\n",
    "**CE477: Machine Learning**\n",
    "\n",
    "\n",
    "\n",
    "__Instructor:__ Dr. Ali Sharifi-Zarchi\n",
    "\n",
    "\n",
    "\n",
    "__Author:__ Ramtin Moslemi\n",
    "\n",
    "\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/SharifiZarchi/Introduction_to_Machine_Learning/blob/main/Jupyter_Notebooks/Chapter_03_Neural_Networks/Improving_NNs.ipynb)\n",
    "\n",
    "[![Open In kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://raw.githubusercontent.com/SharifiZarchi/Introduction_to_Machine_Learning/main/Jupyter_Notebooks/Chapter_03_Neural_Networks/Improving_NNs.ipynb)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "## Notebook Objectives\n",
    "\n",
    "\n",
    "\n",
    "In this notebook we are going to start with a dataset, and step by step, try and improve our models. We will build on what we have learned from the previous lectures and notebooks and use these lessons to obtain better results!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fp-rOMnHi505"
   },
   "source": [
    "## Imports and Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7zbVxFHgi7we"
   },
   "source": [
    "In this notebook we are going to work with the `EMNIST` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-11-05T00:47:36.189171Z",
     "iopub.status.busy": "2024-11-05T00:47:36.188858Z",
     "iopub.status.idle": "2024-11-05T00:47:40.793842Z",
     "shell.execute_reply": "2024-11-05T00:47:40.792873Z",
     "shell.execute_reply.started": "2024-11-05T00:47:36.189137Z"
    },
    "id": "-fRNfLoRQ2UW",
    "outputId": "c4927ffc-463c-4874-b355-87c6e8764b37"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import EMNIST\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import trange\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Get cpu, gpu or mps device for training.\n",
    "device = (\"cuda\" if torch.cuda.is_available()\n",
    "          else \"mps\" if torch.backends.mps.is_available()\n",
    "          else \"cpu\")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {
     "iopub.execute_input": "2024-11-05T00:47:40.795376Z",
     "iopub.status.busy": "2024-11-05T00:47:40.794991Z",
     "iopub.status.idle": "2024-11-05T00:47:40.806320Z",
     "shell.execute_reply": "2024-11-05T00:47:40.805472Z",
     "shell.execute_reply.started": "2024-11-05T00:47:40.795343Z"
    },
    "id": "2-SxYbOjsfwg"
   },
   "outputs": [],
   "source": [
    "# @title plotting functions\n",
    "\n",
    "def plot_results(train_losses, train_accs, test_losses, test_accs):\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    xs = np.arange(1, len(train_losses) + 1, 1)\n",
    "    axes[0].plot(xs, train_losses, label='Train')\n",
    "    axes[0].plot(xs, test_losses, label='Test')\n",
    "    axes[0].set_title('Loss')\n",
    "    axes[0].set_xlabel('Epoch')\n",
    "    axes[0].set_ylabel('Loss')\n",
    "    axes[0].set_xticks(xs)\n",
    "    axes[1].plot(xs, train_accs, label='Train')\n",
    "    axes[1].plot(xs, test_accs, label='Test')\n",
    "    axes[1].set_title('Accuracy')\n",
    "    axes[1].set_xlabel('Epoch')\n",
    "    axes[1].set_ylabel('Accuracy')\n",
    "    axes[1].set_xticks(xs)\n",
    "    plt.tight_layout()\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {
     "iopub.execute_input": "2024-11-05T00:47:40.808424Z",
     "iopub.status.busy": "2024-11-05T00:47:40.807455Z",
     "iopub.status.idle": "2024-11-05T00:47:40.820261Z",
     "shell.execute_reply": "2024-11-05T00:47:40.819444Z",
     "shell.execute_reply.started": "2024-11-05T00:47:40.808377Z"
    },
    "id": "gApTH8VyeDgg"
   },
   "outputs": [],
   "source": [
    "# @title helper functions\n",
    "\n",
    "\n",
    "def train_step(model, criterion, optimizer, dataloader, device=device):\n",
    "    running_loss, correct = 0, 0\n",
    "    # Move the model to training mode\n",
    "    model.train()\n",
    "    # Iterate over the dataloader\n",
    "    for x, y in dataloader:\n",
    "        # Move the datapoints to same device as the model\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        # Clear the gradients\n",
    "        optimizer.zero_grad()\n",
    "        # Predict the output and perform the forward pass\n",
    "        pred = model(x)\n",
    "        # Compute prediction error\n",
    "        loss = criterion(pred, y)\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        # Update the model weights\n",
    "        optimizer.step()\n",
    "        # Update the running loss\n",
    "        running_loss += loss.item()\n",
    "        # Update the correctly predicted counter\n",
    "        correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    # Calculate and return the average loss and accuracy\n",
    "    return running_loss / len(dataloader), 100 * correct / len(dataloader.dataset)\n",
    "\n",
    "\n",
    "def test_step(model, criterion, dataloader, device=device):\n",
    "    running_loss, correct = 0, 0\n",
    "    # Move the model to training mode\n",
    "    model.eval()\n",
    "    # Disable gradient calculation\n",
    "    with torch.no_grad():\n",
    "      # Iterate over the dataloader\n",
    "      for x, y in dataloader:\n",
    "          # Move the datapoints to same device as the model\n",
    "          x, y = x.to(device), y.to(device)\n",
    "          # Predict the output and perform the forward pass\n",
    "          pred = model(x)\n",
    "          # Update the running loss\n",
    "          running_loss += criterion(pred, y).item()\n",
    "          # Update the correctly predicted counter\n",
    "          correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    # Calculate and return the average accuracy\n",
    "    return running_loss / len(dataloader), 100 * correct / len(dataloader.dataset)\n",
    "\n",
    "\n",
    "def train_model(model, n_epochs, optimizer, train_loader, test_loader, device=device):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    train_losses, train_accs = [], []\n",
    "    test_losses, test_accs = [], []\n",
    "    for epoch in (pbar := trange(n_epochs)):\n",
    "        # Train the model\n",
    "        train_loss, train_acc = train_step(model, criterion, optimizer, train_loader, device)\n",
    "        # Evaluate the model\n",
    "        test_loss, test_acc = test_step(model, criterion, test_loader, device)\n",
    "        # Display the results\n",
    "        pbar.set_description(f'Train Loss = {train_loss:.3f} | Train Acc = {train_acc:.2f}% | Test Loss = {test_loss:.3f} | Test Acc = {test_acc:.2f}% ')\n",
    "        # Store the results\n",
    "        train_losses.append(train_loss), train_accs.append(train_acc), test_losses.append(test_loss), test_accs.append(test_acc)\n",
    "    # Plot the results\n",
    "    plot_results(train_losses, train_accs, test_losses, test_accs)\n",
    "    return {'train_loss': train_losses, 'train_acc': train_accs, 'test_loss': test_losses, 'test_acc': test_accs}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yHoUcJA5jxaH"
   },
   "source": [
    "# Loading the `EMNIST` Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tu5TKCGEjwVp"
   },
   "source": [
    "To work with the data we must first download the dataset using `EMNIST`. We will normalize our data as well to get better results during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-11-05T00:47:40.822877Z",
     "iopub.status.busy": "2024-11-05T00:47:40.822542Z",
     "iopub.status.idle": "2024-11-05T00:47:59.920289Z",
     "shell.execute_reply": "2024-11-05T00:47:59.919277Z",
     "shell.execute_reply.started": "2024-11-05T00:47:40.822835Z"
    },
    "id": "BBPItL0-O08w",
    "outputId": "29e35174-7412-4c45-ee6b-db1dd62457c8"
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "train_dataset = EMNIST(root='./data', split='balanced', train=True, download=True, transform=transform)\n",
    "test_dataset = EMNIST(root='./data', split='balanced', train=False, download=True, transform=transform)\n",
    "\n",
    "print(f'Train dataset size: {len(train_dataset)}')\n",
    "print(f'Test dataset size: {len(test_dataset)}')\n",
    "\n",
    "classes = train_dataset.classes\n",
    "print(f'Classes:\\n{classes}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ag9520Ijg8Dc"
   },
   "source": [
    "As always, we must next define our dataloaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-11-05T00:47:59.921992Z",
     "iopub.status.busy": "2024-11-05T00:47:59.921593Z",
     "iopub.status.idle": "2024-11-05T00:47:59.988329Z",
     "shell.execute_reply": "2024-11-05T00:47:59.987380Z",
     "shell.execute_reply.started": "2024-11-05T00:47:59.921947Z"
    },
    "id": "WY3hFUULhASZ",
    "outputId": "d85a11ce-2a9b-4702-ceb2-9b984ccedceb"
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Iterate over the data\n",
    "for x, y in test_loader:\n",
    "    print(f\"Shape of X [N, C, H, W]: {x.shape}\")\n",
    "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R606nUhtkOZB"
   },
   "source": [
    "# First Try: Simple Net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5GgfWlhskQjq"
   },
   "source": [
    "First we start working by a simple neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-05T00:47:59.989853Z",
     "iopub.status.busy": "2024-11-05T00:47:59.989523Z",
     "iopub.status.idle": "2024-11-05T00:47:59.996615Z",
     "shell.execute_reply": "2024-11-05T00:47:59.995643Z",
     "shell.execute_reply.started": "2024-11-05T00:47:59.989817Z"
    },
    "id": "ayJR3yClFdto"
   },
   "outputs": [],
   "source": [
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.network = nn.Sequential(nn.Flatten(), nn.Linear(input_size, 128),\n",
    "                                     nn.ReLU(), nn.Linear(128, 128),\n",
    "                                     nn.ReLU(), nn.Linear(128, 64),\n",
    "                                     nn.ReLU(), nn.Linear(64, num_classes))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uVjWx6mHooTA"
   },
   "source": [
    "We will use a simple stochastic gradient descent optimizer and train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 544
    },
    "execution": {
     "iopub.execute_input": "2024-11-05T00:47:59.998190Z",
     "iopub.status.busy": "2024-11-05T00:47:59.997895Z",
     "iopub.status.idle": "2024-11-05T01:02:10.875982Z",
     "shell.execute_reply": "2024-11-05T01:02:10.875062Z",
     "shell.execute_reply.started": "2024-11-05T00:47:59.998157Z"
    },
    "id": "ZGfzpDnXtKZ9",
    "outputId": "092329e5-5004-42fe-da52-51e08ea1eaa9"
   },
   "outputs": [],
   "source": [
    "model = SimpleNN(input_size=28*28, num_classes=len(classes)).to(device)\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "results = train_model(model, 30, optimizer, train_loader, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0r4dUDzsv9EG"
   },
   "source": [
    "It appears this dataset is far more challenging than the original `MNIST` digits dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EG4uumBCxy-d"
   },
   "source": [
    "# Second Try: Adam Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QsMfknK1l5kY"
   },
   "source": [
    "As you recall, we can use better optimizers to speed up convergence.\n",
    "Usually, the `Adam` optimizer is a good choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 463
    },
    "execution": {
     "iopub.execute_input": "2024-11-05T01:02:10.877654Z",
     "iopub.status.busy": "2024-11-05T01:02:10.877267Z",
     "iopub.status.idle": "2024-11-05T01:11:56.072227Z",
     "shell.execute_reply": "2024-11-05T01:11:56.071320Z",
     "shell.execute_reply.started": "2024-11-05T01:02:10.877609Z"
    },
    "id": "XIGg0VB6x0Qm",
    "outputId": "2f95eab9-f7ea-4752-a589-1165076087fa"
   },
   "outputs": [],
   "source": [
    "model = SimpleNN(input_size=28*28, num_classes=len(classes)).to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "results = train_model(model, 20, optimizer, train_loader, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ryc5azrxmZuF"
   },
   "source": [
    "Wow! Evidently replacing `SGD` with `Adam` improved our resutls drastically! ðŸ”¥"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g0SwkPeemgod"
   },
   "source": [
    "# Third Try: Deeper Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LC9j2hXVmwMY"
   },
   "source": [
    "Perhaps a bigger network could yield better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-05T01:11:56.073773Z",
     "iopub.status.busy": "2024-11-05T01:11:56.073459Z",
     "iopub.status.idle": "2024-11-05T01:11:56.082050Z",
     "shell.execute_reply": "2024-11-05T01:11:56.081164Z",
     "shell.execute_reply.started": "2024-11-05T01:11:56.073739Z"
    },
    "id": "Mu0Zcwxumvh0"
   },
   "outputs": [],
   "source": [
    "class BiggerNN(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(BiggerNN, self).__init__()\n",
    "        self.network = nn.Sequential(nn.Flatten(), nn.Linear(input_size, 512),\n",
    "                                     nn.ReLU(), nn.Linear(512, 512),\n",
    "                                     nn.ReLU(), nn.Linear(512, 512),\n",
    "                                     nn.ReLU(), nn.Linear(512, 256),\n",
    "                                     nn.ReLU(), nn.Linear(256, 256),\n",
    "                                     nn.ReLU(), nn.Linear(256, 256),\n",
    "                                     nn.ReLU(), nn.Linear(256, 128),\n",
    "                                     nn.ReLU(), nn.Linear(128, 128),\n",
    "                                     nn.ReLU(), nn.Linear(128, 128),\n",
    "                                     nn.ReLU(), nn.Linear(128, 64),\n",
    "                                     nn.ReLU(), nn.Linear(64, 64),\n",
    "                                     nn.ReLU(), nn.Linear(64, 64),\n",
    "                                     nn.ReLU(), nn.Linear(64, num_classes))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 463
    },
    "execution": {
     "iopub.execute_input": "2024-11-05T01:11:56.084028Z",
     "iopub.status.busy": "2024-11-05T01:11:56.083336Z",
     "iopub.status.idle": "2024-11-05T01:22:23.379504Z",
     "shell.execute_reply": "2024-11-05T01:22:23.378492Z",
     "shell.execute_reply.started": "2024-11-05T01:11:56.083982Z"
    },
    "id": "idqtAoiQnXfI",
    "outputId": "ba00eadd-38dc-429f-a734-9f2a60bfe5d1"
   },
   "outputs": [],
   "source": [
    "model = BiggerNN(input_size=28*28, num_classes=len(classes)).to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "results = train_model(model, 20, optimizer, train_loader, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vKMv462AxOtO"
   },
   "source": [
    "Hmm ðŸ¤” it seems that making the network deeper wasn't that helpful. How can this be?\n",
    "\n",
    "After all this bigger network has more representative power.\n",
    "We will try to address this issue next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3wZL5ybznQic"
   },
   "source": [
    "# Fourth Try: Batch Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GiDliGwJ76P9"
   },
   "source": [
    "Batch normalization does wonders for better optimization, especially as the networks get deeper!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-05T01:22:23.381086Z",
     "iopub.status.busy": "2024-11-05T01:22:23.380739Z",
     "iopub.status.idle": "2024-11-05T01:22:23.390236Z",
     "shell.execute_reply": "2024-11-05T01:22:23.389224Z",
     "shell.execute_reply.started": "2024-11-05T01:22:23.381051Z"
    },
    "id": "YImDbx1UnmFf"
   },
   "outputs": [],
   "source": [
    "class BiggerBN(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(BiggerBN, self).__init__()\n",
    "        self.network = nn.Sequential(nn.Flatten(), nn.Linear(28*28, 512), nn.BatchNorm1d(512),\n",
    "                                     nn.ReLU(), nn.Linear(512, 512), nn.BatchNorm1d(512),\n",
    "                                     nn.ReLU(), nn.Linear(512, 512), nn.BatchNorm1d(512),\n",
    "                                     nn.ReLU(), nn.Linear(512, 256), nn.BatchNorm1d(256),\n",
    "                                     nn.ReLU(), nn.Linear(256, 256), nn.BatchNorm1d(256),\n",
    "                                     nn.ReLU(), nn.Linear(256, 256), nn.BatchNorm1d(256),\n",
    "                                     nn.ReLU(), nn.Linear(256, 128), nn.BatchNorm1d(128),\n",
    "                                     nn.ReLU(), nn.Linear(128, 128), nn.BatchNorm1d(128),\n",
    "                                     nn.ReLU(), nn.Linear(128, 128), nn.BatchNorm1d(128),\n",
    "                                     nn.ReLU(), nn.Linear(128, 128), nn.BatchNorm1d(128),\n",
    "                                     nn.ReLU(), nn.Linear(128, 64), nn.BatchNorm1d(64),\n",
    "                                     nn.ReLU(), nn.Linear(64, 64), nn.BatchNorm1d(64),\n",
    "                                     nn.ReLU(), nn.Linear(64, 64), nn.BatchNorm1d(64),\n",
    "                                     nn.ReLU(), nn.Linear(64, num_classes))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 463
    },
    "execution": {
     "iopub.execute_input": "2024-11-05T01:22:23.391703Z",
     "iopub.status.busy": "2024-11-05T01:22:23.391395Z",
     "iopub.status.idle": "2024-11-05T01:33:52.315986Z",
     "shell.execute_reply": "2024-11-05T01:33:52.314859Z",
     "shell.execute_reply.started": "2024-11-05T01:22:23.391669Z"
    },
    "id": "MjIpAWEUn9F6",
    "outputId": "4ebc3613-72f7-475e-d27a-24993b7913d7"
   },
   "outputs": [],
   "source": [
    "model = BiggerBN(input_size=28*28, num_classes=len(classes)).to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "results = train_model(model, 20, optimizer, train_loader, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jmgvV6Qo9IMW"
   },
   "source": [
    "As you can see this really helped increase the accuracy, however there is something wrong.\n",
    "Can you spot it?\n",
    "\n",
    "As you can see there is a gap between our train and test losses and accuracies.\n",
    "In other words we are observing **overfitting**!\n",
    "Our next approach will focus on this issue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4KfV1DGj8feB"
   },
   "source": [
    "# Fifth Try: Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RTaeIe_e8u38"
   },
   "outputs": [],
   "source": [
    "class BiggerDO(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(BiggerDO, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Flatten(), nn.Linear(28*28, 512), nn.BatchNorm1d(512),\n",
    "            nn.ReLU(), nn.Dropout(0.20), nn.Linear(512, 512), nn.BatchNorm1d(512),\n",
    "            nn.ReLU(), nn.Dropout(0.20), nn.Linear(512, 512), nn.BatchNorm1d(512),\n",
    "            nn.ReLU(), nn.Dropout(0.20), nn.Linear(512, 256), nn.BatchNorm1d(256),\n",
    "            nn.ReLU(), nn.Dropout(0.15), nn.Linear(256, 256), nn.BatchNorm1d(256),\n",
    "            nn.ReLU(), nn.Dropout(0.15), nn.Linear(256, 256), nn.BatchNorm1d(256),\n",
    "            nn.ReLU(), nn.Dropout(0.15), nn.Linear(256, 128), nn.BatchNorm1d(128),\n",
    "            nn.ReLU(), nn.Dropout(0.10), nn.Linear(128, 128), nn.BatchNorm1d(128),\n",
    "            nn.ReLU(), nn.Dropout(0.10), nn.Linear(128, 128), nn.BatchNorm1d(128),\n",
    "            nn.ReLU(), nn.Dropout(0.10), nn.Linear(128, 64), nn.BatchNorm1d(64),\n",
    "            nn.ReLU(), nn.Dropout(0.05), nn.Linear(64, 64), nn.BatchNorm1d(64),\n",
    "            nn.ReLU(), nn.Dropout(0.05), nn.Linear(64, 64), nn.BatchNorm1d(64),\n",
    "            nn.ReLU(), nn.Dropout(0.05), nn.Linear(64, num_classes))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 544
    },
    "id": "o6UEKrJI88mu",
    "outputId": "33302d3b-0427-4985-d668-79ce97a9c26e"
   },
   "outputs": [],
   "source": [
    "model = BiggerDO(input_size=28*28, num_classes=len(classes)).to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "results = train_model(model, 20, optimizer, train_loader, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qOgjskR_Aedg"
   },
   "source": [
    "This time the generalization gap is not as wide as before.\n",
    "In fact here we are getting better results on our test set than our training set.\n",
    "This means we are underfitting (keep in mind `model.eval()` disables the dropout layers and stops updating the batch normalization, however `model.train()` enables them again).\n",
    "\n",
    "Our next and final approach takes note that the training curve flattens near the end, therefore using smaller learning rates can help with the optimization process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GnoS_3-sAYaB"
   },
   "source": [
    "# Sixth Try: Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FbFwTqjOBlag"
   },
   "source": [
    "This time we won't initialize a new network, instead we will continue training with a smaller learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 463
    },
    "id": "Rcg_AjBy_Pjm",
    "outputId": "6f3a0db3-6dae-48d6-d6ce-d388785c7332"
   },
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "results = train_model(model, 10, optimizer, train_loader, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tUSS8ScsoBZj"
   },
   "source": [
    "# Final Remarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CVNCi5FzoELK"
   },
   "source": [
    "This notebook is to give you some sense as to how we go about improving neural networks.\n",
    "The process involves a fair bit of experimentation.\n",
    "However you should have the following takeaways from this notebook:\n",
    "*   When in doubt, go with `Adam` as this also allows us to train for epochs or\n",
    "*   use `Deeper Networks` that will outperform samller networks.\n",
    "*   `Batch Normalization` does wonders!\n",
    "*   `Dropout` layers can help with overfitting.\n",
    "*   `Hyperparameter Tuning` can improve resutls as well.\n",
    "\n",
    "A number of tips that were not covered in this notebook but you should keep in mind:\n",
    "*   `Data Augmentation` can really help (don't worry we will learn more about this in the next chapter)\n",
    "*   Incorporating the `Inductive Bias` allows us to come up with better architectures which in turn will yield much better results more efficiently! (our next two chapters will focus on the concept of different architectures)\n",
    "*   Sometimes you must tune the hyperparameters to get the results you want.\n",
    "\n",
    "Keep in mind some of these methods might not work as well as you expect and should always experiment on your own!\n",
    "\n",
    "As a challenge try and apply the methods covered in this notebook in a different order to see their effects.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "N3q3WK-FCcNJ"
   ],
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

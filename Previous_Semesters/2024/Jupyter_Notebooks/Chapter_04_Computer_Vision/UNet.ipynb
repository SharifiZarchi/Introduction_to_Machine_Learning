{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iQq8DDDGeaSt"
   },
   "source": [
    "# **U - Net**\n",
    "\n",
    "[CE477: Machine Learning](https://www.sharifml.ir/)\n",
    "\n",
    "__Course Instructor__: Dr. Sharifi-Zarchi\n",
    "\n",
    "__Notebook Authors__: Kiarash Joolaei\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/SharifiZarchi/Introduction_to_Machine_Learning/blob/main/Jupyter_Notebooks/Chapter_04_Computer_Vision/UNet.ipynb)\n",
    "[![Open In kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://raw.githubusercontent.com/SharifiZarchi/Introduction_to_Machine_Learning/main/Jupyter_Notebooks/Chapter_04_Computer_Vision/UNet.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "## Notebook Objectives\n",
    "In this notebook we introduce the U - Net architecture and investigate its use cases in different tasks of computer vision.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P5cRblesfMAp"
   },
   "source": [
    "## Transposed Convolution\n",
    "\n",
    "To start things off we first get familiar with the transposed convolution operation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o4QNoyncg6yV"
   },
   "source": [
    "**Transposed convolution**, also known as deconvolution, is a way **upsample** feature maps, often for tasks like image segmentation and generation. It essentially reverses the downsampling effect of a standard convolution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uyq2SVyklM3A"
   },
   "source": [
    "In 2D manner, transposed convolution can be defined alongside convolution as below.\n",
    "\n",
    "\n",
    "*   Convolution\n",
    "$$\n",
    "\\text{output}[i, j] = \\sum_{k=0}^{K-1} \\sum_{l=0}^{L-1} \\text{input}[i+k, j+l] \\times \\text{kernel}[k, l]\n",
    "$$\n",
    "*   Transposed Convolution\n",
    "$$\n",
    "\\text{reconstructed  input}[i, j] = \\sum_{k=0}^{K-1} \\sum_{l=0}^{L-1} \\text{output}[i-k, j-l] \\times \\text{kernel}[k, l]\n",
    "$$\n",
    "\n",
    "It can be seen that transposed convolution increases the dimensionality of the its input. As an example you can see and compare how convolution and transposed convolution below :\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C2QW2rKZkfZm"
   },
   "source": [
    "<div style=\"text-align:center;\">\n",
    "  <img src=\"Images/transposed.gif\" style=\"width:600px; height:auto;\" />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fb8nMcV7n4I5"
   },
   "source": [
    "To be more precise, the output size can be calculated as :\n",
    "\n",
    "$$O = S \\times (I - 1) + K - 2P$$\n",
    "\n",
    "where $O$ is the output size, $I$ is the input size, $K$ is the kernel size, and lastly $S$ and $P$ are stride and padding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vbUYhGMbol9w"
   },
   "source": [
    "Now we will implement this operation and see its results on images. We first convolve and then transpose convolve a kernel to compare the output with input. Note that the output is not gonna be the input exactly but rather an unsampled embedding of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 673
    },
    "id": "ty6Y_Z-bouO1",
    "outputId": "6b310cc8-6468-413b-fe4c-397ed62615d8"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "# Function to apply a 2D convolution\n",
    "def convolution_2d(input, kernel, stride=1, padding=0):\n",
    "    # Apply padding to the input\n",
    "    if padding > 0:\n",
    "        input = np.pad(input, ((padding, padding), (padding, padding)), mode='constant')\n",
    "\n",
    "    # Calculate the output size\n",
    "    input_height, input_width = input.shape\n",
    "    kernel_size = kernel.shape[0]\n",
    "    output_height = (input_height - kernel_size) // stride + 1\n",
    "    output_width = (input_width - kernel_size) // stride + 1\n",
    "\n",
    "    # Initialize output with zeros\n",
    "    output = np.zeros((output_height, output_width))\n",
    "\n",
    "    # Perform convolution operation\n",
    "    for i in range(0, output_height):\n",
    "        for j in range(0, output_width):\n",
    "            # Extract the current region of interest\n",
    "            current_region = input[i*stride:i*stride+kernel_size, j*stride:j*stride+kernel_size]\n",
    "            # Perform element-wise multiplication and sum the result\n",
    "            output[i, j] = np.sum(current_region * kernel)\n",
    "\n",
    "    return output\n",
    "\n",
    "# Function to apply a 2D transposed convolution\n",
    "def transposed_convolution_2d(input, kernel, stride=1, padding=0):\n",
    "    # Upsample by inserting zeros between input elements\n",
    "    upsampled_size = ((input.shape[0] - 1) * stride + 1, (input.shape[1] - 1) * stride + 1)\n",
    "    upsampled = np.zeros(upsampled_size)\n",
    "    upsampled[::stride, ::stride] = input  # Insert input values into upsampled array\n",
    "\n",
    "    # Apply padding to the upsampled input\n",
    "    if padding > 0:\n",
    "        upsampled = np.pad(upsampled, ((padding, padding), (padding, padding)), mode='constant')\n",
    "\n",
    "    # Calculate the output size\n",
    "    upsampled_height, upsampled_width = upsampled.shape\n",
    "    kernel_size = kernel.shape[0]\n",
    "    output_height = upsampled_height - kernel_size + 1\n",
    "    output_width = upsampled_width - kernel_size + 1\n",
    "\n",
    "    # Initialize output with zeros\n",
    "    output = np.zeros((output_height, output_width))\n",
    "\n",
    "    # Perform convolution-like operation on the upsampled input\n",
    "    for i in range(output_height):\n",
    "        for j in range(output_width):\n",
    "            # Perform element-wise multiplication and sum the result\n",
    "            output[i, j] = np.sum(upsampled[i:i+kernel_size, j:j+kernel_size] * kernel)\n",
    "\n",
    "    return output\n",
    "\n",
    "# Example kernel (3x3)\n",
    "kernel = np.array([[-1, -1, -1],\n",
    "                   [-1, 8, -1],\n",
    "                   [-1, -1, -1]])\n",
    "\n",
    "# Input feature map (for simplicity, let's use a 4x4 input)\n",
    "input_map = np.array([[1, 2, 1, 2],\n",
    "                      [3, 4, 3, 4],\n",
    "                      [1, 2, 1, 2],\n",
    "                      [3, 4, 3, 4]])\n",
    "\n",
    "# Apply the convolution\n",
    "conv_output = convolution_2d(input_map, kernel, stride=1, padding=1)\n",
    "\n",
    "# Apply the transposed convolution to the convolution output\n",
    "trans_conv_output = transposed_convolution_2d(conv_output, kernel, stride=1, padding=1)\n",
    "\n",
    "print(\"Input Map:\")\n",
    "print(input_map)\n",
    "print(\"\\nConvolution Output:\")\n",
    "print(conv_output)\n",
    "print(\"\\nTransposed Convolution Output:\")\n",
    "print(trans_conv_output)\n",
    "\n",
    "# Visualize the input and outputs\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(input_map, cmap='gray')\n",
    "plt.title('Input Feature Map')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow(conv_output, cmap='gray')\n",
    "plt.title('Convolution Output')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.imshow(trans_conv_output, cmap='gray')\n",
    "plt.title('Transposed Convolution Output')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 378
    },
    "id": "P1Kb5BoSpCIr",
    "outputId": "5104de80-667a-40cc-ff3c-12ece2d8d9dd"
   },
   "outputs": [],
   "source": [
    "# Function to preprocess an image\n",
    "def preprocess_image(image_path):\n",
    "    img = Image.open(image_path).convert('L')  # Load as grayscale\n",
    "    transform = transforms.Compose([transforms.ToTensor(), transforms.Resize((128, 128))])\n",
    "    return transform(img).squeeze(0).numpy()  # Remove batch dimension for simplicity\n",
    "\n",
    "# Function to visualize input and output images\n",
    "def visualize_images(input_image, conv_output, trans_conv_output):\n",
    "    plt.figure(figsize=(15, 5))\n",
    "\n",
    "    # Original Image\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.imshow(input_image.squeeze(), cmap='gray')\n",
    "    plt.title('Input Image')\n",
    "    plt.axis('off')\n",
    "\n",
    "    # Convolution Output\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.imshow(conv_output, cmap='gray')\n",
    "    plt.title('Convolution Output')\n",
    "    plt.axis('off')\n",
    "\n",
    "    # Transposed Convolution Output\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.imshow(trans_conv_output, cmap='gray')\n",
    "    plt.title('Transposed Convolution Output')\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Load and preprocess the image\n",
    "input_image = preprocess_image('image.jpg')\n",
    "\n",
    "\n",
    "# Apply the transposed convolution to the image\n",
    "output_conv = convolution_2d(input_image, kernel, stride=1, padding=1)\n",
    "output_image = transposed_convolution_2d(output_conv, kernel, stride=1, padding=1)\n",
    "# Visualize the results\n",
    "visualize_images(input_image, output_conv, output_image)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vz-DcK6s1mQw"
   },
   "source": [
    "We can check the results with existing `torch.nn` modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 378
    },
    "id": "LBhK8e1GvW17",
    "outputId": "ce19913c-d3d1-4158-df07-d4d6ae7b0e4e"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Load and preprocess an image\n",
    "def load_image(image_path):\n",
    "    img = Image.open(image_path).convert('L')  # Load as grayscale\n",
    "    transform = transforms.Compose([transforms.Resize((128, 128)), transforms.ToTensor()])\n",
    "    input_image = transform(img).unsqueeze(0)  # Add batch dimension\n",
    "    return input_image\n",
    "\n",
    "# Visualize the input, convolution output, and transposed convolution output\n",
    "def visualize_results(input_image, conv_output, trans_conv_output):\n",
    "    plt.figure(figsize=(15, 5))\n",
    "\n",
    "    # Original Image\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.imshow(input_image.squeeze(), cmap='gray')\n",
    "    plt.title('Input Image')\n",
    "    plt.axis('off')\n",
    "\n",
    "    # Convolution Output\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.imshow(conv_output.detach().squeeze(), cmap='gray')\n",
    "    plt.title('Convolution Output')\n",
    "    plt.axis('off')\n",
    "\n",
    "    # Transposed Convolution Output\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.imshow(trans_conv_output.detach().squeeze(), cmap='gray')\n",
    "    plt.title('Transposed Convolution Output')\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Define a convolution layer with a 3x3 kernel\n",
    "conv = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "\n",
    "edge_detection_kernel = torch.tensor([[[[-1, -1, -1],\n",
    "                                        [-1,  8, -1],\n",
    "                                        [-1, -1, -1]]]], dtype=torch.float32)\n",
    "\n",
    "conv.weight.data = edge_detection_kernel\n",
    "\n",
    "input_image = load_image('image.jpg')\n",
    "\n",
    "# Apply the first convolution\n",
    "conv_output = conv(input_image)\n",
    "\n",
    "# Define a transposed convolution layer (using the same kernel size and padding)\n",
    "conv_transpose = nn.ConvTranspose2d(in_channels=1, out_channels=1, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "conv_transpose.weight.data = edge_detection_kernel\n",
    "\n",
    "# Apply the transposed convolution to the result of the first convolution\n",
    "trans_conv_output = conv_transpose(conv_output)\n",
    "\n",
    "visualize_results(input_image, conv_output, trans_conv_output)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W8NYginM2Q3p"
   },
   "source": [
    "## U - Net Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "93NaFGHv2k1x"
   },
   "source": [
    "In this section, we're going to first implement the structure of U - Net and then use it in a segmentation task on Oxford Pets III dataset.\n",
    "\n",
    "One of the most popular usages of U - Nets is semantic segmentation in which we try to segment different pixels of an image and put the pixels with synonymous semantics in similar segments.\n",
    "\n",
    "Oxford Pets III dataset is designed solely for the purpose of segmenting the existing pet in each photo.\n",
    "This dataset is a simple and understandable candidate for a segmentation task that can be solved using U - Net."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aCjolSEC4Jol"
   },
   "source": [
    "<div style=\"text-align:center;\">\n",
    "  <img src=\"Images/segment.jpg\" style=\"width:700px; height:auto;\" />\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NFUvAGMALKSg"
   },
   "source": [
    "### Dataset Download"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YJk1tUXe2hdP"
   },
   "source": [
    "Let's download the dataset from kaggle. Importing **Kaggle API Token** is essential."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 73
    },
    "id": "RalMSI5ccjvh",
    "outputId": "03c79446-a5f0-494d-b399-90f3cf42713c"
   },
   "outputs": [],
   "source": [
    "# Create the directory where the Kaggle API expects the file\n",
    "!mkdir -p /root/.kaggle\n",
    "\n",
    "from google.colab import files\n",
    "files.upload()\n",
    "# Move the kaggle.json file to the correct directory\n",
    "!mv kaggle.json /root/.kaggle/\n",
    "\n",
    "# Set permissions for the API token file\n",
    "!chmod 600 /root/.kaggle/kaggle.json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BM5dzUeJJCAd",
    "outputId": "3cedc0a8-b326-47c7-83a2-2e8aed18a129"
   },
   "outputs": [],
   "source": [
    "!kaggle datasets download -d julinmaloof/the-oxfordiiit-pet-dataset\n",
    "\n",
    "# Unzip the dataset\n",
    "!unzip the-oxfordiiit-pet-dataset.zip -d oxford_pets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t1jSgtUiLC1c"
   },
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "49EzpDaKK3Hd"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset,DataLoader,SubsetRandomSampler\n",
    "from torchvision import transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ij6yDLP-LRWA"
   },
   "source": [
    "### U-Net Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uCD_e4qn8APp"
   },
   "source": [
    "U-Net is a convolutional neural network architecture designed primarily for image segmentation tasks. The architecture consists of two main parts: a contracting **encoder** path and an expansive **decoder** path.\n",
    "\n",
    "\n",
    "The encoder captures context and reduces spatial dimensions through convolution and pooling layers, while the decoder upsamples the features to recover spatial resolution, enabling precise localization.\n",
    "\n",
    "\n",
    "A key feature of U-Net is the use of skip connections, which directly transfer feature maps from the encoder to the corresponding layers in the decoder, helping to preserve detailed spatial information and improve segmentation accuracy. This makes U-Net especially effective for medical imaging and other applications requiring high-resolution segmentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7SQGAjZa82E4"
   },
   "source": [
    "<div style=\"text-align:center;\">\n",
    "  <img src=\"./Images/u-net-architecture.png\" style=\"width:700px; height:auto;\" />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SnAbzmVGWDjk"
   },
   "outputs": [],
   "source": [
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(UNet, self).__init__()\n",
    "\n",
    "        def conv_block(in_channels, out_channels):\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "                nn.ReLU(inplace=True)\n",
    "            )\n",
    "\n",
    "        self.encoder1 = conv_block(3, 64)\n",
    "        self.encoder2 = conv_block(64, 128)\n",
    "        self.encoder3 = conv_block(128, 256)\n",
    "        self.encoder4 = conv_block(256, 512)\n",
    "\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "\n",
    "        self.bottleneck = conv_block(512, 1024)\n",
    "\n",
    "        self.upconv4 = nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2)\n",
    "        self.decoder4 = conv_block(1024, 512)\n",
    "\n",
    "        self.upconv3 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
    "        self.decoder3 = conv_block(512, 256)\n",
    "\n",
    "        self.upconv2 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
    "        self.decoder2 = conv_block(256, 128)\n",
    "\n",
    "        self.upconv1 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "        self.decoder1 = conv_block(128, 64)\n",
    "\n",
    "        self.output_layer = nn.Conv2d(64, 1, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        e1 = self.encoder1(x)\n",
    "        e2 = self.encoder2(self.pool(e1))\n",
    "        e3 = self.encoder3(self.pool(e2))\n",
    "        e4 = self.encoder4(self.pool(e3))\n",
    "\n",
    "        # Bottleneck\n",
    "        b = self.bottleneck(self.pool(e4))\n",
    "\n",
    "        # Decoder\n",
    "        d4 = self.upconv4(b)\n",
    "        d4 = torch.cat([d4, e4], dim=1)\n",
    "        d4 = self.decoder4(d4)\n",
    "\n",
    "        d3 = self.upconv3(d4)\n",
    "        d3 = torch.cat([d3, e3], dim=1)\n",
    "        d3 = self.decoder3(d3)\n",
    "\n",
    "        d2 = self.upconv2(d3)\n",
    "        d2 = torch.cat([d2, e2], dim=1)\n",
    "        d2 = self.decoder2(d2)\n",
    "\n",
    "        d1 = self.upconv1(d2)\n",
    "        d1 = torch.cat([d1, e1], dim=1)\n",
    "        d1 = self.decoder1(d1)\n",
    "\n",
    "        out = self.output_layer(d1)\n",
    "        return torch.sigmoid(out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s6_yrn0bLVen"
   },
   "source": [
    "### Dataset Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IcoZ0e299mwX"
   },
   "source": [
    "In this section the dataset is defined as a module inherited from `Dataset`.\n",
    "\n",
    "The masks are mapped into binary classes **pet** and **background**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gryscmNZWJ4E"
   },
   "outputs": [],
   "source": [
    "class OxfordPetsDataset(Dataset):\n",
    "    def __init__(self, image_dir, mask_dir, image_transform=None, mask_transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.image_transform = image_transform\n",
    "        self.mask_transform = mask_transform\n",
    "\n",
    "        # Initialize a list to hold valid images and masks\n",
    "        self.valid_images = []\n",
    "        self.valid_masks = []\n",
    "\n",
    "        # Populate the lists with valid images and masks\n",
    "        for filename in os.listdir(image_dir):\n",
    "            if filename.endswith('.jpg'):\n",
    "                img_path = os.path.join(image_dir, filename)\n",
    "                mask_path = os.path.join(mask_dir, filename.replace('.jpg', '.png'))\n",
    "\n",
    "                # Attempt to read the image\n",
    "                image = cv2.imread(img_path)\n",
    "                if image is not None:  # Only add valid images\n",
    "                    # Attempt to read the mask\n",
    "                    mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "                    if mask is not None:  # Only add valid masks\n",
    "                        self.valid_images.append(img_path)\n",
    "                        self.valid_masks.append(mask_path)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.valid_images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.valid_images[idx]\n",
    "        mask_path = self.valid_masks[idx]\n",
    "\n",
    "        # Read the image\n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Read the mask\n",
    "        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "        # Convert trimap to binary mask (1: pet, 0: background)\n",
    "        mask = np.where(mask == 3, 1, 0).astype('float32')\n",
    "\n",
    "        # Apply transformations if provided\n",
    "        if self.image_transform:\n",
    "            image = self.image_transform(image)\n",
    "\n",
    "        if self.mask_transform:\n",
    "            mask = self.mask_transform(mask)\n",
    "\n",
    "        return image, mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t-FJygAV7cCa"
   },
   "outputs": [],
   "source": [
    "\n",
    "image_transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "mask_transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "dataset = OxfordPetsDataset(\n",
    "    image_dir='oxford_pets/images',\n",
    "    mask_dir='oxford_pets/annotations/trimaps',\n",
    "    image_transform=image_transform,\n",
    "    mask_transform=mask_transform\n",
    ")\n",
    "\n",
    "train_indices, test_indices = train_test_split(np.arange(len(dataset)), test_size=0.2, random_state=42)\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "test_sampler = SubsetRandomSampler(test_indices)\n",
    "\n",
    "train_loader = DataLoader(dataset, batch_size=8, sampler=train_sampler)\n",
    "test_loader = DataLoader(dataset, batch_size=8, sampler=test_sampler)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kohxVQlwLnGA"
   },
   "source": [
    "### Training Phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 628
    },
    "id": "9ZDFchq0WRl_",
    "outputId": "a96f43c9-3d43-4fd8-c5b4-38d4d643b8c2"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = UNet().to(device)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "num_epochs = 10\n",
    "train_loss_history = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for images, masks in train_loader:\n",
    "        images, masks = images.to(device), masks.to(device)  # Add channel dimension to masks\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, masks)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    epoch_loss /= len(train_loader)\n",
    "    train_loss_history.append(epoch_loss)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "# Plot training error\n",
    "plt.plot(range(num_epochs), train_loss_history)\n",
    "plt.title('Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PtmdajZlLqwX"
   },
   "source": [
    "### Evaluation Phase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5ToXX--T6pZD"
   },
   "source": [
    "Here the model is tested on a random sample from the test set. The model should be able to mask out the pet correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 332
    },
    "id": "zQCFFfbMWVar",
    "outputId": "01303bf6-1990-4136-844f-ce3b2c0e80e4"
   },
   "outputs": [],
   "source": [
    "def visualize_sample(model, test_loader, batch_num, idx):\n",
    "    model.eval()\n",
    "\n",
    "    # Get the specified batch of images and masks from the test loader\n",
    "    for i, (images, masks) in enumerate(test_loader):\n",
    "        if i == batch_num:\n",
    "            break  # Stop when the desired batch is found\n",
    "\n",
    "    # Select the image and mask at the specified index within the batch\n",
    "    image = images[idx]\n",
    "    mask = masks[idx].numpy()  # Convert mask to numpy for visualization\n",
    "\n",
    "    # Move the image to the appropriate device (GPU or CPU)\n",
    "    image = image.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Make prediction\n",
    "        pred_mask = model(image.unsqueeze(0)).cpu().numpy()  # Add batch dimension\n",
    "        pred_mask = (pred_mask > 0.5).astype(np.uint8)\n",
    "\n",
    "    # Squeeze the predicted mask to remove the channel dimension\n",
    "    pred_mask = np.squeeze(pred_mask)  # Now shape will be (128, 128)\n",
    "    mask = np.squeeze(mask)  # Ensure the ground truth mask is also (128, 128)\n",
    "\n",
    "    # Display the images and masks\n",
    "    fig, ax = plt.subplots(1, 3, figsize=(12, 4))\n",
    "\n",
    "    ax[0].imshow(image.cpu().permute(1, 2, 0))  # Original Image\n",
    "    ax[0].set_title('Original Image')\n",
    "    ax[0].axis('off')  # Remove axis ticks and labels\n",
    "\n",
    "    ax[1].imshow(mask, cmap='gray')  # Ground Truth Mask\n",
    "    ax[1].set_title('Ground Truth Mask')\n",
    "    ax[1].axis('off')  # Remove axis ticks and labels\n",
    "\n",
    "    ax[2].imshow(pred_mask, cmap='gray')  # Predicted Mask\n",
    "    ax[2].set_title('Predicted Mask')\n",
    "    ax[2].axis('off')  # Remove axis ticks and labels\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Visualize some results from a specific batch and index in the test set\n",
    "visualize_sample(model, test_loader, batch_num=3, idx=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6l7_aGoYLwNR"
   },
   "source": [
    "### Saving the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k2tz_yKY6YVc"
   },
   "source": [
    "Saving the model parameters enables us to load the pre trained model whenever needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5-8GpbbKDGZh",
    "outputId": "dcb4cc0b-cde5-43a3-92ae-a933952d698a"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vgU84XlcFaaV"
   },
   "outputs": [],
   "source": [
    "model_save_path = '/content/drive/My Drive/unet_model.pth'\n",
    "\n",
    "# Save the model\n",
    "torch.save(model.state_dict(), model_save_path)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

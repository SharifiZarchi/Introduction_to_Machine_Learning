{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "introduction",
   "metadata": {
    "id": "introduction"
   },
   "source": [
    "# Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3802b3e",
   "metadata": {
    "id": "c3802b3e"
   },
   "source": [
    "## 1. Introduction to Dimensionality Reduction\n",
    "\n",
    "### What is Dimensionality Reduction?\n",
    "\n",
    "In the realm of machine learning and data analysis, we frequently work with datasets that have a high number of features (dimensions). While more features can potentially provide more information, high-dimensional data often presents several challenges:\n",
    "\n",
    "- **Curse of Dimensionality**: As the number of dimensions increases, the volume of the feature space grows exponentially, making the data sparse. This sparsity can degrade the performance of machine learning algorithms, as they struggle to find patterns in the data.\n",
    "- **Computational Complexity**: High-dimensional data increases the computational cost and time for processing and modeling.\n",
    "- **Overfitting**: Models trained on high-dimensional data can overfit, capturing noise instead of the underlying patterns.\n",
    "- **Difficulty in Visualization**: Visualizing data beyond three dimensions is non-trivial, limiting our ability to intuitively understand and explore the data.\n",
    "\n",
    "**Dimensionality Reduction** refers to the process of reducing the number of input variables in a dataset while preserving as much information as possible. This simplification helps in making the data more manageable and can improve the performance of machine learning models.\n",
    "\n",
    "Dimensionality reduction techniques are broadly categorized into:\n",
    "\n",
    "- **Feature Selection**: Selecting a subset of the most significant features.\n",
    "- **Feature Extraction**: Transforming the data into a lower-dimensional space (e.g., using PCA) by creating new features that are combinations of the original features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "applications",
   "metadata": {
    "id": "applications"
   },
   "source": [
    "## 2. Applications of Dimensionality Reduction\n",
    "\n",
    "Dimensionality reduction has several crucial applications and we will explore the following applications:\n",
    "\n",
    "- **Data Visualization**: By reducing data to two or three dimensions, we can create visual representations that help in identifying patterns, clusters, and outliers.\n",
    "- **Noise Reduction**: Eliminating less important features can reduce noise, leading to cleaner data and better model performance.\n",
    "- **Feature Extraction**: Helps in identifying and combining features that capture the most variance in the data.\n",
    "- **Speeding Up Algorithms**: Simplifying data reduces computational requirements, leading to faster training and prediction times.\n",
    "- **Preventing Overfitting**: Reducing the number of features decreases model complexity, which can prevent overfitting and improve generalization to new data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data-visualization",
   "metadata": {
    "id": "data-visualization"
   },
   "source": [
    "### 2.1 Data Visualization\n",
    "\n",
    "**Dataset**: MNIST Handwritten Digits Dataset\n",
    "\n",
    "**Description**:\n",
    "\n",
    "The MNIST dataset consists of 70,000 images of handwritten digits (0-9), each of size 28x28 pixels, resulting in a 784-dimensional feature space. Visualizing this high-dimensional data is challenging.\n",
    "\n",
    "**Objective**:\n",
    "\n",
    "- Use PCA to reduce the dimensionality of the MNIST dataset from 784 to 2 dimensions.\n",
    "- Visualize the data in 2D to observe patterns and clusters.\n",
    "\n",
    "**Visualization**:\n",
    "\n",
    "![MNIST PCA Visualization](pics/mnist_pca_visualizations.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "noise-reduction",
   "metadata": {
    "id": "noise-reduction"
   },
   "source": [
    "### 2.2 Noise Reduction\n",
    "\n",
    "**Dataset**: Fashion MNIST Dataset with Added Noise\n",
    "\n",
    "**Description**:\n",
    "\n",
    "The Fashion MNIST dataset is similar to MNIST but contains images of clothing items. We'll add noise to the images and use PCA to reconstruct them, effectively reducing noise.\n",
    "\n",
    "**Objective**:\n",
    "\n",
    "- Add Gaussian noise to the images.\n",
    "- Use PCA to reconstruct the images from a reduced number of components.\n",
    "- Compare original, noisy, and reconstructed images to observe noise reduction.\n",
    "\n",
    "**Visualization**:\n",
    "\n",
    "![Fashion MNIST Noise Reduction](pics/fashion_mnist_noise_reduction.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feature-extraction",
   "metadata": {
    "id": "feature-extraction"
   },
   "source": [
    "### 2.3 Feature Extraction\n",
    "\n",
    "**Dataset**: Labeled Faces in the Wild (LFW)\n",
    "\n",
    "**Description**:\n",
    "\n",
    "The LFW dataset consists of images of faces collected from the web. Each face is represented by high-dimensional pixel data.\n",
    "\n",
    "**Objective**:\n",
    "\n",
    "- Use PCA to extract principal components known as \"eigenfaces\".\n",
    "- Visualize the eigenfaces to understand the most significant features in the dataset.\n",
    "\n",
    "**Visualization**:\n",
    "\n",
    "![LFW Eigenfaces](pics/lfw_eigenfaces.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db1f5192",
   "metadata": {
    "id": "db1f5192"
   },
   "source": [
    "## 3. PCA: Principal Component Analysis\n",
    "\n",
    "### Understanding PCA\n",
    "\n",
    "Principal Component Analysis (PCA) is a statistical technique used for dimensionality reduction while preserving as much variability (information) as possible. It transforms the original data into a new coordinate system where the greatest variances lie on the first coordinates (principal components), the second greatest variances on the second coordinates, and so on.\n",
    "\n",
    "### **Goal**\n",
    "\n",
    "Find the directions (principal components) that maximize the variance in the data.\n",
    "\n",
    "### **Process Overview**\n",
    "\n",
    "1. **Standardize the Data**: Center and scale the data.\n",
    "2. **Compute the Covariance Matrix**.\n",
    "3. **Calculate Eigenvalues and Eigenvectors**.\n",
    "4. **Sort Eigenvectors**: Based on eigenvalues in descending order.\n",
    "5. **Project Data**: Onto the selected principal components."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2069e93f",
   "metadata": {
    "id": "2069e93f"
   },
   "source": [
    "### Dataset Description: Fashion MNIST\n",
    "\n",
    "The **Fashion MNIST** dataset is a popular alternative to the original MNIST dataset of handwritten digits. It contains 70,000 grayscale images of fashion items, each of size 28x28 pixels. There are 10 categories/classes:\n",
    "\n",
    "0. T-shirt/top\n",
    "1. Trouser\n",
    "2. Pullover\n",
    "3. Dress\n",
    "4. Coat\n",
    "5. Sandal\n",
    "6. Shirt\n",
    "7. Sneaker\n",
    "8. Bag\n",
    "9. Ankle boot\n",
    "\n",
    "Each image is represented as a 784-dimensional vector (since 28x28 = 784), making it suitable for demonstrating PCA on high-dimensional data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3cc8561",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f3cc8561",
    "outputId": "0d94c397-a984-4f6c-dbdb-ca160503b084"
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load Fashion MNIST dataset\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "\n",
    "# Load data\n",
    "(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n",
    "\n",
    "# Combine train and test data for PCA\n",
    "X = np.concatenate((X_train, X_test), axis=0)\n",
    "y = np.concatenate((y_train, y_test), axis=0)\n",
    "\n",
    "# Flatten images to create feature vectors\n",
    "X_flattened = X.reshape(X.shape[0], -1)\n",
    "\n",
    "# Display the shape of the data\n",
    "print(f\"Data shape: {X_flattened.shape}\")  # Should be (70000, 784)\n",
    "print(f\"Labels shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875ed771",
   "metadata": {
    "id": "875ed771"
   },
   "source": [
    "#### Visualize a Sample Image\n",
    "\n",
    "Let's visualize a sample image from the dataset to understand what the data represents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df0d677",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 428
    },
    "id": "7df0d677",
    "outputId": "988ff097-9503-4e9f-d73b-2af35d5b140a"
   },
   "outputs": [],
   "source": [
    "# Define class names\n",
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "\n",
    "# Visualize a sample image\n",
    "plt.imshow(X[0], cmap='gray')\n",
    "plt.title(f\"Label: {class_names[y[0]]}\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c9c583",
   "metadata": {},
   "source": [
    "### From Scratch Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3551ae0",
   "metadata": {
    "id": "e3551ae0"
   },
   "source": [
    "#### Step 1: Standardize the Data\n",
    "\n",
    "Before performing PCA, it's important to standardize the data because PCA is sensitive to the variances of the original variables. Variables with larger scales can dominate the principal components if data is not standardized.\n",
    "\n",
    "##### **Mathematical Expression**\n",
    "\n",
    "Given a dataset $X$ with $n$ samples and $p$ features, the standardized data $Z$ is computed as:\n",
    "\n",
    "$$\n",
    "Z_{ij} = \\frac{X_{ij} - \\mu_j}{\\sigma_j}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $X_{ij}$ is the value of the $j^{th}$ feature for the $i^{th}$ sample.\n",
    "- $\\mu_j$ is the mean of the $j^{th}$ feature.\n",
    "- $\\sigma_j$ is the standard deviation of the $j^{th}$ feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24cae8b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e24cae8b",
    "outputId": "d4f8e660-3d67-478e-a724-c12dbd375916"
   },
   "outputs": [],
   "source": [
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_standardized = scaler.fit_transform(X_flattened)\n",
    "\n",
    "# Display the shape of the standardized data\n",
    "print(f\"Standardized Data Shape: {X_standardized.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec2a089",
   "metadata": {
    "id": "2ec2a089"
   },
   "source": [
    "#### Step 2: Compute the Covariance Matrix\n",
    "\n",
    "The covariance matrix expresses the correlation between different features in the data. It is a square matrix that provides the covariance between each pair of features.\n",
    "\n",
    "##### **Mathematical Expression**\n",
    "\n",
    "For standardized data $Z$, the covariance matrix $C$ is computed as:\n",
    "\n",
    "$$\n",
    "C = \\frac{1}{n - 1} Z^T Z\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $Z^T$ is the transpose of the standardized data matrix $Z$.\n",
    "- $n$ is the number of samples.\n",
    "\n",
    "**Note**: Since the number of features (784) is large, computing the full covariance matrix can be computationally intensive. Therefore, we may use techniques like Singular Value Decomposition (SVD) or truncated computations to handle large datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bebdfc36",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bebdfc36",
    "outputId": "b3879c74-dc41-46f2-ca62-663fb444409c"
   },
   "outputs": [],
   "source": [
    "# Compute the covariance matrix\n",
    "# For large datasets, it's more efficient to compute the covariance matrix using numpy's dot product\n",
    "n_samples = X_standardized.shape[0]\n",
    "cov_matrix = np.dot(X_standardized.T, X_standardized) / (n_samples - 1)\n",
    "\n",
    "# Display the shape of the covariance matrix\n",
    "print(f\"Covariance Matrix Shape: {cov_matrix.shape}\")\n",
    "\n",
    "# Display the covariance matrix\n",
    "print(f\"Covariance Matrix: \\n{cov_matrix}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd3c883",
   "metadata": {
    "id": "1cd3c883"
   },
   "source": [
    "#### Step 3: Calculate Eigenvalues and Eigenvectors\n",
    "\n",
    "Eigenvalues and eigenvectors are computed from the covariance matrix to identify the principal components. The eigenvectors represent the directions of maximum variance (principal components), and the eigenvalues indicate the magnitude of variance in those directions.\n",
    "\n",
    "##### **Mathematical Expression**\n",
    "\n",
    "We solve the equation:\n",
    "\n",
    "$$\n",
    "C \\mathbf{v} = \\lambda \\mathbf{v}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $C$ is the covariance matrix.\n",
    "- $\\mathbf{v}$ is the eigenvector.\n",
    "- $\\lambda$ is the eigenvalue corresponding to $\\mathbf{v}$.\n",
    "\n",
    "**Note**: Computing eigenvalues and eigenvectors for large covariance matrices can be computationally intensive. We can use SVD to compute the principal components directly without computing the covariance matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a965ece3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a965ece3",
    "outputId": "a0902d4b-1f68-4a80-d1b4-1067091faae9"
   },
   "outputs": [],
   "source": [
    "# Compute eigenvalues and eigenvectors using Singular Value Decomposition (SVD)\n",
    "U, S, VT = np.linalg.svd(X_standardized, full_matrices=False)\n",
    "\n",
    "# Eigenvalues are the square of singular values divided by (n_samples - 1)\n",
    "eig_values = (S ** 2) / (n_samples - 1)\n",
    "\n",
    "# Eigenvectors are the transpose of V from SVD\n",
    "eig_vectors = VT.T\n",
    "\n",
    "# Display the shape of eigenvalues and eigenvectors\n",
    "print(f\"Eigenvalues Shape: {eig_values.shape}\")\n",
    "print(f\"Eigenvectors Shape: {eig_vectors.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b84e4e",
   "metadata": {
    "id": "e8b84e4e"
   },
   "source": [
    "#### Step 4: Sort Eigenvectors by Eigenvalues in Descending Order\n",
    "\n",
    "We sort the eigenvectors so that the principal components are ordered according to the amount of variance they explain.\n",
    "\n",
    "##### **Process**\n",
    "\n",
    "- Pair each eigenvalue with its corresponding eigenvector.\n",
    "- Sort the pairs in descending order based on the eigenvalues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a351594d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a351594d",
    "outputId": "7058f415-8df8-453f-c7da-e3112c4d0f34"
   },
   "outputs": [],
   "source": [
    "# Create a list of (eigenvalue, eigenvector) tuples\n",
    "eig_pairs = [(eig_values[i], eig_vectors[:, i]) for i in range(len(eig_values))]\n",
    "\n",
    "# Sort the (eigenvalue, eigenvector) tuples from high to low\n",
    "eig_pairs.sort(key=lambda x: x[0], reverse=True)\n",
    "\n",
    "# Extract the sorted eigenvalues and eigenvectors\n",
    "sorted_eig_values = np.array([eig_pair[0] for eig_pair in eig_pairs])\n",
    "sorted_eig_vectors = np.array([eig_pair[1] for eig_pair in eig_pairs]).T\n",
    "\n",
    "# Display the top 10 eigenvalues\n",
    "print(\"Top 10 Eigenvalues:\")\n",
    "for i in range(10):\n",
    "    print(f\"Eigenvalue {i+1}: {sorted_eig_values[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba2840d",
   "metadata": {
    "id": "9ba2840d"
   },
   "source": [
    "#### Step 5: Project Data onto the Selected Principal Components\n",
    "\n",
    "We construct a projection matrix using the top $k$ eigenvectors to transform the data into the new feature space (principal components).\n",
    "\n",
    "##### **Mathematical Expression**\n",
    "\n",
    "The projection matrix $W$ is formed by stacking the top $k$ eigenvectors:\n",
    "\n",
    "$$\n",
    "W = [\\mathbf{v}_1, \\mathbf{v}_2, \\dots, \\mathbf{v}_k]\n",
    "$$\n",
    "\n",
    "The projected data $Z$ is obtained by:\n",
    "\n",
    "$$\n",
    "Z = X_{\\text{standardized}} \\cdot W\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $X_{\\text{standardized}}$ is the standardized data matrix.\n",
    "- $W$ is the projection matrix containing the top $k$ eigenvectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457af594",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "457af594",
    "outputId": "073d56f0-8a31-48ae-9e9a-5af907681980"
   },
   "outputs": [],
   "source": [
    "# Select the top k eigenvectors (here we choose k=2 for visualization)\n",
    "k = 2\n",
    "matrix_w = sorted_eig_vectors[:, :k]\n",
    "\n",
    "# Project the standardized data onto the new feature space\n",
    "X_pca = X_standardized.dot(matrix_w)\n",
    "\n",
    "# Create a DataFrame with the projected data\n",
    "principal_df = pd.DataFrame(X_pca, columns=[f'PC{i+1}' for i in range(k)])\n",
    "principal_df['label'] = y\n",
    "\n",
    "# Display the first 5 rows of the projected data\n",
    "print(\"Projected Data (first 5 samples):\")\n",
    "print(principal_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148b8b43",
   "metadata": {
    "id": "148b8b43"
   },
   "source": [
    "#### Explained Variance Ratio\n",
    "\n",
    "The explained variance ratio indicates how much variance is explained by each of the selected principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576a7ddd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 755
    },
    "id": "576a7ddd",
    "outputId": "a2b00296-653b-4e7f-bd81-dd8de8ea8196"
   },
   "outputs": [],
   "source": [
    "# Compute the explained variance ratio\n",
    "explained_variance_ratio = sorted_eig_values / np.sum(sorted_eig_values)\n",
    "\n",
    "# Display the explained variance ratio for the top 10 components\n",
    "print(\"Explained Variance Ratio for Top 10 Components:\")\n",
    "for i in range(10):\n",
    "    print(f\"PC{i+1}: {explained_variance_ratio[i]:.4f}\")\n",
    "\n",
    "# Plot the explained variance ratio for the top 10 components\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "components = np.arange(1, 11)\n",
    "plt.bar(components, explained_variance_ratio[:10], alpha=0.7, color='blue')\n",
    "plt.xlabel('Principal Components')\n",
    "plt.ylabel('Explained Variance Ratio')\n",
    "plt.title('Explained Variance Ratio of Top 10 Principal Components')\n",
    "plt.xticks(components)\n",
    "plt.grid(axis='y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hVU00twdsOf2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "id": "hVU00twdsOf2",
    "outputId": "791be972-761b-456a-fbd2-0f0f6d2e4eee"
   },
   "outputs": [],
   "source": [
    "cumulative_variance_ratio = np.cumsum(explained_variance_ratio)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(range(1, len(cumulative_variance_ratio) + 1), cumulative_variance_ratio, marker='o', linestyle='--', color='green')\n",
    "plt.xlabel('Number of Principal Components')\n",
    "plt.ylabel('Cumulative Explained Variance Ratio')\n",
    "plt.title('Cumulative Explained Variance Ratio')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e64fa95",
   "metadata": {
    "id": "9e64fa95"
   },
   "source": [
    "#### Visualization of the Projected Data\n",
    "\n",
    "Now, we can visualize the data in the new feature space defined by the first two principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5b6cdc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 718
    },
    "id": "da5b6cdc",
    "outputId": "23759480-5296-45d6-adf3-c939bde63757"
   },
   "outputs": [],
   "source": [
    "# Plot the projected data\n",
    "plt.figure(figsize=(10,8))\n",
    "\n",
    "for label in np.unique(y):\n",
    "    indices = principal_df['label'] == label\n",
    "    plt.scatter(principal_df.loc[indices, 'PC1'],\n",
    "                principal_df.loc[indices, 'PC2'],\n",
    "                s=1, alpha=0.5, label=class_names[label])\n",
    "\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.title('PCA of Fashion MNIST Dataset')\n",
    "plt.legend(markerscale=6)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b1c715",
   "metadata": {
    "id": "b8b1c715"
   },
   "source": [
    "### PCA using Scikit-Learn's PCA Function\n",
    "\n",
    "While we've implemented PCA step by step, scikit-learn provides a convenient and optimized `PCA` class that performs PCA efficiently. It handles the computations internally and offers additional functionality, such as selecting the number of components based on the explained variance ratio.\n",
    "\n",
    "#### **Scikit-Learn PCA Parameters**\n",
    "\n",
    "- `n_components`: Number of principal components to keep. If `n_components` is between 0 and 1, it selects the number of components such that the amount of variance that needs to be explained is greater than the percentage specified by `n_components`.\n",
    "- `whiten`: When `True`, the components are multiplied by the square root of the eigenvalues to ensure uncorrelated outputs with unit variance.\n",
    "- `svd_solver`: Specifies the algorithm to use for computation. Options include `'auto'`, `'full'`, `'arpack'`, and `'randomized'`.\n",
    "\n",
    "#### **Advantages**\n",
    "\n",
    "- Optimized for performance and can handle large datasets efficiently.\n",
    "- Simplifies the process with fewer lines of code.\n",
    "- Provides additional features like inverse transformation, which can be useful for tasks like data reconstruction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151a1684",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "151a1684",
    "outputId": "5c460070-ca8b-4b4a-e660-fdcc54f5974b"
   },
   "outputs": [],
   "source": [
    "# Import PCA from scikit-learn\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Initialize PCA with the desired number of components\n",
    "k = 2  # Number of principal components\n",
    "pca = PCA(n_components=k)\n",
    "\n",
    "# Fit PCA on the standardized data\n",
    "X_pca_sklearn = pca.fit_transform(X_standardized)\n",
    "\n",
    "# Create a DataFrame with the projected data\n",
    "principal_df_sklearn = pd.DataFrame(X_pca_sklearn, columns=[f'PC{i+1}' for i in range(k)])\n",
    "principal_df_sklearn['label'] = y\n",
    "\n",
    "# Display the explained variance ratio\n",
    "print(\"Explained Variance Ratio by scikit-learn PCA:\")\n",
    "for i, ratio in enumerate(pca.explained_variance_ratio_):\n",
    "    print(f\"PC{i+1}: {ratio:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3cbebc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 718
    },
    "id": "5f3cbebc",
    "outputId": "6cd83992-803d-4a05-90db-f60db7958d51"
   },
   "outputs": [],
   "source": [
    "# Plot the projected data using scikit-learn PCA\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# No sampling - use the entire dataset\n",
    "for label in np.unique(principal_df_sklearn['label']):\n",
    "    label_indices = principal_df_sklearn['label'] == label\n",
    "    plt.scatter(principal_df_sklearn.loc[label_indices, 'PC1'],\n",
    "                principal_df_sklearn.loc[label_indices, 'PC2'],\n",
    "                s=1, alpha=0.5, label=class_names[label])\n",
    "\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.title('PCA of Fashion MNIST Dataset (scikit-learn)')\n",
    "plt.legend(markerscale=6)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9a7a03",
   "metadata": {
    "id": "ae9a7a03"
   },
   "source": [
    "#### Comparison and Discussion\n",
    "\n",
    "- **Consistency**: The results obtained using scikit-learn's PCA are consistent with our step-by-step implementation.\n",
    "- **Efficiency**: Scikit-learn's PCA is optimized and may be more efficient, especially for large datasets.\n",
    "- **Explained Variance**: We can easily access the explained variance ratio using the `explained_variance_ratio_` attribute.\n",
    "- **Ease of Use**: Scikit-learn simplifies the PCA process with fewer lines of code and additional functionalities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9744bf51",
   "metadata": {
    "id": "9744bf51"
   },
   "source": [
    "## 4. Case Study: Leukemia Gene Expression Dataset\n",
    "\n",
    "### Dataset Description\n",
    "\n",
    "The **Leukemia Gene Expression Dataset** consists of 72 samples with 7,129 gene expression features. The dataset includes two classes of leukemia:\n",
    "\n",
    "- **Acute Myeloid Leukemia (AML)**\n",
    "- **Acute Lymphoblastic Leukemia (ALL)**\n",
    "\n",
    "Due to the high dimensionality (7,129 features) and the small number of samples (72), this dataset is prone to overfitting. Dimensionality reduction techniques like PCA can be effective in preventing overfitting by reducing the number of features.\n",
    "\n",
    "### Data Preprocessing and Visualization\n",
    "\n",
    "We will:\n",
    "\n",
    "- Load the dataset.\n",
    "- Preprocess the data.\n",
    "- Apply PCA to reduce dimensionality.\n",
    "- Visualize the results.\n",
    "- Evaluate the performance of a logistic regression classifier with and without PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bbea738",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1bbea738",
    "outputId": "1ec8bd13-5b8e-4f9c-f3e9-73f9ed8e79a8"
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load the Leukemia dataset from OpenML\n",
    "leukemia = fetch_openml(data_id=1104, as_frame=False)\n",
    "X = leukemia.data\n",
    "y = leukemia.target\n",
    "\n",
    "# Convert labels to binary (0 and 1)\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)\n",
    "\n",
    "print(f\"Dataset shape: {X.shape}\")\n",
    "print(f\"Number of classes: {len(np.unique(y_encoded))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56be5462",
   "metadata": {
    "id": "56be5462"
   },
   "source": [
    "### Data Preprocessing\n",
    "\n",
    "- **Standardize the Data**: Standardizing features by removing the mean and scaling to unit variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403204e0",
   "metadata": {
    "id": "403204e0"
   },
   "outputs": [],
   "source": [
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ac03b5",
   "metadata": {
    "id": "d5ac03b5"
   },
   "source": [
    "### Model Training and Evaluation\n",
    "\n",
    "- **Define a Logistic Regression Model**\n",
    "- **Perform Stratified K-Fold Cross-Validation**\n",
    "- **Evaluate Model Performance Without PCA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a965ff54",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a965ff54",
    "outputId": "0e85ee44-4c37-4922-88bd-62bd0c0052b9"
   },
   "outputs": [],
   "source": [
    "# Define a logistic regression model\n",
    "model = LogisticRegression(max_iter=1000, solver='saga')\n",
    "\n",
    "# Stratified k-fold cross-validation\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Cross-validation without PCA\n",
    "scores_no_pca = cross_val_score(model, X_scaled, y_encoded, cv=cv, scoring='accuracy')\n",
    "mean_score_no_pca = np.mean(scores_no_pca)\n",
    "\n",
    "print(f\"\\nCross-validation accuracy without PCA: {mean_score_no_pca:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36af481a",
   "metadata": {
    "id": "36af481a"
   },
   "source": [
    "### Applying PCA\n",
    "\n",
    "- **Reduce Dimensionality Using PCA**\n",
    "- **Evaluate Model Performance With PCA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2579987e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2579987e",
    "outputId": "ba8963f7-692a-4bd6-ab7e-13d852c841ef"
   },
   "outputs": [],
   "source": [
    "# Apply PCA to reduce dimensionality\n",
    "pca = PCA(n_components=20, random_state=42)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Cross-validation with PCA\n",
    "scores_pca = cross_val_score(model, X_pca, y_encoded, cv=cv, scoring='accuracy')\n",
    "mean_score_pca = np.mean(scores_pca)\n",
    "\n",
    "print(f\"Cross-validation accuracy with PCA (20 components): {mean_score_pca:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6ab523",
   "metadata": {
    "id": "de6ab523"
   },
   "source": [
    "### Experimenting with Different Numbers of Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03609e2d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "03609e2d",
    "outputId": "bf1dfaf4-46bd-4048-a702-f28feef8546d"
   },
   "outputs": [],
   "source": [
    "# Optional: Experiment with different numbers of components\n",
    "components = [5, 10, 20, 50]\n",
    "scores = []\n",
    "\n",
    "for n in components:\n",
    "    pca = PCA(n_components=n, random_state=42)\n",
    "    X_pca = pca.fit_transform(X_scaled)\n",
    "    score = np.mean(cross_val_score(model, X_pca, y_encoded, cv=cv, scoring='accuracy'))\n",
    "    scores.append(score)\n",
    "    print(f\"Accuracy with PCA ({n} components): {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e64713",
   "metadata": {
    "id": "57e64713"
   },
   "source": [
    "### Results Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221ee017",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "id": "221ee017",
    "outputId": "ce9cc2c7-318e-44d3-ae9f-2c7d9e42b88b"
   },
   "outputs": [],
   "source": [
    "# Plotting the results\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(components, scores, marker='o', label='With PCA')\n",
    "plt.axhline(y=mean_score_no_pca, color='r', linestyle='--', label='Without PCA')\n",
    "plt.title('Model Accuracy vs. Number of PCA Components')\n",
    "plt.xlabel('Number of Principal Components')\n",
    "plt.ylabel('Cross-validation Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de2d201",
   "metadata": {
    "id": "0de2d201"
   },
   "source": [
    "### Results and Analysis\n",
    "\n",
    "- **Cross-validation accuracy without PCA**: `0.8629`\n",
    "\n",
    "- **Cross-validation accuracies with PCA**: `0.9029`\n",
    "\n",
    "  <table>\n",
    "  <tr>\n",
    "    <th>Number of Components</th>\n",
    "    <th>Accuracy</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>5</td>\n",
    "    <td>0.9314</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>10</td>\n",
    "    <td>0.8905</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>20</td>\n",
    "    <td>0.9029</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>50</td>\n",
    "    <td>0.8752</td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "**Plot**:\n",
    "\n",
    "- The plot shows the model accuracy vs. the number of principal components used.\n",
    "\n",
    "**Interpretation**:\n",
    "\n",
    "- Without PCA, the model may overfit due to the high dimensionality and small sample size.\n",
    "- With PCA, reducing the dimensionality helps prevent overfitting and may improve model performance.\n",
    "- The optimal number of principal components balances between retaining enough variance and reducing dimensionality.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "- PCA can effectively reduce overfitting in high-dimensional datasets like the Leukemia Gene Expression Dataset.\n",
    "- Selecting the appropriate number of principal components is crucial for optimal performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "other-methods",
   "metadata": {
    "id": "other-methods"
   },
   "source": [
    "## 5. Other Dimensionality Reduction Methods\n",
    "\n",
    "### t-SNE and UMAP\n",
    "\n",
    "While PCA is a powerful linear dimensionality reduction technique, it may not capture complex non-linear relationships in the data. To address this, we can use non-linear dimensionality reduction methods like **t-SNE** and **UMAP**.\n",
    "\n",
    "- **t-SNE (t-Distributed Stochastic Neighbor Embedding)**:\n",
    "  - Focuses on preserving local relationships (neighboring points) in high-dimensional data when mapped to lower dimensions.\n",
    "  - Useful for visualizing clusters in data.\n",
    "  - Computationally intensive for large datasets.\n",
    "\n",
    "- **UMAP (Uniform Manifold Approximation and Projection)**:\n",
    "  - Preserves both local and some global structure of data.\n",
    "  - Generally faster than t-SNE and scales better to large datasets.\n",
    "  - Based on manifold learning techniques and topological data analysis.\n",
    "\n",
    "In this section, we'll apply PCA, t-SNE, and UMAP to the MNIST dataset and visualize the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "WwgOTWpnuhjQ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WwgOTWpnuhjQ",
    "outputId": "63cc89f3-cc24-4af8-d4c9-df725197dada"
   },
   "outputs": [],
   "source": [
    "!pip install umap-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be5d7a4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8be5d7a4",
    "outputId": "313daf13-7f23-48dc-a5e0-3e968b2a9f3f"
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import umap.umap_ as umap\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load MNIST dataset\n",
    "mnist = fetch_openml('mnist_784', version=1)\n",
    "X, y = mnist[\"data\"], mnist[\"target\"]\n",
    "\n",
    "# Convert labels to integers\n",
    "y = y.astype(int)\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Number of samples you want in total\n",
    "sample_size = 2000\n",
    "\n",
    "# Number of unique labels/classes\n",
    "num_classes = len(np.unique(y))\n",
    "\n",
    "# Number of samples per class\n",
    "samples_per_class = sample_size // num_classes\n",
    "\n",
    "# Ensure sample_size is divisible by num_classes\n",
    "sample_size = samples_per_class * num_classes\n",
    "\n",
    "print(f\"Total sample size adjusted to {sample_size} to evenly distribute among classes.\")\n",
    "\n",
    "# Initialize lists to collect indices\n",
    "indices_list = []\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# For each class, randomly select samples_per_class indices\n",
    "for class_label in np.unique(y):\n",
    "    # Find indices of all samples with the current class label\n",
    "    class_indices = np.where(y == class_label)[0]\n",
    "    # Randomly select samples_per_class indices from the class_indices\n",
    "    selected_indices = np.random.choice(class_indices, size=samples_per_class, replace=False)\n",
    "    # Add the selected indices to the list\n",
    "    indices_list.extend(selected_indices)\n",
    "\n",
    "# Convert list of indices to a numpy array\n",
    "indices = np.array(indices_list)\n",
    "\n",
    "# Shuffle the indices to mix the classes\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "# Get the sampled data and labels\n",
    "X_sample = X_scaled[indices]\n",
    "y_sample = y[indices]\n",
    "\n",
    "print(f\"Sampled data shape: {X_sample.shape}\")\n",
    "print(f\"Sampled labels distribution: {np.bincount(y_sample)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963d8a65",
   "metadata": {
    "id": "963d8a65"
   },
   "source": [
    "### PCA on MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b13b054",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "id": "5b13b054",
    "outputId": "eac1887d-78a5-4276-8bb5-47321b73e151"
   },
   "outputs": [],
   "source": [
    "# Apply PCA to reduce to 2 dimensions\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_sample)\n",
    "\n",
    "# Plot the PCA result\n",
    "plt.figure(figsize=(8, 6))\n",
    "scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y_sample, cmap='tab10', s=4, alpha=0.7)\n",
    "plt.title('PCA on MNIST')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.colorbar(scatter, ticks=range(10))\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca43ad0c",
   "metadata": {
    "id": "ca43ad0c"
   },
   "source": [
    "### t-SNE on MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4c2aa9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "id": "2b4c2aa9",
    "outputId": "2ceccc01-6f16-494c-e700-3c0f88ecc327"
   },
   "outputs": [],
   "source": [
    "# Apply t-SNE to reduce to 2 dimensions\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=30, n_iter=1000)\n",
    "X_tsne = tsne.fit_transform(X_sample)\n",
    "\n",
    "# Plot the t-SNE result\n",
    "plt.figure(figsize=(8, 6))\n",
    "scatter = plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=y_sample, cmap='tab10', s=4, alpha=0.7)\n",
    "plt.title('t-SNE on MNIST')\n",
    "plt.xlabel('t-SNE Component 1')\n",
    "plt.ylabel('t-SNE Component 2')\n",
    "plt.colorbar(scatter, ticks=range(10))\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a913916",
   "metadata": {
    "id": "0a913916"
   },
   "source": [
    "### UMAP on MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf63f43e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "id": "cf63f43e",
    "outputId": "c8de142d-f49f-45f2-9c5d-dd6370cb8d6b"
   },
   "outputs": [],
   "source": [
    "# Apply UMAP to reduce to 2 dimensions\n",
    "umap_reducer = umap.UMAP(n_components=2, random_state=42)\n",
    "X_umap = umap_reducer.fit_transform(X_sample)\n",
    "\n",
    "# Plot the UMAP result\n",
    "plt.figure(figsize=(8, 6))\n",
    "scatter = plt.scatter(X_umap[:, 0], X_umap[:, 1], c=y_sample, cmap='tab10', s=4, alpha=0.7)\n",
    "plt.title('UMAP on MNIST')\n",
    "plt.xlabel('UMAP Component 1')\n",
    "plt.ylabel('UMAP Component 2')\n",
    "plt.colorbar(scatter, ticks=range(10))\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e765547",
   "metadata": {
    "id": "8e765547"
   },
   "source": [
    "### Comparison and Discussion\n",
    "\n",
    "- **PCA**:\n",
    "  - Linear method; captures the directions of maximum variance.\n",
    "  - The plot may show overlapping clusters due to its linear nature.\n",
    "\n",
    "- **t-SNE**:\n",
    "  - Non-linear method focusing on preserving local structure.\n",
    "  - Often shows well-separated clusters, making it useful for visualization.\n",
    "  - Computationally intensive; slower on large datasets.\n",
    "\n",
    "- **UMAP**:\n",
    "  - Non-linear method that preserves both local and some global structure.\n",
    "  - Generally faster than t-SNE and scales better to large datasets.\n",
    "  - Can capture more complex structures in the data.\n",
    "\n",
    "From the visualizations, we can observe how each method reduces the dimensionality of the MNIST dataset and the extent to which they separate the different digit classes."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

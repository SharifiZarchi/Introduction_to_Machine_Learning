{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hWmyKlSBjVqc"
   },
   "source": [
    "<br>\n",
    "<font>\n",
    "<!-- <img src=\"https://cdn.freebiesupply.com/logos/large/2x/sharif-logo-png-transparent.png\" alt=\"SUT logo\" width=300 height=300 align=left class=\"saturate\"> -->\n",
    "<div dir=ltr align=center>\n",
    "<img src=\"https://cdn.freebiesupply.com/logos/large/2x/sharif-logo-png-transparent.png\" width=200 height=200>\n",
    "<br>\n",
    "<font color=0F5298 size=8>\n",
    "Introduction to Machine Learning <br>\n",
    "<font color=696880 size=5>\n",
    "<!-- <br> -->\n",
    "Computer Engineering Department\n",
    "<br>\n",
    "Sharif University of Technology\n",
    "\n",
    "<font color=696880 size=5>\n",
    "<br>\n",
    "CE 40477 - Fall 2024\n",
    "\n",
    "<font color=GREEN size=5>\n",
    "<br>\n",
    "Mahan Bayhaghi & Arshia Gharooni\n",
    "<!-- <br> -->\n",
    "\n",
    "____\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iucKtjV-JUQb"
   },
   "source": [
    "# Regression\n",
    "In this section, we will try to solve the problem of **Regression**. In our first step, we will atack the problem from the analytical view."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Ck3wWUWTADI"
   },
   "source": [
    "### Generate Synthetic Data\n",
    "\n",
    "We will use rather simple line of $ y = 3x + 8 $ with noise of $ \\epsilon = 5 $ to generate test and train data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "94ofY7fWD1Ng"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o5hs1H0zEG6s"
   },
   "outputs": [],
   "source": [
    "def generate_data(n=50, noise=5.0):\n",
    "    np.random.seed(42)\n",
    "    X = np.linspace(-10, 10, n)\n",
    "    # Ground truth line: y = 3x + 8\n",
    "    true_slope = 3\n",
    "    true_intercept = 8\n",
    "    noise = np.random.randn(n) * noise\n",
    "    y = true_slope * X + true_intercept + noise\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 492
    },
    "id": "F95y79nnEJBU",
    "outputId": "ca2505cc-a385-4f39-f85c-515526733ea0"
   },
   "outputs": [],
   "source": [
    "X, y = generate_data(n=50, noise=5.0)\n",
    "plt.scatter(X, y, color='blue', label='Data Points')\n",
    "plt.title(\"Generated Data (Univariate)\")\n",
    "plt.xlabel(\"$x$\")\n",
    "plt.ylabel(\"$y$\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Ip2SVWzJoBk"
   },
   "source": [
    "## Linear Regression: Analytical Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8FOoAeLWT38E"
   },
   "source": [
    "### Implement the Closed-Form Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gJ7Ns6Z_EMh8",
    "outputId": "64fceebb-7888-4cf3-e003-2377b3a23619"
   },
   "outputs": [],
   "source": [
    "# Hypothesis: h_w(x) = w_0 + w_1 * x_1\n",
    "def h_w(x, w):\n",
    "    return w[0] + w[1] * x  # equivalent to w_0 + w_1 * x\n",
    "\n",
    "# Linear Regression using closed-form solution\n",
    "def linear_regression_closed_form(X, y):\n",
    "    # Adding bias term (x_0 = 1) to input vector X\n",
    "    X_b = np.c_[np.ones((len(X), 1)), X]  # X_b is now the full input vector with bias term\n",
    "    # Closed-form solution: w = (X^T * X)^-1 * X^T * y\n",
    "    w = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)\n",
    "    return w\n",
    "\n",
    "# Get parameter vector w\n",
    "w = linear_regression_closed_form(X, y)\n",
    "print(f\"Parameters (w): \")\n",
    "print(f\"w_1 = {w[1]:.2f}, w_0 = {w[0]:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 492
    },
    "id": "5jPFrAPvEVC4",
    "outputId": "58f9e5df-5fc7-4f7a-f59b-5563bb0ca719"
   },
   "outputs": [],
   "source": [
    "y_pred = h_w(X, w)\n",
    "\n",
    "plt.scatter(X, y, color='blue', label='Actual Data')\n",
    "plt.plot(X, y_pred, color='red', label='Prediction (Closed Form)')\n",
    "plt.title(\"Linear Regression - Closed Form Solution\")\n",
    "plt.xlabel(\"$x$\")\n",
    "plt.ylabel(\"$y$\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mlZPhd-BUAyj"
   },
   "source": [
    "## Polynomial Regression: Analytical Solution\n",
    "Linear regression can be extended to model nonlinear relationships by introducing polynomial terms.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jHboZxPvK7bp"
   },
   "source": [
    "### Engineering Polynomial Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_2Q55cbtEYmI"
   },
   "outputs": [],
   "source": [
    "# Function to generate polynomial features (input matrix X')\n",
    "def polynomial_features(X, degree):\n",
    "    X_poly = np.c_[np.ones(len(X))]\n",
    "    for i in range(1, degree + 1):\n",
    "        X_poly = np.c_[X_poly, X**i]\n",
    "    return X_poly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wxZQ0aMHUNrP"
   },
   "source": [
    "### Implement Polynomial Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6nF0bx7pEmF8",
    "outputId": "18e8b16c-07ec-448d-bd31-61e787b48b64"
   },
   "outputs": [],
   "source": [
    "def polynomial_regression(X, y, degree):\n",
    "    X_poly = polynomial_features(X, degree)\n",
    "    # Closed-form solution: w = (X'^T * X')^-1 * X'^T * y\n",
    "    w = np.linalg.inv(X_poly.T.dot(X_poly)).dot(X_poly.T).dot(y)\n",
    "    return w\n",
    "\n",
    "m = 5  # Degree of the polynomial regression\n",
    "w_poly = polynomial_regression(X, y, m)  # Parameter vector w\n",
    "\n",
    "print(f\"Parameters (w) for Degree {m}: {w_poly}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rMEmdZGHUQVa"
   },
   "source": [
    "### Visualize the Polynomial Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 492
    },
    "id": "-x2FvSOMEnaJ",
    "outputId": "3faeb3c7-3106-4797-8c7b-b922aa2465bf"
   },
   "outputs": [],
   "source": [
    "X_fit = np.linspace(X.min(), X.max(), 200)\n",
    "X_fit_poly = polynomial_features(X_fit, m)\n",
    "y_poly_pred = X_fit_poly.dot(w_poly)  # h_w(x) = X' * w\n",
    "\n",
    "# Plot the actual data and the polynomial fit\n",
    "plt.scatter(X, y, color='blue', label='Actual Data')\n",
    "plt.plot(X_fit, y_poly_pred, color='green', label=f'Polynomial Fit (Degree {m})')\n",
    "plt.title(f\"Polynomial Regression (Degree {m})\")\n",
    "plt.xlabel(\"$x$\")\n",
    "plt.ylabel(\"$y$\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uZUPXpRHUX80"
   },
   "source": [
    "## Visualizing $E_{rms}$\n",
    "The Root Mean Square Error (RMSE) helps us understand how well our model fits the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "heHlHqROErmb"
   },
   "outputs": [],
   "source": [
    "def compute_rms_error(y_true, y_pred):\n",
    "    return np.sqrt(np.mean((y_true - y_pred) ** 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xh17mgxlU09t"
   },
   "source": [
    "### Visualizing RMSE for different Polynomial degrees\n",
    "We could visualize the $ E_{rms} $ better if we split generated data into train and test datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "4jNwnTpCEtRh",
    "outputId": "8c55e7fc-1f9d-45bb-a327-775d565e9ec7"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into 80% training and 20% testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "degrees = range(0, 9)\n",
    "train_rms_errors = []\n",
    "test_rms_errors = []\n",
    "\n",
    "\n",
    "for d in degrees:\n",
    "    # Train the model on the training set\n",
    "    w_poly = polynomial_regression(X_train, y_train, d)\n",
    "\n",
    "    # Compute predictions for the training set\n",
    "    X_train_poly = polynomial_features(X_train, d)\n",
    "    y_train_pred = X_train_poly.dot(w_poly)\n",
    "\n",
    "    # Compute predictions for the test set\n",
    "    X_test_poly = polynomial_features(X_test, d)\n",
    "    y_test_pred = X_test_poly.dot(w_poly)\n",
    "\n",
    "    # Calculate RMSE for both training and test sets\n",
    "    train_rms_error = compute_rms_error(y_train, y_train_pred)\n",
    "    test_rms_error = compute_rms_error(y_test, y_test_pred)\n",
    "\n",
    "    # Store the errors\n",
    "    train_rms_errors.append(train_rms_error)\n",
    "    test_rms_errors.append(test_rms_error)\n",
    "\n",
    "    # Print the RMSE for the current degree\n",
    "    print(f\"Degree {d}: Train RMSE = {train_rms_error:.2f}, Test RMSE = {test_rms_error:.2f}\")\n",
    "\n",
    "    # Plot the polynomial fit on the training data\n",
    "    plt.scatter(X_train, y_train, color='blue', label=\"Training Data\")\n",
    "    plt.scatter(X_test, y_test, color='red', label=\"Test Data\", alpha=0.6)\n",
    "    X_fit = np.linspace(X.min(), X.max(), 200)\n",
    "    X_fit_poly = polynomial_features(X_fit, d)\n",
    "    y_fit_pred = X_fit_poly.dot(w_poly)\n",
    "    plt.plot(X_fit, y_fit_pred, label=f\"Degree {d} Fit\", color='green')\n",
    "    plt.title(f\"Polynomial Regression (Degree {d})\")\n",
    "    plt.xlabel(\"$x$\")\n",
    "    plt.ylabel(\"$y$\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Plot RMSE for training and test sets\n",
    "plt.plot(degrees, train_rms_errors, marker='o', linestyle='-', color='blue', label='Train RMSE')\n",
    "plt.plot(degrees, test_rms_errors, marker='o', linestyle='-', color='red', label='Test RMSE')\n",
    "plt.title(\"Train vs Test RMSE vs Polynomial Degree\")\n",
    "plt.xlabel(\"Polynomial Degree\")\n",
    "plt.ylabel(\"RMSE\")\n",
    "plt.xticks(degrees)\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kqwHXRLOVTnT"
   },
   "source": [
    "As you can see from the RMSE plot above, by increasing the degree polynomial, the training error never increases **(Why ?)**. However; testing error can increase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-WQ6j36_Y2cS"
   },
   "source": [
    "## Gradient Descent:\n",
    "In this section, we will use the popular iterative method called **Gradient Descent** to solve the regression problem.\n",
    "\n",
    "Assuming we need to find $ w_0\\ and\\ w_1 $ in the problem of linear regression, update rule using gradinet descent will be:\n",
    "$$\n",
    "\\begin{array}{l}\n",
    "w_0 \\leftarrow w_0 - \\eta \\frac{\\partial J}{\\partial w_0} = w_0 - \\eta \\sum_{i=1}^{m} (h_w(x^{(i)}) - y^{(i)}) \\\\\n",
    "w_1 \\leftarrow w_1 - \\eta \\frac{\\partial J}{\\partial w_1} = w_1 - \\eta \\sum_{i=1}^{m} (h_w(x^{(i)}) - y^{(i)}) (x^{(i)}) \\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "In which $ \\eta $ is the learning rate. To overcome the problem of overflow, assume cost function is $ J(\\mathbf{w}) = \\frac{SSE}{training \\ \\ size} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y9VT8Smtalqs"
   },
   "source": [
    "### Implementing GD for linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 492
    },
    "id": "A9mZQFSXaaBd",
    "outputId": "106f4d48-a334-461d-8c34-7dfdf72bf7d6"
   },
   "outputs": [],
   "source": [
    "# SSE cost function\n",
    "def cost_function(X, y, w):\n",
    "    return np.sum((h_w(X, w) - y)**2) / len(X)\n",
    "\n",
    "# Gradient descent\n",
    "def gradient_descent(X, y, w, alpha, num_iters):\n",
    "    m = len(X)\n",
    "    cost_history = []\n",
    "    w_history = [w.copy()]\n",
    "\n",
    "    for i in range(num_iters):\n",
    "        # updates\n",
    "        gradient_w0 = np.sum(h_w(X, w) - y) / m\n",
    "        gradient_w1 = np.sum((h_w(X, w) - y) * X) / m\n",
    "        w[0] -= alpha * gradient_w0\n",
    "        w[1] -= alpha * gradient_w1\n",
    "\n",
    "        cost_history.append(cost_function(X, y, w))\n",
    "        w_history.append(w.copy())  # Store a copy of w, not the reference\n",
    "\n",
    "    return w, cost_history, w_history\n",
    "\n",
    "X, y = generate_data(n=50, noise=5.0)\n",
    "w_initial = [0, 0]  # Start with w0 = 0, w1 = 0\n",
    "eta = 0.05  # Learning rate\n",
    "num_iters = 500\n",
    "\n",
    "# Run Gradient Descent\n",
    "w_final, cost_history, w_history = gradient_descent(X, y, w_initial, eta, num_iters)\n",
    "\n",
    "# Visualize cost function (log of J(w))\n",
    "w0_vals = np.linspace(-10, 20, 100)\n",
    "w1_vals = np.linspace(-1, 5, 100)\n",
    "J_vals = np.zeros((len(w0_vals), len(w1_vals)))\n",
    "\n",
    "for i in range(len(w0_vals)):\n",
    "    for j in range(len(w1_vals)):\n",
    "        w = [w0_vals[i], w1_vals[j]]\n",
    "        J_vals[i, j] = cost_function(X, y, w)\n",
    "\n",
    "# Plot GD Progression (without labels for lines, different alphas)\n",
    "plt.scatter(X, y, color='blue', label='Actual Data')\n",
    "\n",
    "# Plot lines for every 50th step with increasing alpha\n",
    "for idx, w in enumerate(w_history[::num_iters // 100]):\n",
    "    alpha = 0.15 + 0.85*(idx) / 100  # Gradually increase alpha for each line\n",
    "    plt.plot(X, h_w(X, w), color='red', alpha=alpha)\n",
    "\n",
    "# Final line in bold\n",
    "plt.plot(X, h_w(X, w_final), color='red', lw=2, label='Final Line')\n",
    "\n",
    "plt.title(\"GD Progression - Linear Regression\")\n",
    "plt.xlabel(\"$x$\")\n",
    "plt.ylabel(\"$y$\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2YuQ-dfdeCuI"
   },
   "source": [
    "As you can see from the lines above, we first started by a neutral hypothesis which was a simple line $ y = 0 $. The update at each iteration tries to minimize cost function thus improving weights. As you can see, the final line after 500 iterations is the line best describing datapoints. **But how can we be sure that cost function is optimizable ?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Vv5zacveny5"
   },
   "source": [
    "### Plotting cost function\n",
    "To get a better sense of SSE cost function, let's visualize it for univariate linear regression discussed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 618
    },
    "id": "d0Ha9m2HdfZb",
    "outputId": "f62d8c1a-f67e-4a10-88df-d5556125f3a1"
   },
   "outputs": [],
   "source": [
    "# 3D Plot of J(w)\n",
    "fig = plt.figure(figsize=(10, 7))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "W0, W1 = np.meshgrid(w0_vals, w1_vals)\n",
    "ax.plot_surface(W0, W1, J_vals.T, cmap='viridis')\n",
    "ax.set_xlabel('w0')\n",
    "ax.set_ylabel('w1')\n",
    "ax.set_zlabel('log(J(w))')\n",
    "plt.title(\"Cost Function Surface\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TTo1t_rzxBB6"
   },
   "source": [
    "The plot above can be somewhat confusing. Let us visualize $ log J(\\mathbf{w}) $ instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 618
    },
    "id": "barPTPqpxKpS",
    "outputId": "f7a7a330-a9e3-4796-aa48-9c5f6695f928"
   },
   "outputs": [],
   "source": [
    "# 3D Plot of log J(w)\n",
    "fig = plt.figure(figsize=(10, 7))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "W0, W1 = np.meshgrid(w0_vals, w1_vals)\n",
    "ax.plot_surface(W0, W1, np.log(J_vals.T), cmap='viridis')\n",
    "ax.set_xlabel('w0')\n",
    "ax.set_ylabel('w1')\n",
    "ax.set_zlabel('log(J(w))')\n",
    "plt.title(\"Cost Function Surface (Log Scale)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-A384rwRxUDv"
   },
   "source": [
    "Gradient descent tries to reach minimum point of the plot above in each step. But are we actually reaching our goal ? Let's visualize it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 618
    },
    "id": "LwtmhsqByRbr",
    "outputId": "c310a2c8-f5d7-466a-d98f-cebd7d0c3d56"
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "fig = plt.figure(figsize=(10, 7))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "W0, W1 = np.meshgrid(w0_vals, w1_vals)\n",
    "ax.plot_surface(W0, W1, np.log(J_vals.T), cmap='viridis', alpha=0.25)\n",
    "ax.set_xlabel('w0')\n",
    "ax.set_ylabel('w1')\n",
    "ax.set_zlabel('log(J(w))')\n",
    "plt.title(\"Cost Function Surface (Log Scale)\")\n",
    "\n",
    "# Plot the points on the 3D surface for each GD iteration\n",
    "w_history_array = np.array(w_history)  # Convert list to array for easier slicing\n",
    "w0_history = w_history_array[:, 0]\n",
    "w1_history = w_history_array[:, 1]\n",
    "cost_history_log = np.log(np.array(cost_history))  # Log of the cost history\n",
    "\n",
    "# Plot the path of gradient descent in 3D\n",
    "ax.plot(w0_history[:num_iters], w1_history[:num_iters], cost_history_log, marker='o', color='r', label='GD Path', markersize=3)\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9arBxt2qy7OT"
   },
   "source": [
    "As shown in the plot above, GD is trying to reach optimal point at each step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YzzhUJuJzWkC"
   },
   "source": [
    "### Effect of learning rate ($ \\eta $)\n",
    "Choosing $ \\eta $ could be tricky. Large learning rates can lead to divergence. Small learning rates on the other hand could slow down the convergence by requiring more iterations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "yENS5XjH0J7r",
    "outputId": "62ab5a36-61fa-4a71-ca15-0a1ca53cb67c"
   },
   "outputs": [],
   "source": [
    "learning_rates = [0.1, 0.02, 0.001]\n",
    "num_iters = 100\n",
    "w_initial = [0, 0]\n",
    "\n",
    "colors = ['purple', 'green', 'orange']\n",
    "\n",
    "w0_vals = np.linspace(-10, 20, 100)\n",
    "w1_vals = np.linspace(-1, 5, 100)\n",
    "J_vals = np.zeros((len(w0_vals), len(w1_vals)))\n",
    "\n",
    "for i in range(len(w0_vals)):\n",
    "    for j in range(len(w1_vals)):\n",
    "        w = [w0_vals[i], w1_vals[j]]\n",
    "        J_vals[i, j] = cost_function(X, y, w)\n",
    "\n",
    "cost_histories = []\n",
    "\n",
    "# GD for each eta\n",
    "for idx, eta in enumerate(learning_rates):\n",
    "    w_final, cost_history, w_history = gradient_descent(X, y, w_initial.copy(), eta, num_iters)\n",
    "    cost_histories.append(cost_history)\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for step_idx, w in enumerate(w_history[::num_iters // 100]):\n",
    "        alpha_val = 0.15 + 0.85*(idx) / 100\n",
    "        plt.plot(X, h_w(X, w), color=colors[idx], alpha=alpha_val)\n",
    "\n",
    "    plt.plot(X, h_w(X, w_final), lw=2, label=f'Final Line (eta={eta})', color=colors[idx])\n",
    "    plt.title(f\"Lines during Gradient Descent (Learning Rate {eta})\")\n",
    "    plt.xlabel(\"$x$\")\n",
    "    plt.ylabel(\"$y$\")\n",
    "    plt.legend()\n",
    "    plt.scatter(X, y, color='blue', label='Actual Data')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rc90d9vW7qPX"
   },
   "source": [
    "As shown in the plots above, choosing a large learning rate leads to divergence. In this example, the update rule keeps making weights larger and larger and the weights will never converge. Choosing a small learning rate on the other hand, leads to slow convergence. In this example, learning $ w_0 $ is happening at a slow time because the update rule is being changed almost minimially !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 492
    },
    "id": "5PMs-qu_86WB",
    "outputId": "fd0b14e1-cbbf-4639-912f-e7379950e57d"
   },
   "outputs": [],
   "source": [
    "plt.title(\"Cost Function (log scale) over Iterations for Different Learning Rates\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"log(J(w))\")\n",
    "for idx in range(len(cost_histories)):\n",
    "  plt.plot(np.log(cost_histories[idx]), label=f'eta={learning_rates[idx]}', color=colors[idx])\n",
    "plt.ylim(bottom=2, top=10)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QNHOkmgtI-pt"
   },
   "source": [
    "## Regularization: Ridge and Lasso Regression\n",
    "In this section, we will try to visualize the effect of regularization using **L1 norm (Lasso regression)** and **L2 norm (Ridge regression)**. Let us have a small number of datapoints and try to fit a complex model to it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jUOv5aW-Jg-i"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "def generate_data(n=100, noise=10.0):\n",
    "    np.random.seed(42)\n",
    "    X = np.random.uniform(-10, 10, n)\n",
    "    y = X**2 - 2 * X + np.random.randn(n) * noise  # x**2 - 2*x + noise\n",
    "    return X, y\n",
    "\n",
    "X, y = generate_data(n=15)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "emmh2wbz-gdJ",
    "outputId": "62b4863f-6c25-408a-c50d-4a21989fcf21"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "\n",
    "degrees = [2, 6, 8]\n",
    "lambdas = [1e4, 1, 1e-4, 1e-8]\n",
    "\n",
    "ridge_rmse_train = np.zeros((len(degrees), len(lambdas)))\n",
    "ridge_rmse_test = np.zeros((len(degrees), len(lambdas)))\n",
    "lasso_rmse_train = np.zeros((len(degrees), len(lambdas)))\n",
    "lasso_rmse_test = np.zeros((len(degrees), len(lambdas)))\n",
    "\n",
    "for degree_idx, degree in enumerate(degrees):\n",
    "    fig, axs = plt.subplots(1, 4, figsize=(20, 5))  # Create a 1x4 grid of subplots\n",
    "    for lambda_idx, lambda_val in enumerate(lambdas):\n",
    "        poly_features = PolynomialFeatures(degree=degree)\n",
    "        X_train_poly = poly_features.fit_transform(X_train[:, np.newaxis])\n",
    "        X_test_poly = poly_features.transform(X_test[:, np.newaxis])\n",
    "\n",
    "        # Ridge Regression using scikit-learn\n",
    "        ridge_model = Ridge(alpha=lambda_val)\n",
    "        ridge_model.fit(X_train_poly, y_train)\n",
    "        y_train_pred_ridge = ridge_model.predict(X_train_poly)\n",
    "        y_test_pred_ridge = ridge_model.predict(X_test_poly)\n",
    "\n",
    "        # Lasso Regression using scikit-learn\n",
    "        lasso_model = Lasso(alpha=lambda_val, max_iter=10000)\n",
    "        lasso_model.fit(X_train_poly, y_train)\n",
    "        y_train_pred_lasso = lasso_model.predict(X_train_poly)\n",
    "        y_test_pred_lasso = lasso_model.predict(X_test_poly)\n",
    "\n",
    "        ridge_rmse_train[degree_idx, lambda_idx] = compute_rms_error(y_train, y_train_pred_ridge)\n",
    "        ridge_rmse_test[degree_idx, lambda_idx] = compute_rms_error(y_test, y_test_pred_ridge)\n",
    "        lasso_rmse_train[degree_idx, lambda_idx] = compute_rms_error(y_train, y_train_pred_lasso)\n",
    "        lasso_rmse_test[degree_idx, lambda_idx] = compute_rms_error(y_test, y_test_pred_lasso)\n",
    "\n",
    "        # Plot the fitted curves for both Ridge and Lasso\n",
    "        X_plot = np.linspace(-10, 10, 100)\n",
    "        X_plot_poly = poly_features.transform(X_plot[:, np.newaxis])\n",
    "\n",
    "        y_plot_ridge = ridge_model.predict(X_plot_poly)\n",
    "        y_plot_lasso = lasso_model.predict(X_plot_poly)\n",
    "\n",
    "        ax = axs[lambda_idx]\n",
    "        ax.scatter(X_train, y_train, color='blue', label='Train Data')\n",
    "        ax.scatter(X_test, y_test, color='green', label='Test Data')\n",
    "        ax.plot(X_plot, y_plot_ridge, color='red', label=f'Ridge (λ={lambda_val})')\n",
    "        ax.plot(X_plot, y_plot_lasso, color='orange', linestyle='--', label=f'Lasso (λ={lambda_val})')\n",
    "        ax.set_title(f'Polynomial Degree {degree} - λ={lambda_val}')\n",
    "        ax.set_xlabel('X')\n",
    "        ax.set_ylabel('y')\n",
    "        ax.legend()\n",
    "\n",
    "    plt.suptitle(f'Polynomial Degree {degree} - Regularization Comparison')\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "UkDo0fRxKida",
    "outputId": "80e675bc-a4ec-45d2-ca50-2caf1f41de82"
   },
   "outputs": [],
   "source": [
    "# plot RMSE\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "for degree_idx, degree in enumerate(degrees):\n",
    "    plt.plot(lambdas, ridge_rmse_test[degree_idx], marker='x', label=f'Ridge - Degree {degree}')\n",
    "plt.xscale('log')\n",
    "plt.xlabel('Regularization Parameter (λ)')\n",
    "plt.ylabel('RMSE')\n",
    "plt.title('RMSE for Ridge at Different Polynomial Degrees')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "for degree_idx, degree in enumerate(degrees):\n",
    "  plt.plot(lambdas, lasso_rmse_test[degree_idx], marker='x', label=f'Lasso - Degree {degree}')\n",
    "\n",
    "plt.xscale('log')\n",
    "plt.xlabel('Regularization Parameter (λ)')\n",
    "plt.ylabel('RMSE')\n",
    "plt.title('RMSE for Lasso at Different Polynomial Degrees')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tMkQJeyKI0Gq"
   },
   "source": [
    "# Real-World Example 1: California House Price Prediction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5xmDnCi5T4Qu"
   },
   "source": [
    "## Dataset\n",
    "We'll use the California Housing Dataset available in scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wpXdXvlhI1Va",
    "outputId": "0c96060f-b456-41f3-a0cb-85d2c8f72237"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "X, y = housing.data, housing.target\n",
    "\n",
    "feature_names = housing.feature_names\n",
    "print(\"Feature names:\", feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "laP4V9RyXoP3"
   },
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JD03BaDdJDSJ"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iQj4xj_bXrt2"
   },
   "source": [
    "## Implement Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zUabEaciJGMZ",
    "outputId": "61bc2cab-4246-4c9f-8d5e-f8d90694d2cc"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(f\"Test RMSE: {rmse:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XtD8-Os7Xx_I"
   },
   "source": [
    "## Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tOIwdqQ9JI7p",
    "outputId": "f675e03d-63e8-4ab3-965b-07ca2456f5b8"
   },
   "outputs": [],
   "source": [
    "coefficients = model.coef_\n",
    "for name, coef in zip(feature_names, coefficients):\n",
    "    print(f\"{name}: {coef:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SwuIdGmnXt3x"
   },
   "source": [
    "## Visualize Actual vs Predicted Prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "id": "I-ootgVLJKMK",
    "outputId": "b4b7cdce-0ec5-4616-d98a-ab90feafcc8c"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(y_test, y_pred, alpha=0.5)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2)\n",
    "plt.xlabel(\"Actual Prices\")\n",
    "plt.ylabel(\"Predicted Prices\")\n",
    "plt.title(\"Actual vs Predicted House Prices\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i-g63Weeisqj"
   },
   "source": [
    "# Real-World Example 2: Tehran House Price Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B30LdQchH7FQ"
   },
   "source": [
    "Fot this section, we will use regression to predict hourse prices in different regions of Tehran. The [Dataset](https://www.kaggle.com/datasets/mokar2001/house-price-tehran-iran) used is records on Divar website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_oKsn_Pfe9qk"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 226
    },
    "id": "qOZjgUxjbi6z",
    "outputId": "c8a889c4-327e-4128-9faa-ba00129b7778"
   },
   "outputs": [],
   "source": [
    "file_path = './assets/housePrice.xlsx'\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jquORkhAehW1",
    "outputId": "bbdc6cc6-cd4e-4f01-bff9-99b856e36277"
   },
   "outputs": [],
   "source": [
    "print(df['Area'].describe())\n",
    "print(df[df['Area'] > 1e6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 320
    },
    "id": "Hh1fvdcEpFcP",
    "outputId": "4a48a760-a1e7-47a1-bb7d-da9b1b021792"
   },
   "outputs": [],
   "source": [
    "# Data cleaning - removing outliers based on IQR\n",
    "Q1_area = df['Area'].quantile(0.25)\n",
    "Q3_area = df['Area'].quantile(0.75)\n",
    "IQR_area = Q3_area - Q1_area\n",
    "\n",
    "Q1_price = df['PriceUSD'].quantile(0.25)\n",
    "Q3_price = df['PriceUSD'].quantile(0.75)\n",
    "IQR_price = Q3_price - Q1_price\n",
    "\n",
    "lower_bound_area = Q1_area - 1.5 * IQR_area\n",
    "upper_bound_area = Q3_area + 1.5 * IQR_area\n",
    "\n",
    "lower_bound_price = Q1_price - 1.5 * IQR_price\n",
    "upper_bound_price = Q3_price + 1.5 * IQR_price\n",
    "\n",
    "df_cleaned = df[(df['Area'] >= lower_bound_area) & (df['Area'] <= upper_bound_area) &\n",
    "                (df['PriceUSD'] >= lower_bound_price) & (df['PriceUSD'] <= upper_bound_price)]\n",
    "\n",
    "# Check the cleaned dataset\n",
    "df_cleaned.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 488
    },
    "id": "IYTsJY3npKRi",
    "outputId": "8ff6a2b5-0640-4ffa-c965-27d3819cb372"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(df_cleaned['Area'], bins=20, color='blue', alpha=0.7)\n",
    "plt.title('Distribution of Area')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(df_cleaned['PriceUSD'], bins=20, color='green', alpha=0.7)\n",
    "plt.title('Distribution of PriceUSD')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-Z2sD9bdbbqI"
   },
   "outputs": [],
   "source": [
    "# Function to compute the Root Mean Squared Error (RMSE)\n",
    "def compute_rms_error(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "# Function to create polynomial features\n",
    "def polynomial_features(X, degree):\n",
    "    poly = PolynomialFeatures(degree=degree)\n",
    "    return poly.fit_transform(X)\n",
    "\n",
    "# Function to perform polynomial regression\n",
    "def polynomial_regression(X, y, degree):\n",
    "    poly = PolynomialFeatures(degree=degree)\n",
    "    X_poly = poly.fit_transform(X)\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_poly, y)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "fJcLWxUqbdvG",
    "outputId": "30a8fbdc-d035-4f18-dd39-58d1cee18f9e"
   },
   "outputs": [],
   "source": [
    "# Group by 'Address' and perform analysis for each region\n",
    "addresses = df_cleaned['Address'].unique()\n",
    "\n",
    "for address in addresses:\n",
    "    df_address = df_cleaned[df_cleaned['Address'] == address]\n",
    "\n",
    "    # Skip if not enough data points\n",
    "    if len(df_address) < 2:\n",
    "        print(f\"Skipping address {address} due to insufficient samples.\")\n",
    "        continue\n",
    "\n",
    "    X = df_address[['Area']]\n",
    "    y = df_address['PriceUSD']\n",
    "\n",
    "    # Perform train-test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Skip if train or test sets are empty\n",
    "    if len(X_train) < 50 or len(X_test) < 25:\n",
    "        # print(f\"Skipping address {address} due to train-test split issues.\")\n",
    "        continue\n",
    "    else:\n",
    "      print(f\"Processing address: {address}\")\n",
    "\n",
    "    # Set polynomial degrees to evaluate\n",
    "    degrees = [2, 3, 5, 8, 10]\n",
    "\n",
    "    train_rms_errors = []\n",
    "    test_rms_errors = []\n",
    "\n",
    "    for degree in degrees:\n",
    "        model = polynomial_regression(X_train, y_train, degree)\n",
    "\n",
    "        X_train_poly = polynomial_features(X_train, degree)\n",
    "        y_train_pred = model.predict(X_train_poly)\n",
    "\n",
    "        X_test_poly = polynomial_features(X_test, degree)\n",
    "        y_test_pred = model.predict(X_test_poly)\n",
    "\n",
    "        train_rms_error = compute_rms_error(y_train, y_train_pred)\n",
    "        test_rms_error = compute_rms_error(y_test, y_test_pred)\n",
    "\n",
    "        train_rms_errors.append(train_rms_error)\n",
    "        test_rms_errors.append(test_rms_error)\n",
    "\n",
    "        # print(f\"Address: {address}, Degree {degree}: Train RMSE = {train_rms_error:.2f}, Test RMSE = {test_rms_error:.2f}\")\n",
    "\n",
    "        # Visualize Polynomial Regression for each degree\n",
    "    fig, axs = plt.subplots(1, len(degrees), figsize=(20, 5))  # Create a grid for subplots\n",
    "\n",
    "    for idx, degree in enumerate(degrees):\n",
    "        model = polynomial_regression(X_train, y_train, degree)\n",
    "\n",
    "        X_train_poly = polynomial_features(X_train, degree)\n",
    "        y_train_pred = model.predict(X_train_poly)\n",
    "\n",
    "        X_test_poly = polynomial_features(X_test, degree)\n",
    "        y_test_pred = model.predict(X_test_poly)\n",
    "\n",
    "        # Scatter plot of actual data and polynomial fit\n",
    "        axs[idx].scatter(X_train, y_train, color='blue', label=\"Training Data\")\n",
    "        axs[idx].scatter(X_test, y_test, color='red', label=\"Test Data\", alpha=0.6)\n",
    "        X_fit = np.linspace(X.min(), X.max(), 200).reshape(-1, 1)\n",
    "        X_fit_poly = polynomial_features(X_fit, degree)\n",
    "        y_fit_pred = model.predict(X_fit_poly)\n",
    "        axs[idx].plot(X_fit, y_fit_pred, label=f\"Degree {degree} Fit\", color='green')\n",
    "        axs[idx].set_title(f\"{address} - Degree {degree}\")\n",
    "        axs[idx].set_xlabel(\"Area\")\n",
    "        axs[idx].set_ylabel(\"PriceUSD\")\n",
    "        axs[idx].legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    # Plot Train RMSE and Test RMSE vs Polynomial Degree for each address\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(degrees, train_rms_errors, marker='o', label='Train RMSE', color='blue')\n",
    "    plt.plot(degrees, test_rms_errors, marker='o', label='Test RMSE', color='red')\n",
    "    plt.title(f\"RMSE vs Degree of Polynomial for {address}\")\n",
    "    plt.xlabel(\"Polynomial Degree\")\n",
    "    plt.ylabel(\"RMSE\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "tMkQJeyKI0Gq",
    "i-g63Weeisqj"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

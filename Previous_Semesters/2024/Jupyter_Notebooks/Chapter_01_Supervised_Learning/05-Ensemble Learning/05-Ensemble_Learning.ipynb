{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<font>\n",
    "<div dir=ltr align=center>\n",
    "<img src=\"https://cdn.freebiesupply.com/logos/large/2x/sharif-logo-png-transparent.png\" width=200 height=200>\n",
    "<br>\n",
    "<font color=0F5298 size=8>\n",
    "Introduction to Machine Learning <br>\n",
    "<font color=696880 size=5>\n",
    "<!-- <br> -->\n",
    "Computer Engineering Department\n",
    "<br>\n",
    "Sharif University of Technology\n",
    "\n",
    "<font color=696880 size=5>\n",
    "<br>\n",
    "CE 40477 - Fall 2024\n",
    "\n",
    "<font color=GREEN size=5>\n",
    "<br>\n",
    "Mahan Bayhaghi & Nikan Vasei\n",
    "<!-- <br> -->\n",
    "\n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Classifier\n",
    "\n",
    "### Quick Review\n",
    "- **Decision Tree** is a `supervised` machine learning model used for both classification and regression tasks.\n",
    "- It models decisions based on features by splitting the dataset into smaller subsets.\n",
    "- Each `node` in the tree represents a `feature`, and `edges` represent the `decision` based on a feature's value. \n",
    "- The goal is to create a model that predicts the value of a target variable by learning `decision rules` from the features.\n",
    "\n",
    "### How it works\n",
    "- A decision tree recursively splits the data into two or more homogeneous sets based on the `most significant attribute`. The splitting is guided by various metrics, like `Information Gain (IG)` or `Gini Impurity`.\n",
    "\n",
    "### Methods of Decision Trees\n",
    "1. **Information Gain (IG)**\n",
    "    - Information gain measures the reduction in entropy (uncertainty) after a dataset is split on a feature.\n",
    "    - Entropy is calculated as: \n",
    "    $$\n",
    "    Entropy(S) = - \\sum p(x)log_2p(x)\n",
    "    $$\n",
    "    - The higher the information gain, the better the feature is at splitting the data.\n",
    "\n",
    "2. **Gini Impurity**\n",
    "    - Measures the likelihood of an incorrect classification if a random sample is classified based on the distribution of labels.\n",
    "    - Gini impurity is calculated as:\n",
    "    $$\n",
    "    Gini(S) = 1 - \\sum p(x)^2\n",
    "    $$\n",
    "    - Lower Gini values indicate purer splits.\n",
    "\n",
    "### Steps to Create a Decision Tree\n",
    "1. **Select the Best Feature**: The dataset is split based on a feature that provides the best separation.\n",
    "2. **Calculate Entropy or Gini**: These metrics help determine how pure the subsets are.\n",
    "3. **Recursively Split**: The tree splits the dataset recursively until a stopping condition is met (like max depth or min samples).\n",
    "4. **Assign Labels**: Once the tree reaches the leaf nodes, it assigns a predicted class label.\n",
    "\n",
    "\n",
    "*Below we're going to use the IG method to implement a Decision Stump (Decision Tree with max_depth = 1) class from scratch.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionStump:\n",
    "    \"\"\"\n",
    "    A decision stump classifier for multi-class classification problems (depth = 1).\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.feature = None\n",
    "        self.threshold = None\n",
    "        self.value_left = None\n",
    "        self.value_right = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fits a decision stump to the dataset (X, y).\n",
    "        \"\"\"\n",
    "        best_gain = -1\n",
    "        for feature_index in range(X.shape[1]):\n",
    "            thresholds = np.unique(X[:, feature_index])\n",
    "            for threshold in thresholds:\n",
    "                left_mask = X[:, feature_index] <= threshold\n",
    "                right_mask = ~left_mask\n",
    "                left_y, right_y = y[left_mask], y[right_mask]\n",
    "                if len(left_y) and len(right_y):\n",
    "                    left_weight = len(left_y) / len(y)\n",
    "                    right_weight = 1 - left_weight\n",
    "                    gain = self._entropy(y) - (left_weight * self._entropy(left_y) + right_weight * self._entropy(right_y))\n",
    "                    if gain > best_gain:\n",
    "                        best_gain = gain\n",
    "                        self.feature = feature_index\n",
    "                        self.threshold = threshold\n",
    "                        self.value_left = np.bincount(left_y).argmax()\n",
    "                        self.value_right = np.bincount(right_y).argmax()\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predicts class labels for samples in X.\n",
    "        \"\"\"\n",
    "        return np.where(X[:, self.feature] <= self.threshold, self.value_left, self.value_right)\n",
    "\n",
    "    def _entropy(self, y):\n",
    "        \"\"\"\n",
    "        Computes entropy for a set of labels.\n",
    "        \"\"\"\n",
    "        proportions = np.bincount(y) / len(y)\n",
    "        return -np.sum([p * np.log2(p) for p in proportions if p > 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we'll use the `Iris plants` dataset which is a binary classification dataset.\n",
    "\n",
    "**Dataset Characteristics**:\n",
    "- Number of Instances: 150 (50 in each of three classes)\n",
    "- Number of Attributes: 4 numeric, predictive attributes and the class\n",
    "\n",
    "**Attribute Information**:\n",
    "- sepal length in cm\n",
    "- sepal width in cm\n",
    "- petal length in cm\n",
    "- petal width in cm\n",
    "- **class**:\n",
    "    - Iris-Setosa\n",
    "    - Iris-Versicolour\n",
    "    - Iris-Virginica\n",
    "\n",
    "We use the Iris dataset because it's simple, well-balanced, and requires minimal preprocessing. This allows us to focus on building and evaluating the decision tree model without the need for complex data preparation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris()\n",
    "\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "print(f\"Iris features: {iris.feature_names}\")\n",
    "print(f\"Iris target: {iris.target_names}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can split the dataset into train and test sets. \n",
    "\n",
    "Also ensure that `y` is reshaped to avoid dimensional errors during model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y.flatten(), test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Shape of X_train: {X_train.shape}, y_train: {y_train.shape}\")\n",
    "print(f\"Shape of X_test: {X_test.shape}, y_test: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can train our custom `DecisionTree()` model and evaluate its performance using both the `accuracy_score` and the `f1_score`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "stump = DecisionStump()\n",
    "stump.fit(X_train, y_train)\n",
    "\n",
    "stump_predictions = stump.predict(X_test)\n",
    "\n",
    "print(f\"Decision Stump Accuracy: {accuracy_score(y_test, stump_predictions):.3f}\")\n",
    "print(f\"Decision Stump F1-Score: {f1_score(y_test, stump_predictions, average='weighted'):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see how useful these extremely simple and fast classifiers can be."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Scikit-learn's Decision Tree Classifier\n",
    "After implementing a custom Decision Stump from scratch, we will now compare the results by utilizing the Scikit-learn library, which provides a highly optimized implementation of decision trees. This allows us to observe the behavior of decision trees without dealing with the complexities of manually building one. We’ll also visualize the decision tree to gain insight into how it makes decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "\n",
    "dt_sklearn = DecisionTreeClassifier(criterion='entropy', max_depth=2)\n",
    "dt_sklearn.fit(X_train, y_train)\n",
    "\n",
    "dt_skl_predictions = dt_sklearn.predict(X_test)\n",
    "\n",
    "print(f\"Sklearn DT Accuracy: {accuracy_score(y_test, dt_skl_predictions):.3f}\")\n",
    "print(f\"Sklearn DT F1-Score: {f1_score(y_test, dt_skl_predictions, average='weighted'):.3f}\")\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.title(\"Decision Tree Visualization - Sklearn\")\n",
    "plot_tree(dt_sklearn, feature_names=list(iris.feature_names), class_names=list(iris.target_names), filled=True, rounded=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the plot above, you can see the decision rules that have been made for splitting the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest\n",
    "\n",
    "### Quick Review\n",
    "- A **Random Forest** is an ensemble method that `combines multiple decision trees` to improve accuracy and reduce overfitting. \n",
    "- Each tree is trained on `random subsets` of the data and features, and their predictions are `averaged` for better performance.\n",
    "- It’s robust, reduces variance, and works well for both classification and regression.\n",
    "\n",
    "### Steps to Create a Random Forest\n",
    "1. **Bagging**: Randomly sample subsets of the data (with replacement) for each tree.\n",
    "2. For each tree, randomly select a subset of features at each split.\n",
    "3. Train a decision tree on each sampled dataset.\n",
    "4. Repeat steps 1–3 to build multiple decision trees.\n",
    "5. **Aggregating**: For predictions, aggregate the outputs of all the trees (e.g., majority vote for classification or averaging for regression)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "class RandomForest:\n",
    "    \"\"\"\n",
    "    A random forest classifier for multi-class classification problems (using decision stumps with depth 1).\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_trees=7):\n",
    "        self.n_trees = n_trees\n",
    "        self.trees = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fits a random forest to the dataset (X, y).\n",
    "        \"\"\"\n",
    "        self.trees = []\n",
    "        for _ in range(self.n_trees):\n",
    "            stump = DecisionStump()\n",
    "            X_sample, y_sample = self._bootstrap_samples(X, y)\n",
    "            stump.fit(X_sample, y_sample)\n",
    "            self.trees.append(stump)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predicts class labels for samples in X.\n",
    "        \"\"\"\n",
    "        stump_predictions = np.array([stump.predict(X) for stump in self.trees])\n",
    "        return self._majority_vote(stump_predictions)\n",
    "    \n",
    "    def _bootstrap_samples(self, X, y):\n",
    "        \"\"\"\n",
    "        Applies bootstrap resampling to the dataset.\n",
    "        \"\"\"\n",
    "        return resample(X, y, n_samples=len(X), replace=True)\n",
    "    \n",
    "    def _majority_vote(self, predictions):\n",
    "        \"\"\"\n",
    "        Returns the majority vote of the predictions.\n",
    "        \"\"\"\n",
    "        return np.apply_along_axis(lambda x: np.bincount(x).argmax(), axis=0, arr=predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we'll use the `Breast cancer wisconsin (diagnostic)` dataset which is a binary classification dataset.\n",
    "\n",
    "**Dataset Characteristics**:\n",
    "- Number of Instances: 569\n",
    "- Number of Attributes: 30 numeric, predictive attributes and the class\n",
    "\n",
    "**Attribute Information**:\n",
    "- radius (mean of distances from center to points on the perimeter)\n",
    "- texture (standard deviation of gray-scale values)\n",
    "- perimeter\n",
    "- area\n",
    "- smoothness (local variation in radius lengths)\n",
    "- compactness (perimeter^2 / area - 1.0)\n",
    "- concavity (severity of concave portions of the contour)\n",
    "- concave points (number of concave portions of the contour)\n",
    "- symmetry\n",
    "- fractal dimension (“coastline approximation” - 1)\n",
    "- **class**:\n",
    "    - WDBC-Malignant\n",
    "    - WDBC-Benign\n",
    "\n",
    "*The mean, standard error, and “worst” or largest (mean of the three worst/largest values) of these features were computed for each image, resulting in 30 features. For instance, field 0 is Mean Radius, field 10 is Radius SE, field 20 is Worst Radius.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "breast_cancer = load_breast_cancer()\n",
    "\n",
    "X, y = breast_cancer.data, breast_cancer.target\n",
    "\n",
    "print(f\"Breast Cancer features: {breast_cancer.feature_names}\")\n",
    "print(f\"Breast Cancer target: {breast_cancer.target_names}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can split the dataset into train and test sets. \n",
    "\n",
    "Also ensure that `y` is reshaped to avoid dimensional errors during model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Shape of X_train: {X_train.shape}, y_train: {y_train.shape}\")\n",
    "print(f\"Shape of X_test: {X_test.shape}, y_test: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can train our custom `RandomForest()` model and evaluate its performance using both the `accuracy_score` and the `f1_score`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_custom = RandomForest()\n",
    "rf_custom.fit(X_train, y_train)\n",
    "\n",
    "rf_cust_predictions = rf_custom.predict(X_test)\n",
    "\n",
    "print(f\"Custom RF Accuracy: {accuracy_score(y_test, rf_cust_predictions):.3f}\")\n",
    "print(f\"Custom RF F1-Score: {f1_score(y_test, rf_cust_predictions, average='weighted'):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Scikit-learn's Random Forest Classifier\n",
    "After implementing a custom Random Forest from scratch, we will now compare the results by utilizing the Scikit-learn library, which provides a highly optimized implementation of random forests. This allows us to observe the behavior of random forests without dealing with the complexities of manually building one. We’ll also visualize the decision trees used in the random forest to gain insight into how it makes decisions.\n",
    "\n",
    "*Make sure to try different combinations of hyperparameters (n_estimators, max_depth, min_samples, etc.)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf_sklearn = RandomForestClassifier(n_estimators=7, max_depth=1, criterion='entropy', random_state=42)\n",
    "rf_sklearn.fit(X_train, y_train.ravel())\n",
    "\n",
    "rf_skl_predictions = rf_sklearn.predict(X_test)\n",
    "\n",
    "print(f\"Sklearn RF Accuracy: {accuracy_score(y_test, rf_skl_predictions):.3f}\")\n",
    "print(f\"Sklearn RF F1-Score: {f1_score(y_test, rf_skl_predictions, average='weighted'):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, tree in enumerate(rf_sklearn.estimators_):\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plot_tree(tree, filled=True, feature_names=list(breast_cancer.feature_names), class_names=list(breast_cancer.target_names))\n",
    "    plt.title(f\"Random Forest Visualization - Tree {idx + 1}\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now choose a random sample to illustrate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_idx = 112\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.DataFrame(X_test[112].reshape(1, -1), columns=breast_cancer.feature_names).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "votes = [tree.predict(X_test[112].reshape(1, -1)) for tree in rf_sklearn.estimators_]\n",
    "final_prediction = rf_sklearn.predict(X_test[sample_idx].reshape(1, -1))[0]\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.scatter([range(1, len(rf_sklearn.estimators_) + 1)], votes, s=100, alpha=0.7, label='Votes')\n",
    "plt.axhline(y=final_prediction, color='r', linestyle='--', label='Final Prediction')\n",
    "plt.yticks([0, 1], ['Class 0', 'Class 1'])\n",
    "plt.xlabel('Decision Trees')\n",
    "plt.ylabel('Votes')\n",
    "plt.title(f'Random Forest: Votes from Each DT for Sample #{sample_idx + 1}')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost\n",
    "\n",
    "You've learned about the `AdaBoost` algorithm in class and through the slides. Another prominent algorithm within the ensemble methods is `XGBoost`. While you may have encountered it before, you can explore more details through this [link]((https://en.wikipedia.org/wiki/XGBoost)). This algorithm builds upon the decision trees you studied earlier.\n",
    "\n",
    "**XGBoost**, or Extreme Gradient Boosting, is an efficient and scalable implementation of the gradient boosting framework. It is widely used in machine learning competitions and practical applications due to its speed and performance. XGBoost enhances the standard gradient boosting algorithm by introducing optimizations and regularization techniques, making it robust against overfitting.\n",
    "\n",
    "### Key Features of XGBoost:\n",
    "- Parallel Processing: Utilizes parallel and distributed computing to speed up training.\n",
    "- Regularization: Incorporates `L1 (Lasso)` and `L2 (Ridge)` regularization to control overfitting.\n",
    "- Tree Pruning: Employs a depth-first approach to prune trees, leading to faster computations.\n",
    "- Handling Missing Values: Automatically learns how to handle missing data, eliminating the need for imputation.\n",
    "- Feature Importance: Provides insights into feature importance for better interpretability.\n",
    "\n",
    "\n",
    "### How it works\n",
    "1. **Initialization**: Starts with an initial prediction (e.g., mean value).\n",
    "2. **Building Trees**: Iteratively adds decision trees to minimize the loss function.\n",
    "3. **Gradient Descent**: Uses the gradients of the loss function to fit new trees, improving the model iteratively.\n",
    "4. **Boosting Process**: Combines the predictions of all trees to form a strong learner, reducing bias and variance.\n",
    "\n",
    "Below is an overview of how XGBoost works:\n",
    "\n",
    "<dev style=\"text-align: center\">\n",
    "<img src=\"./assets/XGBoost.png\" />\n",
    "</dev>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, again we'll use the `Breast cancer wisconsin (diagnostic)` dataset which is a binary classification dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier, to_graphviz\n",
    "\n",
    "xgb_model = XGBClassifier(eval_metric='logloss', random_state=42)\n",
    "xgb_model.fit(X_train, y_train.ravel())\n",
    "\n",
    "xgb_predictions = xgb_model.predict(X_test)\n",
    "\n",
    "print(f\"XGB Accuracy: {accuracy_score(y_test, xgb_predictions):.3f}\")\n",
    "print(f\"XGB F1-Score: {f1_score(y_test, xgb_predictions, average='weighted'):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use the built-in functions to plot the XGB decision rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model.get_booster().feature_names = list(breast_cancer.feature_names)\n",
    "graph = to_graphviz(xgb_model)\n",
    "graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will utilize the `Breast Cancer Wisconsin (Diagnostic)` dataset to compare the accuracy of the two primary methods discussed earlier: `Random Forest` and `XGBoost`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report, ConfusionMatrixDisplay\n",
    "\n",
    "def test_dataset(X, y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    rf_model = RandomForestClassifier(n_estimators=10, random_state=42)\n",
    "    rf_model.fit(X_train, y_train)\n",
    "    rf_predictions = rf_model.predict(X_test)\n",
    "\n",
    "    xgb_model = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
    "    xgb_model.fit(X_train, y_train)\n",
    "    xgb_predictions = xgb_model.predict(X_test)\n",
    "\n",
    "    rf_accuracy = accuracy_score(y_test, rf_predictions)\n",
    "    xgb_accuracy = accuracy_score(y_test, xgb_predictions)\n",
    "\n",
    "    print(f\"Random Forest Accuracy: {rf_accuracy:.4f}\")\n",
    "    print(f\"XGBoost Accuracy: {xgb_accuracy:.4f}\")\n",
    "\n",
    "    _, ax = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "    cm_rf = confusion_matrix(y_test, rf_predictions)\n",
    "    ConfusionMatrixDisplay(cm_rf, display_labels=list(breast_cancer.target_names)).plot(ax=ax[0])\n",
    "    ax[0].set_title('Random Forest Confusion Matrix')\n",
    "\n",
    "    cm_xgb = confusion_matrix(y_test, xgb_predictions)\n",
    "    ConfusionMatrixDisplay(cm_xgb, display_labels=list(breast_cancer.target_names)).plot(ax=ax[1])\n",
    "    ax[1].set_title('XGBoost Confusion Matrix')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    print(\"\\nRandom Forest Classification Report:\\n\", classification_report(y_test, rf_predictions, target_names=list(breast_cancer.target_names)))\n",
    "    print(\"-\" * 60)\n",
    "    print(\"\\nXGBoost Classification Report:\\n\", classification_report(y_test, xgb_predictions, target_names=list(breast_cancer.target_names)))\n",
    "\n",
    "test_dataset(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that they have a quite similar performance in simple and `balanced` datasets like the one above.\n",
    "\n",
    "But there are many proofs that XGBoost works much better than the Random Forest, when we're working with `more complicated` or `imbalanced` datasets. Below we can compare them again on a different and more complicated dataset.\n",
    "\n",
    "For this task, we'll use three different imbalanced datasets which are located in the \"./assets/imbalanced_datasets\" directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd1 = pd.read_csv(\"./assets/imbalanced_datasets/1.csv\")\n",
    "pd2 = pd.read_csv(\"./assets/imbalanced_datasets/2.csv\")\n",
    "pd3 = pd.read_csv(\"./assets/imbalanced_datasets/3.csv\")\n",
    "\n",
    "print(f\"Shape of the first dataset: {pd1.shape}\")\n",
    "print(f\"Shape of the second dataset: {pd2.shape}\")\n",
    "print(f\"Shape of the third dataset: {pd3.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1, y1 = pd1.drop(columns=['target']), pd1['target']\n",
    "X2, y2 = pd2.drop(columns=['target']), pd2['target']\n",
    "X3, y3 = pd3.drop(columns=['target']), pd3['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the first dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset(X1, y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset(X2, y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset(X3, y3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that overall, XGBoost performs better on imbalanced datasets. \n",
    "\n",
    "But keep in mind that both of them are very strong classifiers, and we can use different methods like hyperparameter optimization, ... to make them even stronger. e.g. Even on these imbalanced datasets, we can use higher number of estimators for the Random Forest to make its accuracy better than XGBoost."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

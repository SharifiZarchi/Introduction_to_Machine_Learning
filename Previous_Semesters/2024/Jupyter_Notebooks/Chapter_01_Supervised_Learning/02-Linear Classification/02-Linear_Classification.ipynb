{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yEhXpMmWKBSB"
   },
   "source": [
    "<br>\n",
    "<font>\n",
    "<!-- <img src=\"https://cdn.freebiesupply.com/logos/large/2x/sharif-logo-png-transparent.png\" alt=\"SUT logo\" width=300 height=300 align=left class=\"saturate\"> -->\n",
    "<div dir=ltr align=center>\n",
    "<img src=\"https://cdn.freebiesupply.com/logos/large/2x/sharif-logo-png-transparent.png\" width=200 height=200>\n",
    "<br>\n",
    "<font color=0F5298 size=7>\n",
    "Introduction to Machine Learning <br>\n",
    "<font color=696880 size=4>\n",
    "<!-- <br> -->\n",
    "CE 40477 - Fall 2024\n",
    "\n",
    "<font color=GREEN size=5>\n",
    "<br>\n",
    "Mahan Bayhaghi & Hamidreza Gandomi\n",
    "<!-- <br> -->\n",
    "\n",
    "____\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4yJjsFY8WHg2"
   },
   "source": [
    "# Section 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xnVz-DVsWKaf"
   },
   "source": [
    "In this section, we would like to visualize why using techniques of linear regression (using SSE cost function) fails in linear classification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 584
    },
    "id": "4IZXKdfdWUdq",
    "outputId": "ceacdad8-fe3f-451c-f5b6-bf07c75bcb42"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(42)\n",
    "class_A = np.random.normal(loc=(4, 4), scale=0.25, size=(100, 2))\n",
    "labels_A = np.ones(class_A.shape[0])\n",
    "\n",
    "class_B = np.random.normal(loc=(-1, -1), scale=1.5, size=(50, 2))\n",
    "labels_B = np.zeros(class_B.shape[0])\n",
    "\n",
    "X = np.vstack((class_A, class_B))\n",
    "y = np.hstack((labels_A, labels_B))\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(class_A[:, 0], class_A[:, 1], label='Class A', color='blue')\n",
    "plt.scatter(class_B[:, 0], class_B[:, 1], label='Class B', color='red')\n",
    "plt.title(\"Generated Data: Class A and Class B\")\n",
    "plt.xlabel(\"x1\")\n",
    "plt.ylabel(\"x2\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KxRdPIUcWeed"
   },
   "source": [
    "As you can see, the data points are linearly separable in many ways. Now let's fit a line to them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 601
    },
    "id": "jWDwiVKHWlgG",
    "outputId": "9653efc5-355b-41b6-8130-49b55d5c0476"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "w1, w2 = model.coef_  # weights\n",
    "w0 = model.intercept_  # bias\n",
    "\n",
    "print(f\"Model weights: w1 = {w1:.2f}, w2 = {w2:.2f}, bias = {w0:.2f}\")\n",
    "\n",
    "# Plot the decision boundary: w1 * x1 + w2 * x2 + b = 0.5\n",
    "x_vals = np.linspace(-4, 6, 100)\n",
    "decision_boundary = (-w1 * x_vals - w0 + 0.5) / w2\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(class_A[:, 0], class_A[:, 1], label='Class A', color='blue')\n",
    "plt.scatter(class_B[:, 0], class_B[:, 1], label='Class B', color='red')\n",
    "plt.plot(x_vals, decision_boundary, label='Decision Boundary (SSE)', color='green')\n",
    "plt.title(\"Linear Classifier Using SSE for Classification\")\n",
    "plt.xlabel(\"x1\")\n",
    "plt.ylabel(\"x2\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HqCsxUmSWw8o"
   },
   "source": [
    "Even the best fitted line fails to classify points. **Can you explain why ?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O4TzTi2JkYZs"
   },
   "source": [
    "# Section 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jk9NyHbQ9t6C"
   },
   "source": [
    "In this section, we would visualize linearly separable and non-linearly separable data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NRAV7FDF9b92"
   },
   "source": [
    "## Part A: Linearly Separable Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UjaANj2tmSd2"
   },
   "outputs": [],
   "source": [
    "def generate_ab_class(n_points=100):\n",
    "    class_A = []\n",
    "    class_B = []\n",
    "    while len(class_A) < n_points or len(class_B) < n_points:\n",
    "        x = np.random.uniform(0, 10)\n",
    "        y = np.random.uniform(0, 10)\n",
    "        if y > x and len(class_A) < n_points:\n",
    "            class_A.append([x, y])\n",
    "        elif y < x and len(class_B) < n_points:\n",
    "            class_B.append([x, y])\n",
    "    return np.array(class_A), np.array(class_B)\n",
    "\n",
    "class_A, class_B = generate_ab_class()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2TC74CSI-2db"
   },
   "source": [
    "Plot the data points and include the decision boundary \\\\( y = x \\\\)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 738
    },
    "id": "zUa7r8rU-2lX",
    "outputId": "d7ae1b25-54e3-4934-b039-795277252f15"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "plt.scatter(class_A[:, 0], class_A[:, 1], color='green', label='Class A (y > x)')\n",
    "plt.scatter(class_B[:, 0], class_B[:, 1], color='orange', label='Class B (y < x)')\n",
    "plt.plot([0, 10], [0, 10], color='black', linestyle='--', label='Decision Boundary (y = x)')\n",
    "plt.xlabel('X-axis')\n",
    "plt.ylabel('Y-axis')\n",
    "plt.title('Classification Based on y > x and y < x')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_-jBpjjo_0tz"
   },
   "source": [
    "## Part B: Non-Linearly Separable Data (XOR Problem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3zFLNdIw-2v7"
   },
   "outputs": [],
   "source": [
    "def generate_xor_data(n_points=200, seed=42):\n",
    "    np.random.seed(seed)\n",
    "    class_A = []\n",
    "    class_B = []\n",
    "    while len(class_A) < n_points or len(class_B) < n_points:\n",
    "        x = np.random.uniform(0, 1)\n",
    "        y = np.random.uniform(0, 1)\n",
    "        if (x > 0.5 and y > 0.5) or (x < 0.5 and y < 0.5):\n",
    "            if len(class_A) < n_points:\n",
    "                class_A.append([x, y])\n",
    "        else:\n",
    "            if len(class_B) < n_points:\n",
    "                class_B.append([x, y])\n",
    "    return np.array(class_A), np.array(class_B)\n",
    "\n",
    "class_A_xor, class_B_xor = generate_xor_data()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Ksf3cXp_CpA"
   },
   "source": [
    "Plot the data points. Try to draw a linear decision boundary. Comment on why the classes cannot be separated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 738
    },
    "id": "H8WReSdQ_DLv",
    "outputId": "a6ff0c8e-d565-458c-ce98-cae445e8e7c3"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "plt.scatter(class_A_xor[:, 0], class_A_xor[:, 1], color='green', label='Class A (XOR)')\n",
    "plt.scatter(class_B_xor[:, 0], class_B_xor[:, 1], color='orange', label='Class B (XOR)')\n",
    "plt.plot([0, 1], [0, 1], color='black', linestyle='--', label='Attempted Decision Boundary (y = x)')\n",
    "plt.xlabel('X-axis')\n",
    "plt.ylabel('Y-axis')\n",
    "plt.title('XOR Classification and Linear Decision Boundary')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v8xUV1aXflch"
   },
   "source": [
    "## Part C: Non-Linearly Separable Data (Circular data points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N06MLwbtffco"
   },
   "outputs": [],
   "source": [
    "def generate_data(n_points=200, seed=42):\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # Class 0: points inside a circle with radius 5\n",
    "    radius_0 = 5\n",
    "    theta_0 = np.random.uniform(0, 2 * np.pi, n_points)\n",
    "    r_0 = radius_0 * np.sqrt(np.random.uniform(0, 1, n_points))  # sqrt for uniform distribution\n",
    "    x0 = r_0 * np.cos(theta_0)\n",
    "    y0 = r_0 * np.sin(theta_0)\n",
    "    class_0 = np.vstack((x0, y0)).T\n",
    "\n",
    "    # Class 1: points in an annulus between radius 8 and 10\n",
    "    inner_radius_1 = 8\n",
    "    outer_radius_1 = 10\n",
    "    theta_1 = np.random.uniform(0, 2 * np.pi, n_points)\n",
    "    r_1 = np.sqrt(np.random.uniform(inner_radius_1**2, outer_radius_1**2, n_points))\n",
    "    x1 = r_1 * np.cos(theta_1)\n",
    "    y1 = r_1 * np.sin(theta_1)\n",
    "    class_1 = np.vstack((x1, y1)).T\n",
    "\n",
    "    return class_0, class_1\n",
    "\n",
    "class_0, class_1 = generate_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 738
    },
    "id": "RLnVgnyVfi9Q",
    "outputId": "457fc90f-fe58-48b3-d667-2a1f17487f6d"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "plt.scatter(class_0[:, 0], class_0[:, 1], color='blue', label='Class 0 (Circle)')\n",
    "plt.scatter(class_1[:, 0], class_1[:, 1], color='red', label='Class 1 (Annulus)')\n",
    "plt.xlabel('X-axis')\n",
    "plt.ylabel('Y-axis')\n",
    "plt.title('Class 0 and Class 1 Distribution')\n",
    "plt.legend()\n",
    "plt.axis('equal')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dl7K_EQrakrj"
   },
   "source": [
    "# Section 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kE3P9L30amiy"
   },
   "source": [
    "In this section, we will implement a perceptron classifier from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7_dmlWigmavx"
   },
   "outputs": [],
   "source": [
    "class Perceptron:\n",
    "    def __init__(self, learning_rate=0.01, n_epochs=1000):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_epochs = n_epochs\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "        self.errors_ = []  # storing the number of misclassifications in each epoch\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Train the Perceptron model on the provided data.\n",
    "\n",
    "        Parameters:\n",
    "        X : array-like, shape = [n_samples, n_features]\n",
    "            Training vectors.\n",
    "        y : array-like, shape = [n_samples]\n",
    "            Target values. Must be +1 or -1.\n",
    "        \"\"\"\n",
    "        n_samples, n_features = X.shape\n",
    "        # starting weights and bias equal zeros\n",
    "        self.weights = np.zeros(n_features)\n",
    "        self.bias = 0.0\n",
    "\n",
    "        for epoch in range(self.n_epochs):\n",
    "            errors = 0\n",
    "            for idx in range(n_samples):\n",
    "                linear_output = np.dot(X[idx], self.weights) + self.bias  # w^T x + b\n",
    "                y_pred = self._unit_step(linear_output)\n",
    "                if y[idx] != y_pred: # misclassfied\n",
    "                    update = self.learning_rate * y[idx]\n",
    "                    self.weights += update * X[idx]\n",
    "                    self.bias += update\n",
    "                    errors += 1\n",
    "            self.errors_.append(errors)\n",
    "            # if no errors, convergence achieved\n",
    "            if errors == 0:\n",
    "                print(f\"Converged after {epoch+1} epochs\")\n",
    "                break\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict class labels for samples in X.\n",
    "\n",
    "        Parameters:\n",
    "        X : array-like, shape = [n_samples, n_features]\n",
    "\n",
    "        Returns:\n",
    "        array, shape = [n_samples]\n",
    "            Predicted class labels.\n",
    "        \"\"\"\n",
    "        linear_output = np.dot(X, self.weights) + self.bias\n",
    "        return self._unit_step(linear_output)\n",
    "\n",
    "    def _unit_step(self, x):\n",
    "        return np.where(x >= 0, 1, -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z_v9nQlTTD4r",
    "outputId": "029e0e84-9362-4022-aa88-cdc69ada1705"
   },
   "outputs": [],
   "source": [
    "X_ab = np.vstack((class_A, class_B))\n",
    "y_ab = np.hstack((np.ones(class_A.shape[0]), -np.ones(class_B.shape[0])))\n",
    "\n",
    "shuffle_idx = np.random.permutation(len(X_ab))\n",
    "X_ab, y_ab = X_ab[shuffle_idx], y_ab[shuffle_idx]\n",
    "\n",
    "print(\"Combined Data Sample Points:\\n\", X_ab[:5])\n",
    "print(\"Combined Labels:\\n\", y_ab[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HcCPCmSrTHOV",
    "outputId": "4f3f3529-e0ad-4ced-8aa3-eadb767c54ec"
   },
   "outputs": [],
   "source": [
    "perceptron = Perceptron(learning_rate=0.01, n_epochs=1000)\n",
    "perceptron.fit(X_ab, y_ab)\n",
    "\n",
    "print(f\"Final Weights: {perceptron.weights}\")\n",
    "print(f\"Final Bias: {perceptron.bias}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v0bYWdKKc9Iy"
   },
   "source": [
    "Let us visualize the decission boundary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 584
    },
    "id": "_hDTaxVrcqxA",
    "outputId": "147d9a33-d09d-4298-95b2-50c55b1019b4"
   },
   "outputs": [],
   "source": [
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "x_min, x_max = X_ab[:, 0].min() - 1, X_ab[:, 0].max() + 1\n",
    "y_min, y_max = X_ab[:, 1].min() - 1, X_ab[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.linspace(x_min, x_max, 500),\n",
    "                     np.linspace(y_min, y_max, 500))\n",
    "\n",
    "grid = np.c_[xx.ravel(), yy.ravel()]\n",
    "Z = perceptron.predict(grid)\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "cmap_light = ListedColormap(['#FFAAAA', '#AAAAFF'])\n",
    "cmap_bold = ListedColormap(['#FF0000', '#0000FF'])\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "plt.contourf(xx, yy, Z, alpha=0.3, cmap=cmap_light)\n",
    "\n",
    "if perceptron.weights[1] != 0:\n",
    "    x_vals = np.array([x_min, x_max])\n",
    "    y_vals = -(perceptron.weights[0] * x_vals + perceptron.bias) / perceptron.weights[1]\n",
    "    plt.plot(x_vals, y_vals, 'k--', label='Decision Boundary')\n",
    "else:\n",
    "    x_val = -perceptron.bias / perceptron.weights[0]\n",
    "    plt.axvline(x=x_val, color='k', linestyle='--', label='Decision Boundary')\n",
    "\n",
    "plt.scatter(class_A[:, 0], class_A[:, 1], color='red', marker='o', label='Class A')\n",
    "plt.scatter(class_B[:, 0], class_B[:, 1], color='blue', marker='s', label='Class B')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('Perceptron Decision Boundary and Decision Regions')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SaNNKHinmSxQ"
   },
   "source": [
    "# Section 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "En76tSBWmNzQ"
   },
   "source": [
    "Let us revisit the breast cancer dataset again:\n",
    "   - Select two features `mean radius` and `mean texture` for visualization purposes.\n",
    "   - Split the dataset into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ce-DqxM-mWbn"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "import pandas as pd\n",
    "\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "df = pd.DataFrame(X, columns=data.feature_names)\n",
    "df['target'] = y\n",
    "\n",
    "selected_features = ['mean radius', 'mean texture']\n",
    "X_selected = df[selected_features].values\n",
    "y_selected = y  # 0 = malignant, 1 = benign\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iGrj1tg_aZ2v"
   },
   "source": [
    "Split data to 80% train and 20% test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8Hsyyz548Nfl",
    "outputId": "d0f8942f-940a-4352-d6e8-f3ebad659d59"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_selected, y_selected, test_size=0.2, random_state=42, stratify=y_selected\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {X_train.shape[0]}\")\n",
    "print(f\"Testing samples: {X_test.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nh8pmXjbdXRZ"
   },
   "outputs": [],
   "source": [
    "# convert labels: 0 -> -1, 1 -> 1\n",
    "y_train_perceptron = np.where(y_train == 0, -1, 1)\n",
    "y_test_perceptron = np.where(y_test == 0, -1, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9--kcQw7dZE5",
    "outputId": "a86e9648-4edc-4534-f8c6-ab9ed3a0c7f7"
   },
   "outputs": [],
   "source": [
    "perceptron = Perceptron(learning_rate=0.01, n_epochs=1000)\n",
    "\n",
    "perceptron.fit(X_train, y_train_perceptron)\n",
    "\n",
    "print(f\"Final Weights: {perceptron.weights}\")\n",
    "print(f\"Final Bias: {perceptron.bias}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I86Qh2-OgLiQ"
   },
   "source": [
    "As you remember, samples are not linearly separable. We can expect number of misclassifications not to converge to 0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 430
    },
    "id": "7zVpOL8EdcrL",
    "outputId": "ff795ee0-3ec4-4ee9-8953-3b53fcb8f705"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(range(1, len(perceptron.errors_) + 1, 20), perceptron.errors_[::20], marker='o')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Number of Misclassifications')\n",
    "plt.title('Perceptron Learning Progress')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 584
    },
    "id": "vYEeOiaZdhP1",
    "outputId": "92621432-0904-4778-a104-a0c1250b89a9"
   },
   "outputs": [],
   "source": [
    "x_min, x_max = X_train[:, 0].min() - 1, X_train[:, 0].max() + 1\n",
    "y_min, y_max = X_train[:, 1].min() - 1, X_train[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.linspace(x_min, x_max, 500),\n",
    "                     np.linspace(y_min, y_max, 500))\n",
    "\n",
    "grid = np.c_[xx.ravel(), yy.ravel()]\n",
    "Z = perceptron.predict(grid)\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "cmap_light = ListedColormap(['#FFAAAA', '#AAAAFF'])\n",
    "cmap_bold = ListedColormap(['#FF0000', '#0000FF'])\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "plt.contourf(xx, yy, Z, alpha=0.3, cmap=cmap_light)\n",
    "\n",
    "if perceptron.weights[1] != 0:\n",
    "    x_vals = np.array([x_min, x_max])\n",
    "    y_vals = -(perceptron.weights[0] * x_vals + perceptron.bias) / perceptron.weights[1]\n",
    "    plt.plot(x_vals, y_vals, 'k--', label='Decision Boundary')\n",
    "else:\n",
    "    x_val = -perceptron.bias / perceptron.weights[0]\n",
    "    plt.axvline(x=x_val, color='k', linestyle='--', label='Decision Boundary')\n",
    "\n",
    "# Malignant: 0 (red), Benign: 1 (blue)\n",
    "plt.scatter(X_test[y_test == 0, 0], X_test[y_test == 0, 1],\n",
    "            color='red', marker='o', edgecolor='k', label='Malignant')\n",
    "plt.scatter(X_test[y_test == 1, 0], X_test[y_test == 1, 1],\n",
    "            color='blue', marker='s', edgecolor='k', label='Benign')\n",
    "\n",
    "plt.ylim(10,40)\n",
    "plt.xlabel('Mean Radius')\n",
    "plt.ylabel('Mean Texture')\n",
    "plt.title('Perceptron Decision Boundary and Decision Regions (Breast Cancer Dataset)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZOMPiX5vQFoT"
   },
   "source": [
    "<br>\n",
    "<font>\n",
    "<!-- <img src=\"https://cdn.freebiesupply.com/logos/large/2x/sharif-logo-png-transparent.png\" alt=\"SUT logo\" width=300 height=300 align=left class=\"saturate\"> -->\n",
    "<div dir=ltr align=center>\n",
    "<img src=\"https://cdn.freebiesupply.com/logos/large/2x/sharif-logo-png-transparent.png\" width=200 height=200>\n",
    "<br>\n",
    "<font color=0F5298 size=8>\n",
    "Introduction to Machine Learning <br>\n",
    "<font color=696880 size=5>\n",
    "<!-- <br> -->\n",
    "Computer Engineering Department\n",
    "<br>\n",
    "Sharif University of Technology\n",
    "\n",
    "<font color=696880 size=5>\n",
    "<br>\n",
    "CE 40477 - Fall 2024\n",
    "\n",
    "<font color=GREEN size=5>\n",
    "<br>\n",
    "Mahan Bayhaghi & Mahdi Akbar\n",
    "<!-- <br> -->\n",
    "\n",
    "____\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h9S7059Ghuym"
   },
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rwBQ-Xn1xc-6"
   },
   "source": [
    "## Binary Classification\n",
    "In this section, we will use [breast cancer dataset](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html) of scikit-learn which is a classic binary classification dataset used for machine learning. This dataset contains multiple features for tumors as well as it's label (either benign or malignant)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4GUthYnyi-SI",
    "outputId": "9e02f41a-6392-444d-da93-8917f077b5b8"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target  # labels (0: malignant, 1: benign)\n",
    "print(data.feature_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y1J6KkHTnK0B"
   },
   "source": [
    "For the sake of visualization, we will only use two features: mean perimeter of tumor and mean smoothness of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 584
    },
    "id": "-6AA32TtjDl3",
    "outputId": "d6a74c4a-b0a3-477a-b5d9-1571e5c3fc5e"
   },
   "outputs": [],
   "source": [
    "# we will only take two features for visualization purposes\n",
    "X = data.data[:, [2, 4]]  # mean perimeter and mean smoothnes\n",
    "\n",
    "# Standardizing the features\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# add bias (x_0 = 1) term to X\n",
    "X_bias = np.hstack((X, np.ones((X.shape[0], 1))))\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_bias, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X_train[y_train == 0, 0], X_train[y_train == 0, 1], color='red', label='Malignant')\n",
    "plt.scatter(X_train[y_train == 1, 0], X_train[y_train == 1, 1], color='blue', label='Benign')\n",
    "plt.xlabel('Mean Perimeter')\n",
    "plt.ylabel('Mean Smoothness')\n",
    "plt.title('Breast Cancer Dataset (Training Set)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UNpVnSzznO6A"
   },
   "source": [
    "First, let us define $ \\sigma(z) $ and function for loss and gradient descent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dwqwnFuokNXs"
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def compute_loss(w, X, y):\n",
    "    z = X @ w\n",
    "    # applying the sigmoid function to get predicted probabilities\n",
    "    predictions = sigmoid(z)\n",
    "    # binary cross-entropy loss\n",
    "    loss = -np.mean(y * np.log(predictions) + (1 - y) * np.log(1 - predictions))\n",
    "    return loss\n",
    "\n",
    "def compute_gradient(w, X, y):\n",
    "    z = X @ w\n",
    "    predictions = sigmoid(z)\n",
    "    # error = difference between predicted and true labels\n",
    "    errors = predictions - y\n",
    "    # compute the gradient of the loss function\n",
    "    gradient = X.T @ errors / len(y)\n",
    "    return gradient\n",
    "\n",
    "def validation_accuracy(w, X_val, y_val):\n",
    "    probabilities = sigmoid(X_val @ w)\n",
    "    predictions = (probabilities > 0.5).astype(int) # decission rule for binary classification\n",
    "    accuracy = np.mean(predictions == y_val)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3qZdpfGukir-"
   },
   "outputs": [],
   "source": [
    "def gradient_descent_logistic(X_train, y_train, X_val, y_val,\n",
    "                              learning_rate=0.1, n_steps=1000, tolerance=1e-6):\n",
    "    w = np.zeros(X_train.shape[1])  # start with all weights equal to 0\n",
    "    loss_history = [compute_loss(w, X_train, y_train)]\n",
    "    val_accuracy_history = [validation_accuracy(w, X_val, y_val)]\n",
    "    weights_history = [w.copy()]  # storing weights for decision boundary plotting\n",
    "\n",
    "    for step in range(1, n_steps + 1):\n",
    "        grad = compute_gradient(w, X_train, y_train)\n",
    "        w -= learning_rate * grad  # update rule\n",
    "        loss = compute_loss(w, X_train, y_train)\n",
    "        loss_history.append(loss)\n",
    "\n",
    "        # compute validation accuracy\n",
    "        acc = validation_accuracy(w, X_val, y_val)\n",
    "        val_accuracy_history.append(acc)\n",
    "\n",
    "        # storing weights every 10 steps for plotting\n",
    "        if step % 10 == 0:\n",
    "            weights_history.append(w.copy())\n",
    "\n",
    "        # check convergence\n",
    "        if np.abs(loss_history[-2] - loss_history[-1]) < tolerance:\n",
    "            print(f'Converged at step {step}')\n",
    "            break\n",
    "\n",
    "        if step % 100 == 0:\n",
    "            print(f'Step {step}: Loss = {loss:.4f}, Validation Accuracy = {acc:.4f}')\n",
    "\n",
    "    return w, loss_history, val_accuracy_history, weights_history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HJpyy4jBseEp"
   },
   "source": [
    "The only thing remaining is to train our model on whole dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GtNCwvg6ZZlN",
    "outputId": "4b4c4a06-5faf-4318-b04d-d7b732f4b788"
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.05\n",
    "n_steps = 800\n",
    "\n",
    "w_opt, loss_history, val_accuracy_history, weights_history = gradient_descent_logistic(\n",
    "    X_train, y_train, X_val, y_val,\n",
    "    learning_rate=learning_rate,\n",
    "    n_steps=n_steps,\n",
    ")\n",
    "\n",
    "print(f'Optimized weights: {w_opt}')\n",
    "print(f'Decision rule: {w_opt[0]} * Mean Perimeter + {w_opt[1]} * Mean Smoothness + {w_opt[2]} > 0 : Benign')\n",
    "print(f'Decision rule: {w_opt[0]} * Mean Perimeter + {w_opt[1]} * Mean Smoothness + {w_opt[2]} < 0 : Malignant')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VH2JVXnnst1z"
   },
   "source": [
    "Let's visualize the change of decision boundary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 600
    },
    "id": "tNKxP5oQNxXT",
    "outputId": "313d5a3a-ccd5-4820-90a0-4dcca2df1dd9"
   },
   "outputs": [],
   "source": [
    "def plot_decision_boundary(w, X, y, step, total_steps):\n",
    "    x_values = np.array([X[:, 0].min() - 1, X[:, 0].max() + 1])\n",
    "    if w[1] != 0:\n",
    "        y_values = -(w[0] * x_values + w[2]) / w[1]\n",
    "        plt.plot(x_values, y_values, color='green', alpha=(0.20 + (step / total_steps) * 0.20))\n",
    "    else:\n",
    "        plt.axvline(x=-w[2]/w[0], color='green')\n",
    "    plt.legend()\n",
    "\n",
    "def plot_decision_boundaries(weights_history, X, y, total_steps):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(X[y == 0, 0], X[y == 0, 1], color='red', label='Malignant')\n",
    "    plt.scatter(X[y == 1, 0], X[y == 1, 1], color='blue', label='Benign')\n",
    "    plt.xlim(-2.5, 3)\n",
    "    plt.ylim(-3, 3)\n",
    "    for i, w in enumerate(weights_history):\n",
    "        step = i * 10\n",
    "        plot_decision_boundary(w, X, y, step, total_steps)\n",
    "    plt.title('Decision Boundaries during Gradient Descent')\n",
    "    plt.show()\n",
    "\n",
    "plot_decision_boundaries(weights_history, X_train[:, :2], y_train, n_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JbbhFgl4s0re"
   },
   "source": [
    "The plot above shows that decision boundary is trying to classify data points better at each step. Let's visualize this behaviour on loss function as well as validation accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "TZ2_Ch7XO2b4",
    "outputId": "d1d5797d-4eee-4a19-d089-4a753f367fd3"
   },
   "outputs": [],
   "source": [
    "def plot_training_loss(loss_history):\n",
    "    epochs = range(len(loss_history))\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(epochs, loss_history, color='tab:blue', label='Training Loss')\n",
    "    plt.xlabel('Iterations')\n",
    "    plt.ylabel('Training Loss')\n",
    "    plt.title('Training Loss over Iterations')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def plot_validation_accuracy(val_accuracy_history):\n",
    "    epochs = range(len(val_accuracy_history))\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(epochs, val_accuracy_history, color='tab:green', label='Validation Accuracy')\n",
    "    plt.xlabel('Iterations')\n",
    "    plt.ylabel('Validation Accuracy')\n",
    "    plt.title('Validation Accuracy over Iterations')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_confusion_matrix_cm(w, X_val, y_val):\n",
    "    probabilities = sigmoid(X_val @ w)\n",
    "    predictions = (probabilities > 0.5).astype(int)\n",
    "\n",
    "    cm = confusion_matrix(y_val, predictions)\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=['Predicted Malignant', 'Predicted Benign'],\n",
    "                yticklabels=['Actual Malignant', 'Actual Benign'])\n",
    "    plt.ylabel('Actual')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()\n",
    "\n",
    "plot_training_loss(loss_history)\n",
    "plot_validation_accuracy(val_accuracy_history)\n",
    "plot_confusion_matrix_cm(w_opt, X_val, y_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1l1D4YtMxl0I"
   },
   "source": [
    "## Multi-class classification\n",
    "So far, we have had implemented binary classification using binary cross entropy loss function. For the paradigm of multi-class problems, we will use softmax and update weights of each class accordingly.\n",
    "\n",
    "For this section, we will use iris dataset of scikit learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nbRvM0V_yD5x",
    "outputId": "a7884b14-5ace-4672-dc24-e1eca16d5dde"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the Iris dataset\n",
    "data = load_iris()\n",
    "X = data.data\n",
    "y = data.target  # labels (0, 1, 2 for the three classes)\n",
    "\n",
    "print(\"Feature Names:\", data.feature_names)\n",
    "print(\"Target Names:\", data.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 584
    },
    "id": "Ik7JKRUCyOZ9",
    "outputId": "519e3341-b056-4118-a81b-4091e5b9752f"
   },
   "outputs": [],
   "source": [
    "# Select two features: petal length and petal width\n",
    "feature_indices = [2, 3]  # 0-based indexing\n",
    "X = X[:, feature_indices]\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Add a bias term (column of ones) to the feature matrix\n",
    "X_bias = np.hstack((X_scaled, np.ones((X_scaled.shape[0], 1))))\n",
    "\n",
    "# Plot the standardized data\n",
    "plt.figure(figsize=(8, 6))\n",
    "colors = ['red', 'blue', 'green']\n",
    "labels = data.target_names\n",
    "\n",
    "for i in range(len(colors)):\n",
    "    plt.scatter(X_scaled[y == i, 0], X_scaled[y == i, 1], color=colors[i], label=labels[i])\n",
    "\n",
    "plt.xlabel('Petal Length (standardized)')\n",
    "plt.ylabel('Petal Width (standardized)')\n",
    "plt.title('Iris Dataset (Two Features)')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7chgTFjJySBx"
   },
   "source": [
    "Let us define softmax and GD as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BK8Vd9inuvM_"
   },
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    exp_z = np.exp(z)\n",
    "    return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "\n",
    "def compute_loss(w, X, y):\n",
    "    z = X @ w\n",
    "    predictions = softmax(z)\n",
    "\n",
    "    # converting labels to one-hot encoding\n",
    "    y_onehot = np.zeros_like(predictions)\n",
    "    y_onehot[np.arange(len(y)), y] = 1\n",
    "\n",
    "    # cross-entropy loss\n",
    "    epsilon = 1e-15  # to avoid log(0)\n",
    "    loss = -np.mean(np.sum(y_onehot * np.log(predictions + epsilon), axis=1))\n",
    "    return loss\n",
    "\n",
    "def compute_gradient(w, X, y):\n",
    "    z = X @ w\n",
    "    predictions = softmax(z)\n",
    "\n",
    "    # converting labels to one-hot encoding\n",
    "    y_onehot = np.zeros_like(predictions)\n",
    "    y_onehot[np.arange(len(y)), y] = 1\n",
    "\n",
    "    # gradient itself\n",
    "    errors = predictions - y_onehot\n",
    "    gradient = X.T @ errors / len(y)\n",
    "    return gradient\n",
    "\n",
    "def gradient_descent_multiclass(X, y, learning_rate=0.1, n_steps=10000, tolerance=1e-6):\n",
    "    n_features = X.shape[1]\n",
    "    n_classes = np.max(y) + 1  # labels are 0-indexed\n",
    "\n",
    "    w = np.zeros((n_features, n_classes)) # start with all weights equal to zero\n",
    "    loss_history = [compute_loss(w, X, y)]\n",
    "    weights_history = [w.copy()]  # For plotting decision boundaries\n",
    "\n",
    "    for step in range(1, n_steps + 1):\n",
    "        grad = compute_gradient(w, X, y)\n",
    "        w -= learning_rate * grad\n",
    "        loss = compute_loss(w, X, y)\n",
    "        loss_history.append(loss)\n",
    "\n",
    "        # storing weights every 10 steps for visualization\n",
    "        if step % 10 == 0:\n",
    "            weights_history.append(w.copy())\n",
    "\n",
    "        # check for convergence\n",
    "        if np.abs(loss_history[-2] - loss_history[-1]) < tolerance:\n",
    "            print(f'Converged at step {step}')\n",
    "            break\n",
    "\n",
    "    return w, loss_history, weights_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 509
    },
    "id": "IOWmF9hZLiXt",
    "outputId": "8dda2f0c-c05c-4a4e-a07c-3599c2f5298d"
   },
   "outputs": [],
   "source": [
    "w, loss_history, weights_history = gradient_descent_multiclass(X_bias, y)\n",
    "\n",
    "# visualize decision boundaries\n",
    "def plot_decision_boundaries(X, y, w):\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.01),\n",
    "                         np.arange(y_min, y_max, 0.01))\n",
    "    X_grid = np.c_[xx.ravel(), yy.ravel(), np.ones(xx.ravel().shape[0])]\n",
    "    Z = np.argmax(softmax(X_grid @ w), axis=1)\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    plt.contourf(xx, yy, Z, alpha=0.3, cmap=plt.cm.Paired)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors='k', cmap=plt.cm.Paired)\n",
    "    plt.xlabel('Petal Length (standardized)')\n",
    "    plt.ylabel('Petal Width (standardized)')\n",
    "    plt.title('Decision Boundaries')\n",
    "    plt.show()\n",
    "\n",
    "# decision boundaries\n",
    "plot_decision_boundaries(X_scaled, y, w)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 584
    },
    "id": "zTw_8xvOP0OK",
    "outputId": "28ea8af0-265d-4a7b-da2e-1486640f2270"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(loss_history, label='Loss')\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss History')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

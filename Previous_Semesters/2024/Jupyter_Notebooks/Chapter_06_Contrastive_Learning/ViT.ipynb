{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LAsvfaZRCEaM"
   },
   "source": [
    "# Vision Transformers\n",
    "\n",
    "**[CE477: Machine Learning](https://www.sharifml.ir/)**\n",
    "\n",
    "__Course Instructor__: Dr. Sharifi-Zarchi\n",
    "\n",
    "__Notebook Author__: Ramtin Moslemi\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/SharifiZarchi/Introduction_to_Machine_Learning/blob/main/Jupyter_Notebooks/Chapter_06_Contrastive_Learning/ViT.ipynb) [![Open In kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://raw.githubusercontent.com/SharifiZarchi/Introduction_to_Machine_Learning/main/Jupyter_Notebooks/Chapter_06_Contrastive_Learning/ViT.ipynb)\n",
    "\n",
    "---\n",
    "## Notebook Objectives\n",
    "\n",
    "In this notebook we are going to implement ViT from scratch for image classification using PyTorch.\n",
    "We will start by transforming the images into embeddings, then we will use Transformer Encoders and an MLP Head to create the ViT Architecture.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "l_JlLMVQB7IS"
   },
   "outputs": [],
   "source": [
    "# @title setup and imports\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import trange\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets.cifar import CIFAR10\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "0PaqVBCygpEl"
   },
   "outputs": [],
   "source": [
    "# @title helper functions\n",
    "\n",
    "def train_epoch(model, dataloader, loss_fn, optimizer, device):\n",
    "    model.train()\n",
    "    train_loss = 0.\n",
    "    train_acc = 0.\n",
    "    for images, labels in dataloader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        logits = model(images)\n",
    "        loss = loss_fn(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        train_loss += loss.item()\n",
    "        train_acc += (logits.argmax(dim=1) == labels).sum().item()\n",
    "    train_loss /= len(dataloader)\n",
    "    train_acc /= len(dataloader.dataset)\n",
    "    return train_loss, train_acc\n",
    "\n",
    "\n",
    "def validate_epoch(model, dataloader, loss_fn, device):\n",
    "    model.eval()\n",
    "    val_loss = 0.\n",
    "    val_acc = 0.\n",
    "    with torch.inference_mode():\n",
    "        for images, labels in dataloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            logits = model(images)\n",
    "            loss = loss_fn(logits, labels)\n",
    "            val_loss += loss.item()\n",
    "            val_acc += (logits.argmax(dim=1) == labels).sum().item()\n",
    "        val_loss /= len(dataloader)\n",
    "        val_acc /= len(dataloader.dataset)\n",
    "    return val_loss, val_acc\n",
    "\n",
    "\n",
    "def train_model(model, train_dataloader, val_dataloader, optimizer, n_epochs, device=device):\n",
    "    history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
    "    loss_fn = CrossEntropyLoss()\n",
    "    for _ in (pbar := trange(n_epochs)):\n",
    "        train_loss, train_acc = train_epoch(model, train_dataloader, loss_fn, optimizer, device)\n",
    "        history['train_loss'].append(train_loss), history['train_acc'].append(train_acc)\n",
    "        val_loss, val_acc = validate_epoch(model, val_dataloader, loss_fn, device)\n",
    "        history['val_loss'].append(val_loss), history['val_acc'].append(val_acc)\n",
    "        pbar.set_description(f'Training Accuracy {100 * train_acc:.2f}% | Validation Accuracy {100 * val_acc:.2f}% ')\n",
    "    return history\n",
    "\n",
    "\n",
    "def plot_history(history):\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    ax1.plot(history['train_loss'], label='train')\n",
    "    ax1.plot(history['val_loss'], label='val')\n",
    "    ax1.set_xlabel('Epochs')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.legend()\n",
    "\n",
    "    ax2.plot(history['train_acc'], label='train')\n",
    "    ax2.plot(history['val_acc'], label='val')\n",
    "    ax2.set_xlabel('Epochs')\n",
    "    ax2.set_ylabel('Accuracy')\n",
    "    ax2.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zykSHg17eouX",
    "outputId": "85e02ac3-7412-4e0c-e608-f5d0d6ad9ac7"
   },
   "outputs": [],
   "source": [
    "# @title CIFAR10 dataset\n",
    "\n",
    "norm_mean = (0.4914, 0.4822, 0.4465)\n",
    "norm_std = (0.2023, 0.1994, 0.2010)\n",
    "batch_size = 128\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(norm_mean, norm_std),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(norm_mean, norm_std),\n",
    "])\n",
    "\n",
    "trainset = CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
    "trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "testset = CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
    "testloader = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer',\n",
    "           'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HEmYWqWOT_U0"
   },
   "source": [
    "# Patch + Position Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q_ERUFUDTfMZ"
   },
   "source": [
    "In order to feed input images to a Transformer model, we need to convert the images to a sequence of vectors. This is done by splitting the image into a grid of non-overlapping patches, which are then linearly projected to obtain a fixed-size embedding vector for each patch. We can use PyTorch's `nn.Conv2d` layer for this purpose:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UKZwmm37CvqW"
   },
   "outputs": [],
   "source": [
    "class PatchEmbeddings(nn.Module):\n",
    "    \"\"\"\n",
    "    Convert the image into patches and then project them into a vector space.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, image_size, patch_size, num_channels, hidden_size):\n",
    "        super().__init__()\n",
    "        # Calculate the number of patches from the image size and patch size\n",
    "        self.num_patches = (image_size // patch_size) ** 2\n",
    "        # Create a projection layer to convert the image into patches\n",
    "        # The layer projects each patch into a vector of size hidden_size\n",
    "        self.projection = nn.Conv2d(num_channels, hidden_size, kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # (batch_size, num_channels, image_size, image_size) -> (batch_size, num_patches, hidden_size)\n",
    "        x = self.projection(x)\n",
    "        x = x.flatten(2).transpose(1, 2)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mtCefUFkTwNH"
   },
   "source": [
    "`kernel_size=self.patch_size` and `stride=self.patch_size` are to make sure the layer's filter is applied to non-overlapping patches.\n",
    "\n",
    "After the patches are converted to a sequence of embeddings, the [CLS] token is added to the beginning of the sequence, it will be used later in the classification layer to classify the image. The [CLS] token's embedding is learned during training.\n",
    "\n",
    "As patches from different positions may contribute differently to the final predictions, we also need a way to encode patch positions into the sequence. We're going to use learnable position embeddings to add positional information to the embeddings. This is similar to how position embeddings are used in Transformer models for NLP tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EGim7hr9C1c-"
   },
   "outputs": [],
   "source": [
    "class Embeddings(nn.Module):\n",
    "    \"\"\"\n",
    "    Combine the patch embeddings with the class token and position embeddings.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, image_size, patch_size, num_channels, hidden_size, hidden_dropout_prob):\n",
    "        super().__init__()\n",
    "        self.patch_embeddings = PatchEmbeddings(image_size, patch_size, num_channels, hidden_size)\n",
    "        # Create a learnable [CLS] token\n",
    "        # Similar to BERT, the [CLS] token is added to the beginning of\n",
    "        # the input sequence and is used to classify the entire sequence\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, hidden_size))\n",
    "        # Create position embeddings for the [CLS] token and the patch embeddings\n",
    "        # Add 1 to the sequence length for the [CLS] token\n",
    "        self.position_embeddings = nn.Parameter(torch.randn(1, self.patch_embeddings.num_patches + 1, hidden_size))\n",
    "        self.dropout = nn.Dropout(hidden_dropout_prob)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.patch_embeddings(x)\n",
    "        batch_size, _, _ = x.size()\n",
    "        # Expand the [CLS] token to the batch size (1, 1, hidden_size) -> (batch_size, 1, hidden_size)\n",
    "        cls_tokens = self.cls_token.expand(batch_size, -1, -1)\n",
    "        # Concatenate the [CLS] token to the beginning of the input sequence\n",
    "        # This results in a sequence length of (num_patches + 1)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        x = x + self.position_embeddings\n",
    "        x = self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uKElOeEfUWTq"
   },
   "source": [
    "# Vision Transformer (ViT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MafJ27talBTW"
   },
   "source": [
    "The following code implements the ViT model for image classification according to:\n",
    "\n",
    "![vit-figure](https://github.com/google-research/vision_transformer/raw/main/vit_figure.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zpvJ-56XC4yp"
   },
   "outputs": [],
   "source": [
    "class ViT(nn.Module):\n",
    "    \"\"\"\n",
    "    The ViT model for classification.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, image_size, patch_size, num_channels, hidden_size, num_heads, num_layers, mlp_dim, num_classes, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        # Step 1: Embedding module (Patch + Position + CLS token)\n",
    "        self.embeddings = Embeddings(image_size, patch_size, num_channels, hidden_size, dropout_rate)\n",
    "\n",
    "        # Step 2: Transformer Encoder using PyTorch's built-in nn.TransformerEncoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=hidden_size,       # the number of expected features in the input\n",
    "            nhead=num_heads,           # the number of heads in the multiheadattention models\n",
    "            dim_feedforward=mlp_dim,   # the dimension of the feedforward network model\n",
    "            dropout=dropout_rate,      # the dropout value\n",
    "            activation='gelu',          # the activation function of the intermediate layer\n",
    "            batch_first = True\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        # Step 3: Classification head (MLP head)\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.LayerNorm(hidden_size),\n",
    "            nn.Linear(hidden_size, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Step 1: Get patch + position embeddings and cls token\n",
    "        x = self.embeddings(x)\n",
    "\n",
    "        # Step 2: Pass through the transformer encoder\n",
    "        x = self.encoder(x)\n",
    "\n",
    "        # Step 3: Classification token is at position 0 of the sequence\n",
    "        cls_token_output = x[:, 0]\n",
    "\n",
    "        # Step 4: Pass through MLP classification head\n",
    "        logits = self.mlp_head(cls_token_output)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EjOMyEpmebtQ"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uIUdEPQdlzTs"
   },
   "source": [
    "All that remains to be done is to train the model and plot its performance. To do so we use the helper functions `train_model` and `plot_history`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 482
    },
    "id": "nY4f69uMebYZ",
    "outputId": "513eee8c-17df-41fe-bd17-6b89dceb16b6"
   },
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'image_size': 32,\n",
    "    'patch_size': 4,\n",
    "    'num_channels': 3,\n",
    "    'hidden_size': 128,\n",
    "    'num_heads': 8,\n",
    "    'num_layers': 6,\n",
    "    'mlp_dim': 256,\n",
    "    'num_classes': 10,\n",
    "    'dropout_rate': 0.1,\n",
    "}\n",
    "\n",
    "vit = ViT(**parameters).to(device)\n",
    "if device == 'cuda':\n",
    "    vit = torch.compile(vit)\n",
    "optim = Adam(vit.parameters(), lr=1e-3)\n",
    "\n",
    "results = train_model(vit, trainloader, testloader, optim, n_epochs=30)\n",
    "plot_history(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LR8IOITCmTSu"
   },
   "source": [
    "# Refrences\n",
    "\n",
    "\n",
    "*   [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929) paper and its [GitHub Repository](https://github.com/google-research/vision_transformer)\n",
    "*   [Implementing Vision Transformer (ViT) from Scratch](https://tintn.github.io/Implementing-Vision-Transformer-from-Scratch/)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "LAsvfaZRCEaM"
   ],
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

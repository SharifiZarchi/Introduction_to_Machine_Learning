{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yHuG8A6NseEX"
   },
   "source": [
    "# CLIP: Connecting text and images\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N4-8jRH7vJV_"
   },
   "source": [
    "\"CLIP\" or \"Contrastive Language-Image Pretraining\" is a powerful model developed by OpenAI that bridges the gap between natural language and image understanding. It can take both images and text as inputs and relate them in meaningful ways, allowing it to perform a variety of tasks such as zero-shot image classification, image search, and more.\n",
    "\n",
    "The model is trained by learning a joint embedding space where images and their corresponding text descriptions are closely aligned. In this notebook, we will explore how CLIP works, how to use it for various tasks, and how to implement it.\n",
    "\n",
    "## why CLIP\n",
    "Traditional image classification models are typically limited to the categories they were trained on. CLIP, on the other hand, can recognize a wide variety of objects and concepts in images without being explicitly trained on specific tasks. This capability is achieved by learning from a massive dataset of image-text pairs gathered from the internet. As a result, CLIP can generalize to many tasks without needing further fine-tuning.\n",
    "\n",
    "Some key use cases for CLIP include:\n",
    "\n",
    "- Zero-shot classification: Classify images based on new categories without additional training.\n",
    "- Image search: Find images related to specific text descriptions.\n",
    "- Text-to-image mapping: Generate embeddings for both images and text, enabling cross-modal understanding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a4xslcEFwXh2"
   },
   "source": [
    "## Basics\n",
    "\n",
    "The raw product of CLIP is a shared representation(embedding) between two modalities (text and images) by training on a large dataset of image-text pairs.\n",
    "\n",
    "1. Input Data: CLIP is trained on a large set of image-text pairs. Each image is accompanied by a textual description (e.g., a picture of a dog and the text \"a dog sitting in a park\").\n",
    "\n",
    "2. Dual Encoder Architecture:\n",
    "  - Image Encoder: CLIP uses a Vision Transformer (ViT) or a ResNet to process the images and generate an embedding vector for each image.\n",
    "\n",
    "  - Text Encoder: A Transformer model is used to process the text descriptions and generate an embedding vector for each description.\n",
    "\n",
    " - Image of the output of dual encoders:\n",
    "![dual encoders output](https://images.ctfassets.net/kftzwdyauwt9/fbc4f633-9ad4-4dc2-3809c22df5e0/0bd2d5abf90d052731538613e4a42668/overview-a.svg)\n",
    "\n",
    "3. Contrastive Loss: The optimization objective in CLIP is contrastive learning. After both the image and the text are passed through their respective encoders to produce embeddings, CLIP uses a contrastive loss that encourages the image and its matching text to have similar embeddings, while mismatched pairs (e.g., a dog image and \"a cat sitting on a tree\") are distinguished in the embedding space.\n",
    "  \n",
    "  This similarity is usually formalised as a distance in the embedding space that is closer to zero when embedded elements are more similar.\n",
    "\n",
    "4. Joint Embedding Space: After training, CLIP learns a joint embedding space where related images and text are close together, and unrelated ones are far apart. This allows CLIP to perform tasks as:\n",
    "\n",
    "  - Zero-Shot Classification: Given a new category (e.g., \"a cat\"), CLIP can classify images by computing the similarity between the image embeddings and the text embedding of the label.\n",
    "  - Text-Image Similarity: CLIP can rank images by their similarity to a textual description or rank text by its similarity to an image.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hc7K_VWqyxkG"
   },
   "source": [
    "## Pretrained Model\n",
    "Lets First take a look at the pretrained implementation of CLIP from OpenAI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Eoni2qi4y4yN",
    "outputId": "b891b9b9-c411-44e1-a5bd-9def869c7bf4"
   },
   "outputs": [],
   "source": [
    "# Install CLIP library\n",
    "!pip install git+https://github.com/openai/CLIP.git\n",
    "!pip install torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QcFrrGjny_D1"
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "import clip\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vgE-msOCzTiy",
    "outputId": "20cc39b2-850c-426e-e2a4-7c5bf0e4f4b5"
   },
   "outputs": [],
   "source": [
    "# Load the model and the preprocessing function\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3qMBZZpezJrD"
   },
   "source": [
    "Let's create a simple function that will accept an image URL and a list of text descriptions. The function will then calculate the similarity between the image and each text description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iazHy6XWzHsQ"
   },
   "outputs": [],
   "source": [
    "# Function to process image and text, and compute similarity\n",
    "def match_image_text(image_url, text_descriptions):\n",
    "    # Load and preprocess the image\n",
    "    response = requests.get(image_url)\n",
    "    img = Image.open(BytesIO(response.content))\n",
    "    image = preprocess(img).unsqueeze(0).to(device)\n",
    "\n",
    "    # Tokenize and encode the text\n",
    "    text = clip.tokenize(text_descriptions).to(device)\n",
    "\n",
    "    # Run the image and the text through the model\n",
    "    with torch.no_grad():\n",
    "        image_features = model.encode_image(image)\n",
    "        text_features = model.encode_text(text)\n",
    "\n",
    "    # Compute similarity\n",
    "    image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "    similarities = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "\n",
    "    return similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HJU3aZloznOV"
   },
   "outputs": [],
   "source": [
    "# @title Interactive Section: Enter an image URL and text descriptions\n",
    "# image_url = input(\"Enter an image URL: \")\n",
    "# text_descriptions = input(\"Enter text descriptions (comma-separated): \").split(',')\n",
    "\n",
    "# # Compute similarities\n",
    "# similarities = match_image_text(image_url, text_descriptions)\n",
    "\n",
    "# # Show results\n",
    "# print(f\"\\nImage URL: {image_url}\")\n",
    "# print(\"Text Descriptions and Similarity Scores:\")\n",
    "# for i, desc in enumerate(text_descriptions):\n",
    "#     print(f\"Description: {desc.strip()} | Similarity: {similarities[0, i].item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BISz4-MHH2ot"
   },
   "source": [
    "This picture shows what an idea of happened in the above code:\n",
    "![similarity scoring text based on image](https://images.ctfassets.net/kftzwdyauwt9/d9d46e4b-6d6a-4f9e-59a242ea1441/c7b386880f1af005fd02f159de7f4d00/overview-b.svg)\n",
    "\n",
    "As you (hopefully) saw the model didnt need to be trained on a dataset of your provided text and image. Hence the term zero-shot prediction.\n",
    "\n",
    "But how do `model.encode_image()` and `model.encode_text()` output the same embedding space for image and text. we will see that soon.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SYY77SxRyjlc"
   },
   "source": [
    "## Contrastive Loss\n",
    "\n",
    "The contrastive loss encourages the model to bring the embeddings of matching image-text pairs \"closer\" together and push the embeddings of non-matching pairs \"further apart\".\n",
    "\n",
    "For notions of \"closer\" and \"furthur\" in the embeddings showing a quanititative similarity between original data pairs we can use **cosine similarity** between the image and text embeddings.\n",
    "\n",
    "The contrastive loss can be formalized using the softmax function applied over the cosine similarity between the image and text embeddings. Here‚Äôs the mathematical formula for the contrastive loss in CLIP:\n",
    "\n",
    "$$\n",
    "L = - \\frac{1}{N} \\sum_{i=1}^{N} \\log \\frac{\\exp(\\text{sim}(x_i,y_i))}{\\sum_{j=1}^{N} \\exp(\\text{sim}(x_j,y_j))}\n",
    "$$\n",
    "Where:\n",
    "- ùëÅ is the number of image-text pairs in the batch.\n",
    "- $x_i$ is the image embedding for the ùëñ-th image.\n",
    "- $y_i$ is the text embedding for the corresponding ùëñ-th text.\n",
    "- $\\text{sim}(ùë•_ùëñ,ùë¶_ùëó)$ is the similarity (usually cosine similarity) between the image embedding $x_i$ and the text embedding $y_j$.\n",
    "\n",
    "The loss function penalizes when the similarity between matching pairs is low or when mismatching pairs have a high similarity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6eL3W2i5Q0s6"
   },
   "source": [
    "Having seen the contrastive loss function, now we can see the training process on $N$ sample image-text pairs with below psudoe-code from the paper:\n",
    "\n",
    "```\n",
    "# image_encoder - ResNet or Vision Transformer\n",
    "# text_encoder - CBOW or Text Transformer\n",
    "# I[n, h, w, c] - minibatch of aligned images\n",
    "# T[n, l] - minibatch of aligned texts\n",
    "# W_i[d_i, d_e] - learned proj of image to embed\n",
    "# W_t[d_t, d_e] - learned proj of text to embed\n",
    "# t - learned temperature parameter\n",
    "\n",
    "# extract feature representations of each modality\n",
    "I_f = image_encoder(I) #[n, d_i]\n",
    "T_f = text_encoder(T) #[n, d_t]\n",
    "\n",
    "# joint multimodal embedding [n, d_e]\n",
    "I_e = l2_normalize(np.dot(I_f, W_i), axis=1)\n",
    "T_e = l2_normalize(np.dot(T_f, W_t), axis=1)\n",
    "\n",
    "# scaled pairwise cosine similarities [n, n]\n",
    "logits = np.dot(I_e, T_e.T) * np.exp(t)\n",
    "\n",
    "# symmetric loss function\n",
    "labels = np.arange(n)\n",
    "loss_i = cross_entropy_loss(logits, labels, axis=0)\n",
    "loss_t = cross_entropy_loss(logits, labels, axis=1)\n",
    "loss = (loss_i + loss_t)/2\n",
    "```\n",
    "\n",
    "The logits created is the matrix or table u saw in the first picture, where on the diagonal is the cosine similarity of matching pairs and the off diagonal elements are missmatched pairs similarities.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HDWuO-of8Yry"
   },
   "source": [
    "## Visualise Embeddings\n",
    "\n",
    "We‚Äôll now use t-SNE to reduce the 512-dimensional embeddings from CLIP to 2D and 3D and visualize the relationship between images and their corresponding text descriptions. t-SNE helps in visualizing how similar or dissimilar image and text embeddings are in the shared embedding space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dBIBxoxhZPMr"
   },
   "source": [
    "visualisation code with t-SNE and plotly.\n",
    "try adjusting the perplexity parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UD8FoAhsoKFP"
   },
   "outputs": [],
   "source": [
    "# !pip install transformers torchvision plotly scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jiGVbs1CoN_n"
   },
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "# # Load the pretrained CLIP model and processor\n",
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
    "# clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H9aaSxbJ9AND"
   },
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import torch\n",
    "from sklearn.manifold import TSNE\n",
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "\n",
    "# Function to fetch and preprocess images\n",
    "def preprocess_images(image_urls):\n",
    "    images = []\n",
    "    for url in image_urls:\n",
    "        response = requests.get(url)\n",
    "        img = Image.open(BytesIO(response.content))\n",
    "        image = preprocess(img).unsqueeze(0).to(device)\n",
    "        images.append(image)\n",
    "    return torch.cat(images)\n",
    "\n",
    "# Function to extract embeddings for images and texts\n",
    "def extract_embeddings(images, text_descriptions):\n",
    "    # encode images\n",
    "    with torch.no_grad():\n",
    "        image_features = model.encode_image(images)\n",
    "\n",
    "    # Tokenize and encode texts\n",
    "    text = clip.tokenize(text_descriptions).to(device)\n",
    "    with torch.no_grad():\n",
    "        text_features = model.encode_text(text)\n",
    "\n",
    "    return image_features, text_features\n",
    "\n",
    "# Function to reduce embeddings to 2D or 3D using t-SNE\n",
    "def reduce_with_tsne(embeddings, dim = 2):\n",
    "    tsne = TSNE(n_components=dim, perplexity=30, learning_rate=200, n_iter=1000)\n",
    "    return tsne.fit_transform(embeddings.cpu())\n",
    "\n",
    "# Function to visualize image and text embeddings using Plotly (2D)\n",
    "def visualize_embeddings_plotly(images, text_descriptions, image_labels =None, dim = 2):\n",
    "    image_features, text_features = extract_embeddings(images, text_descriptions)\n",
    "\n",
    "    # Normalize embeddings\n",
    "    image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    # Combine and reduce to 2D or 3D using t-SNE\n",
    "    all_embeddings = torch.cat([image_features, text_features], dim=0)\n",
    "    reduced_embeddings = reduce_with_tsne(all_embeddings, dim = dim)\n",
    "\n",
    "    # Create labels for plotting\n",
    "    if image_labels == None:\n",
    "      labels = [\"Image \" + str(i+1) for i in range(len(images))] + text_descriptions\n",
    "    else:\n",
    "      labels = [\"Image \" + text_descriptions[image_labels[i]] for i in range(len(image_labels))] + text_descriptions\n",
    "\n",
    "    # Create a DataFrame for Plotly\n",
    "    import pandas as pd\n",
    "    if dim == 2:\n",
    "      df = pd.DataFrame(reduced_embeddings, columns=[\"x\", \"y\"])\n",
    "    elif dim == 3:\n",
    "      df = pd.DataFrame(reduced_embeddings, columns=[\"x\", \"y\", \"z\"])\n",
    "    else:\n",
    "      raise ValueError(\"Invalid dimension. Must be 2 or 3.\")\n",
    "\n",
    "    df[\"label\"] = labels\n",
    "    df[\"type\"] = [\"Image\"] * len(images) + [\"Text\"] * len(text_descriptions)\n",
    "\n",
    "    # # Create an interactive 2D scatter plot using Plotly\n",
    "    if dim == 2:\n",
    "      fig = px.scatter(df, x=\"x\", y=\"y\", color=\"type\", text=\"label\", title=\"Interactive 2D t-SNE Visualization\")\n",
    "      fig.update_traces(textposition='top center')\n",
    "    else:\n",
    "      fig = px.scatter_3d(df, x=\"x\", y=\"y\", z=\"z\", color=\"type\", text=\"label\", title=\"Interactive 3D t-SNE Visualization\")\n",
    "      fig.update_traces(marker=dict(size=5), textposition='top center')\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rdrW_LB2X-ma"
   },
   "source": [
    "visulise your own input images and text. make sure to set perplexity lower than the number of samples for this. dont forget to increase it again for next part üòÄ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9dtBezO0YHMU"
   },
   "outputs": [],
   "source": [
    "image_urls = [\n",
    "    \"https://t4.ftcdn.net/jpg/00/97/58/97/360_F_97589769_t45CqXyzjz0KXwoBZT9PRaWGHRk5hQqQ.jpg\",   # Cat image\n",
    "    \"https://cdn.pixabay.com/photo/2023/08/18/15/02/dog-8198719_640.jpg\", # Dog image\n",
    "    \"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRI2RLOBO8DYvk8aAUNEs6DJzCJzlgHT7HfAg&s\" # Car image\n",
    "]\n",
    "text_descriptions = [\"a cat\", \"a dog\", \"a car\"]\n",
    "\n",
    "images = preprocess_images(image_urls)\n",
    "\n",
    "# Visualize embeddings interactively with Plotly\n",
    "# visualize_embeddings_plotly(images, text_descriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jfTFLoAxX12H",
    "outputId": "bc76aefd-ef93-41cc-a2db-33650a275e78"
   },
   "outputs": [],
   "source": [
    "# @title CIFAR10 dataset\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets.cifar import CIFAR10\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Load the CIFAR-10 training dataset\n",
    "dataset = CIFAR10(root='./data', train=True, download=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OYuXOLozX3bB"
   },
   "source": [
    "visulise on CIFAR10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "id": "cOXpoaj6Z1qf",
    "outputId": "1f8ac7c2-36c0-46b7-ada2-4e2d29c472ed"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "classes = ['a photo of a plane', 'a photo of a car', 'a photo of abird', 'a photo of a cat',\n",
    "           'a photo of a deer', 'a photo of a dog', 'a photo of a frog', 'a photo of a horse', 'a photo of a ship', 'a photo of a truck']\n",
    "\n",
    "random_indices = random.sample(range(len(dataset)), 200)\n",
    "preprocessed_images = torch.stack([preprocess(dataset[i][0]).to(device) for i in random_indices])\n",
    "\n",
    "image_labels = [dataset[i][1] for i in random_indices]  # Extract corresponding labels\n",
    "\n",
    "# Map the numeric labels to their corresponding class names\n",
    "text_descriptions = [classes[label] for label in image_labels]\n",
    "\n",
    "# random_lable_indices = random.choices(range(len(classes)), k=64)\n",
    "\n",
    "\n",
    "# image_labels = [classes[i] for i in random_lable_indices]  # Extract corresponding labels\n",
    "\n",
    "visualize_embeddings_plotly(preprocessed_images,text_descriptions, image_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UJDr08nI5E34"
   },
   "source": [
    "## Zero-shot Classification\n",
    "\n",
    "Zero-shot classification is the ability to classify images into categories without having explicitly trained the model on those specific categories. CLIP enables this by understanding images and text in a shared embedding space. This means that once the model has learned general concepts through contrastive learning, it can generalize to entirely new categories just by providing textual labels.\n",
    "\n",
    "This is one of CLIP's most remarkable capabilities‚Äîperforming tasks without being explicitly trained for them.\n",
    "\n",
    "In a traditional model, you would need to fine-tune the model for specific categories. In contrast, CLIP does this out-of-the-box. All you need to do is provide some candidate class names as text and let CLIP predict which one is the most similar to a given image.\n",
    "\n",
    "- Image Representation: The input image is passed through the image encoder to get its embedding.\n",
    "\n",
    "- Text Representation: Each of the class labels is passed through the text encoder to get their embeddings.\n",
    "\n",
    "- Similarity Calculation: CLIP computes the cosine similarity between the image embedding and each text embedding.\n",
    "\n",
    "- Prediction: The class label with the highest similarity score is chosen as the predicted label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OYQ0ErkC5s4V"
   },
   "outputs": [],
   "source": [
    "# Zero-shot classification function with CLIP\n",
    "def zero_shot_classification(image_url, class_labels):\n",
    "    # Load and preprocess the image\n",
    "    response = requests.get(image_url)\n",
    "    img = Image.open(BytesIO(response.content))\n",
    "    image = preprocess(img).unsqueeze(0).to(device)\n",
    "\n",
    "    # Tokenize and encode the class labels (text descriptions)\n",
    "    text = clip.tokenize(class_labels).to(device)\n",
    "\n",
    "    # Compute image and text embeddings\n",
    "    with torch.no_grad():\n",
    "        image_features = model.encode_image(image)\n",
    "        text_features = model.encode_text(text)\n",
    "\n",
    "    # Compute cosine similarity between image and text embeddings\n",
    "    image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "    similarity_scores = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "\n",
    "    # Output the most likely class\n",
    "    class_probabilities = similarity_scores[0]\n",
    "    best_idx = class_probabilities.argmax().item()\n",
    "\n",
    "    return class_labels[best_idx], class_probabilities[best_idx].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hEPunA985vWd"
   },
   "outputs": [],
   "source": [
    "# # Try it out with a sample image and custom categories\n",
    "# image_url = input(\"Enter the URL of an image: \")\n",
    "\n",
    "# # Define class labels (text descriptions)\n",
    "# class_labels = [\"a dog\", \"a cat\", \"a car\", \"a person\", \"a bird\"]\n",
    "\n",
    "# # Perform zero-shot classification\n",
    "# predicted_class, probability = zero_shot_classification(image_url, class_labels)\n",
    "\n",
    "# # Display results\n",
    "# print(f\"\\nPredicted Class: {predicted_class}\")\n",
    "# print(f\"Confidence: {probability:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JcKkaNQr57ae"
   },
   "outputs": [],
   "source": [
    "# Example: Classifying an image of a sports event\n",
    "# sports_categories = [\"soccer\", \"basketball\", \"tennis\", \"swimming\", \"cycling\"]\n",
    "\n",
    "# # Use an image of a sports event\n",
    "# image_url = \"https://static.vecteezy.com/system/resources/thumbnails/027/829/023/small_2x/close-up-of-many-soccer-players-kicking-a-football-on-a-field-competition-scene-created-with-generative-ai-technology-free-photo.jpg\"\n",
    "\n",
    "# # Perform zero-shot classification\n",
    "# predicted_class, probability = zero_shot_classification(image_url, sports_categories)\n",
    "\n",
    "# # Display results\n",
    "# print(f\"\\nPredicted Class: {predicted_class}\")\n",
    "# print(f\"Confidence: {probability:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FD1Fe4cNjTks"
   },
   "source": [
    "# Some Analysis reported by the paper:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ct4w77vXiuj9"
   },
   "source": [
    "### problems addressed by CLIP:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iv9hLFuTh0Le"
   },
   "source": [
    "CLIP addresses several key challenges in the traditional deep learning approach to computer vision:\n",
    "\n",
    "Costly datasets: Traditional vision models require large, manually labeled datasets like ImageNet, which is expensive to create. CLIP avoids this by learning from publicly available text-image pairs, reducing the need for costly, labeled data.\n",
    "\n",
    "Limited adaptability: Standard models like those trained on ImageNet are restricted to predefined tasks (e.g., 1000 categories). CLIP, however, can adapt to various visual tasks without additional training, simply by providing relevant text prompts for the task's concepts.\n",
    "\n",
    "Poor real-world performance: Vision models often perform well on benchmarks but struggle in real-world applications due to overfitting to benchmark data. CLIP performs more robustly in real-world settings, as it doesn't require training on specific benchmark datasets, making its performance more generalizable. Testing has shown CLIP's performance remains consistent across multiple datasets, unlike models that \"study\" for benchmarks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8AstkyT7gfRn"
   },
   "source": [
    "By not directly optimizing for the benchmark, CLIP becomes much more representative: CLIPs system closes this ‚Äúrobustness gap‚Äù by up to 75% while matching the performance of the original ResNet-507 on ImageNet(opens in a new window) zero-shot without using any of the original 1.28M labeled examples.\n",
    "\n",
    "![comarison with resnet on imagenet](https://blog.lancedb.com/content/images/2024/07/Untitled.png)\n",
    "\n",
    "Although both models have the same accuracy on the ImageNet test set, CLIP‚Äôs performance is much more representative of how it will fare on datasets that measure accuracy in different, non-ImageNet settings. For instance, ObjectNet checks a model‚Äôs ability to recognize objects in many different poses and with many different backgrounds inside homes while ImageNet Rendition and ImageNet Sketch check a model‚Äôs ability to recognize more abstract depictions of objects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bs2D5e2_jR5s"
   },
   "source": [
    "### Key Takeaways:\n",
    "\n",
    "1. **CLIP is highly efficient**:\n",
    "   - CLIP trains on highly varied and noisy data in a zero-shot manner, similar to GPT-2 and GPT-3, but required significant compute to achieve strong performance. To reduce compute costs, two key algorithmic choices were made:\n",
    "     - **Contrastive objective**: Connecting text and images through a contrastive learning approach proved 4x to 10x more efficient than image-to-text methods.\n",
    "     - **Vision Transformer (ViT)**: Adopting ViT resulted in a further 3x gain in efficiency over traditional ResNet models.\n",
    "   - With these optimizations, the best CLIP model was trained on 256 GPUs for 2 weeks, comparable to other large-scale models.\n",
    "\n",
    "2. **CLIP is flexible and general**:\n",
    "   - CLIP learns a wide range of visual concepts from natural language, making it more flexible than models trained on datasets like ImageNet.\n",
    "   - CLIP demonstrated strong **zero-shot performance** across 30+ datasets, covering tasks like fine-grained object classification, geo-localization, action recognition, and even OCR (optical character recognition), which traditional models struggle with.\n",
    "   - In a linear probe evaluation, the best CLIP model outperformed the top ImageNet model (Noisy Student EfficientNet-L2) on 20 out of 26 transfer datasets. This underscores CLIP‚Äôs generalization capabilities across different tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eQ5blQTFkqKx"
   },
   "source": [
    "# Linear Probe\n",
    "A linear probe is a method used in representation learning to evaluate the quality of learned representations. In this context, the linear probe involves freezing the pretrained model (such as CLIP) and training a simple linear classifier on top of its output features to solve a specific task (e.g., classification).\n",
    "\n",
    "- Evaluation of Pretrained Representations: Linear probing helps assess how much information is retained in the representations learned by the CLIP model.\n",
    "- Efficient Transfer Learning: Instead of fine-tuning the whole CLIP model on a new dataset, a linear classifier is trained on top of the frozen CLIP features, saving compute time and resources.\n",
    "- Generalization: If a linear probe performs well, it indicates that the representations are rich enough to solve tasks beyond what the model was trained on (i.e., zero-shot generalization)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_4E--Auxk7EX"
   },
   "source": [
    "Lets try this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DDxfyMcBk8_K",
    "outputId": "188baec2-cafe-4029-dc5e-8b6a01c45564"
   },
   "outputs": [],
   "source": [
    "!pip install transformers torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G1RY14WzlAbm"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "# Load the pretrained CLIP model and processor\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lgSiOcoSlEfJ",
    "outputId": "54f406ab-2665-4e22-cd1f-d1c3f4ffabb4"
   },
   "outputs": [],
   "source": [
    "# @title prepare dataset\n",
    "\n",
    "# Load the CIFAR-10 dataset\n",
    "transform = T.Compose([\n",
    "    T.Resize((224, 224)),  # Resize to CLIP's input size\n",
    "    T.ToTensor()\n",
    "])\n",
    "\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, transform=transform, download=True)\n",
    "subset_indices = torch.randperm(len(train_dataset))[:500]  # Take only 500 samples for faster processing\n",
    "subset_dataset = torch.utils.data.Subset(train_dataset, subset_indices)\n",
    "train_loader = torch.utils.data.DataLoader(subset_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, transform=transform, download=True)\n",
    "subset_indices_test = torch.randperm(len(test_dataset))[:200]  # Take only 200 samples for faster processing\n",
    "subset_dataset_test = torch.utils.data.Subset(test_dataset, subset_indices_test)\n",
    "test_loader = torch.utils.data.DataLoader(subset_dataset_test, batch_size=64, shuffle=False)\n",
    "\n",
    "# train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "# test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vXz6VIYqllXL"
   },
   "source": [
    "Now we extract image features using CLIP's visual encoder. These features will serve as inputs for the linear classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E4VF-ggilTKV",
    "outputId": "e8a38c5e-9e26-45ce-8f74-e7dc7da6aaa4"
   },
   "outputs": [],
   "source": [
    "def extract_clip_features(loader, model):\n",
    "    all_features = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            images = images.to(device)\n",
    "            # Process images to get CLIP features\n",
    "            features = model.get_image_features(images)\n",
    "            all_features.append(features.cpu())\n",
    "            all_labels.append(labels)\n",
    "\n",
    "            print(f\"Extracted features for batch\")\n",
    "\n",
    "    all_features = torch.cat(all_features)\n",
    "    all_labels = torch.cat(all_labels)\n",
    "\n",
    "    return all_features, all_labels\n",
    "\n",
    "# Extract features for training and test sets\n",
    "train_features, train_labels = extract_clip_features(train_loader, clip_model)\n",
    "test_features, test_labels = extract_clip_features(test_loader, clip_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cOVgw3LYlvag",
    "outputId": "2a83bf39-8d35-4239-fb64-6f43a6df7f1a"
   },
   "outputs": [],
   "source": [
    "# Define a simple linear classifier (logistic regression)\n",
    "class LinearClassifier(torch.nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super(LinearClassifier, self).__init__()\n",
    "        self.fc = torch.nn.Linear(input_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "# Initialize the classifier\n",
    "input_dim = train_features.shape[1]  # CLIP embedding dimension\n",
    "num_classes = 10  # CIFAR-10 has 10 classes\n",
    "classifier = LinearClassifier(input_dim, num_classes).to(device)\n",
    "\n",
    "# Define loss and optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(classifier.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "def train_classifier(train_features, train_labels, model, criterion, optimizer, epochs=10):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(train_features.to(device))\n",
    "        loss = criterion(outputs, train_labels.to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print(f'Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}')\n",
    "\n",
    "# Train the classifier\n",
    "train_classifier(train_features, train_labels, classifier, criterion, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AWoNQFiXly-0",
    "outputId": "2002e224-93a1-41c8-d268-fc4c0f9bec9d"
   },
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "def evaluate_classifier(test_features, test_labels, model):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(test_features.to(device))\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        accuracy = (predicted == test_labels.to(device)).float().mean().item()\n",
    "    return accuracy\n",
    "\n",
    "accuracy = evaluate_classifier(test_features, test_labels, classifier)\n",
    "print(f'Test Accuracy: {accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sj6esbJSrfgm"
   },
   "source": [
    "## visualising after linear probe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-FxBqL0V6ZLH"
   },
   "source": [
    "## Text-Image Similarity Search with CLIP\n",
    "\n",
    "Text-image similarity search allows us to search for images that are most closely aligned with a given textual description, or vice versa, by comparing the embeddings in a shared space.\n",
    "\n",
    "Let's create an example where, given a text description, CLIP retrieves the best matching image from a set of images. This will demonstrate CLIP's text-to-image retrieval capability.\n",
    "\n",
    "In this section, we will:\n",
    "\n",
    "1. Provide a list of image URLs.\n",
    "2. Provide a text description as a query.\n",
    "3. CLIP will find the image that best matches the given text description based on cosine similarity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2h6_3PkW7NRq"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import torch\n",
    "\n",
    "# Function to fetch and preprocess images\n",
    "def preprocess_images(image_urls):\n",
    "    images = []\n",
    "    for url in image_urls:\n",
    "        response = requests.get(url)\n",
    "        img = Image.open(BytesIO(response.content))\n",
    "        image = preprocess(img).unsqueeze(0).to(device)\n",
    "        images.append(image)\n",
    "    return torch.cat(images)\n",
    "\n",
    "# Function for text-to-image search\n",
    "def text_to_image_search(text_query, image_urls):\n",
    "    # Preprocess the images\n",
    "    images = preprocess_images(image_urls)\n",
    "\n",
    "    # Encode the images\n",
    "    with torch.no_grad():\n",
    "        image_features = model.encode_image(images)\n",
    "\n",
    "    # Tokenize and encode the text query\n",
    "    text = clip.tokenize([text_query]).to(device)\n",
    "    with torch.no_grad():\n",
    "        text_features = model.encode_text(text)\n",
    "\n",
    "    # Normalize the embeddings\n",
    "    image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    # Compute similarity between text and each image\n",
    "    similarities = (100.0 * image_features @ text_features.T).squeeze(1)\n",
    "\n",
    "    # Find the best matching image\n",
    "    best_match_idx = similarities.argmax().item()\n",
    "    best_similarity_score = similarities[best_match_idx].item()\n",
    "\n",
    "    return image_urls[best_match_idx], best_similarity_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sBBMTqEr7aep"
   },
   "outputs": [],
   "source": [
    "# Example: Set of image URLs to search from\n",
    "image_urls = [\n",
    "    \"https://upload.wikimedia.org/wikipedia/commons/3/3a/Cat03.jpg\",    # Cat image\n",
    "    \"https://upload.wikimedia.org/wikipedia/commons/6/62/Dog_face.png\",  # Dog image\n",
    "    \"https://upload.wikimedia.org/wikipedia/commons/9/9a/Car-Toyota.jpg\",# Car image\n",
    "]\n",
    "\n",
    "# Example text query\n",
    "text_query = \"a picture of a dog\"\n",
    "\n",
    "# Perform text-to-image search\n",
    "best_image_url, similarity_score = text_to_image_search(text_query, image_urls)\n",
    "\n",
    "# Show the result\n",
    "print(f\"Best Matching Image URL: {best_image_url}\")\n",
    "print(f\"Similarity Score: {similarity_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wefumXLR9hoV"
   },
   "source": [
    "## Fine Tuning\n",
    "\n",
    "While CLIP is very effective out-of-the-box, you may want to fine-tune it on a specific dataset for specialized tasks. Fine-tuning CLIP on a domain-specific dataset can help it learn the nuances of that domain, thereby improving performance on domain-specific tasks.\n",
    "\n",
    "Fine-tuning involves continuing to train the model on a smaller, more specialized dataset (related to your domain) after its initial pretraining on a large general dataset. Fine-tuning can help CLIP:\n",
    "\n",
    "Focus on specific visual-text relationships relevant to a given domain.\n",
    "Adjust its embedding space for tasks with specific characteristics (e.g., medical images, satellite images).\n",
    "\n",
    "Here‚Äôs a high-level outline of the steps you would take to fine-tune CLIP on a custom dataset:\n",
    "\n",
    "- Dataset Preparation: You need a dataset of image-text pairs related to the task at hand. The dataset should have labeled images and corresponding textual descriptions.\n",
    "\n",
    "- Modify CLIP for Fine-Tuning: During fine-tuning, it is often beneficial to freeze the early layers of the model that capture general features. We‚Äôll only update the higher layers that are more specific to the task.\n",
    "\n",
    "- Training Objective: The objective remains similar to CLIP‚Äôs original training‚Äîcontrastive loss. We'll minimize the loss between matched image-text pairs while maximizing the loss between mismatched pairs.\n",
    "\n",
    "- Training Loop: The training loop involves calculating contrastive loss between image and text embeddings (like in the original CLIP training) and updating the model‚Äôs weights using a smaller learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xhCPdB43_KN7"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import clip\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HpAjz2Gp_NqR"
   },
   "source": [
    "prepare dataset and dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oEVqjGRb_P2_"
   },
   "outputs": [],
   "source": [
    "# Custom dataset class\n",
    "class CustomImageTextDataset(Dataset):\n",
    "    def __init__(self, image_urls, text_descriptions, preprocess):\n",
    "        self.image_urls = image_urls\n",
    "        self.text_descriptions = text_descriptions\n",
    "        self.preprocess = preprocess\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_urls)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Fetch image\n",
    "        response = requests.get(self.image_urls[idx])\n",
    "        img = Image.open(BytesIO(response.content))\n",
    "        img = self.preprocess(img)\n",
    "\n",
    "        # Fetch text\n",
    "        text = clip.tokenize([self.text_descriptions[idx]])[0]\n",
    "\n",
    "        return img, text\n",
    "\n",
    "# Simulated image URLs and text descriptions\n",
    "image_urls = [\n",
    "    \"https://upload.wikimedia.org/wikipedia/commons/3/3a/Cat03.jpg\",\n",
    "    \"https://upload.wikimedia.org/wikipedia/commons/6/62/Dog_face.png\",\n",
    "    \"https://upload.wikimedia.org/wikipedia/commons/9/9a/Car-Toyota.jpg\"\n",
    "]\n",
    "text_descriptions = [\"a cat\", \"a dog\", \"a car\"]\n",
    "\n",
    "# Initialize the dataset and dataloader\n",
    "dataset = CustomImageTextDataset(image_urls, text_descriptions, preprocess)\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5lx1ZRKu_IbM"
   },
   "outputs": [],
   "source": [
    "# Load CLIP model and optimizer\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Freeze the earlier layers if desired (optional)\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Unfreeze the last few layers\n",
    "for param in model.visual.transformer.resblocks[-1].parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# Define optimizer (fine-tuning specific parameters)\n",
    "optimizer = optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-5)\n",
    "\n",
    "# Fine-tuning loop\n",
    "num_epochs = 5\n",
    "\n",
    "def fine_tune_clip(dataloader, model, optimizer, num_epochs):\n",
    "    model.train()  # Set model to training mode\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for images, texts in dataloader:\n",
    "            images = images.to(device)\n",
    "            texts = texts.to(device)\n",
    "\n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass: get image and text features\n",
    "            image_features = model.encode_image(images)\n",
    "            text_features = model.encode_text(texts)\n",
    "\n",
    "            # Normalize features\n",
    "            image_features = F.normalize(image_features, p=2, dim=-1)\n",
    "            text_features = F.normalize(text_features, p=2, dim=-1)\n",
    "\n",
    "            # Compute contrastive loss\n",
    "            logits_per_image = image_features @ text_features.T\n",
    "            logits_per_text = text_features @ image_features.T\n",
    "\n",
    "            # Targets are diagonal (correct image-text pairs)\n",
    "            targets = torch.arange(len(images), device=images.device)\n",
    "\n",
    "            # Compute contrastive loss in both directions\n",
    "            loss_image = F.cross_entropy(logits_per_image, targets)\n",
    "            loss_text = F.cross_entropy(logits_per_text, targets)\n",
    "            loss = (loss_image + loss_text) / 2\n",
    "\n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(dataloader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 315
    },
    "id": "hi27CybX_XYZ",
    "outputId": "bf611fa8-8ea8-4b0b-c202-50a5818532cb"
   },
   "outputs": [],
   "source": [
    "# Perform fine-tuning\n",
    "fine_tune_clip(dataloader, model, optimizer, num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ezprv3X959sv"
   },
   "source": [
    "# References\n",
    "\n",
    "- https://openai.com/index/clip/\n",
    "\n",
    "- https://arxiv.org/abs/2103.00020\n",
    "\n",
    "- ChatGPT helped as well ‚ö°"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

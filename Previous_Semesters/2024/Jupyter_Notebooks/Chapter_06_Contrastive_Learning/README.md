You should work your way through the notebooks in the following order:


# [1. Vision Transformers](https://github.com/SharifiZarchi/Introduction_to_Machine_Learning/blob/main/Jupyter_Notebooks/Chapter_06_Contrastive_Learning/ViT.ipynb)
In this notebook we implement and train a [Vision Transformer (ViT)](https://arxiv.org/abs/2010.11929) using PyTorch.

![model_scheme.pdf](https://github.com/user-attachments/files/17894760/model_scheme.pdf)

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/SharifiZarchi/Introduction_to_Machine_Learning/blob/main/Jupyter_Notebooks/Chapter_06_Contrastive_Learning/ViT.ipynb)
[![Open In kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://raw.githubusercontent.com/SharifiZarchi/Introduction_to_Machine_Learning/main/Jupyter_Notebooks/Chapter_06_Contrastive_Learning/ViT.ipynb)



# [2. Contrastive Language-Image Pre-training](https://github.com/SharifiZarchi/Introduction_to_Machine_Learning/blob/main/Jupyter_Notebooks/Chapter_06_Contrastive_Learning/CLIP.ipynb)
In this notebook we explore how using pre-trained models on images and languages can help improve performance on a variety of different tasks according to the [CLIP](https://openai.com/index/clip/) model.

![main-diagrams.pdf](https://github.com/user-attachments/files/17894792/main-diagrams.pdf)

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/SharifiZarchi/Introduction_to_Machine_Learning/blob/main/Jupyter_Notebooks/Chapter_06_Contrastive_Learning/CLIP.ipynb)
[![Open In kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://raw.githubusercontent.com/SharifiZarchi/Introduction_to_Machine_Learning/main/Jupyter_Notebooks/Chapter_06_Contrastive_Learning/CLIP.ipynb)

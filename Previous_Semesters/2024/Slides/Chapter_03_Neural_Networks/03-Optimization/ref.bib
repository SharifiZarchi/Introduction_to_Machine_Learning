@misc{cs231n,
  author       = {Fei-Fei Li and Jiajun Wu and Ruohan Gao},
  title        = {CS231n: Convolutional Neural Networks for Visual Recognition},
  howpublished = {\textit{Lecture slides}},
  institution  = {Stanford University},
  year         = {2022},
  month        = apr,
  note         = {Available: http://cs231n.stanford.edu/slides/2022}
}

@misc{cmu_lecture,
  author       = {Machine learning for signal processing group},
  title        = {11-785 Introduction to Deep Learning},
  howpublished = {\textit{Lecture slides}},
  institution  = {Carnegie Mellon University},
  year         = {2024},
  note         = {Available: https://deeplearning.cs.cmu.edu/F24/document/slides}
}

@misc{mit_6s191_l1,
  author       = {Alexander Amini},
  title        = {6S191: Introduction to Deep Learning},
  howpublished = {\textit{Lecture slides}},
  institution  = {Massachusetts Institute of Technology},
  year         = {2024},
  note         = {Available: http://introtodeeplearning.com/}
}
@misc{ml_explained,

    title = {Gradient Descent Explained},
    year = {2021},
    howpublished = {\url{https://ml-explained.com/blog/gradient-descent-explained}},
}

@misc{towardsdatascience,
    author = {Lili Jiang},
    title = {A Visual Explanation of Gradient Descent Methods: Momentum, Adagrad, RMSprop, Adam},
    year = {2021},
    howpublished = {\url{https://towardsdatascience.com/a-visual-explanation-of-gradient-descent-methods-momentum-adagrad-rmsprop-adam-f898b102325c}},
}

@misc{skz_dev,
    author = {Sasha Kuznetsov},
    title = {Gradient Descent},
    year = {2021},
    howpublished = {\url{https://blog.skz.dev/gradient-descent}},
}

@misc{goldstein_loss_landscape,
    author = {Tom Goldstein},
    title = {Loss Landscape},
    year = {2021},
    howpublished = {\url{https://github.com/tomgoldstein/loss-landscape}},
}

@misc{youtube_gradient_descent,
    author = {Grant Sanderson},
    title = {Gradient Descent, Animated},
    year = {2017},
    howpublished = {\url{https://www.youtube.com/watch?v=IHZwWFHWa-w}},
}

@misc{laptrinhx,

    title = {Understanding Optimization Algorithms},
    year = {2021},
    howpublished = {\url{https://laptrinhx.com/understanding-optimization-algorithms-3818430905/}},
}

@misc{papers_with_code,

    title = {SGD with Momentum},
    year = {2021},
    howpublished = {\url{https://paperswithcode.com/method/sgd-with-momentum}},
}

@misc{wiki_saddle,

    title = {Saddle point},
    year = {2024},
    howpublished = {\url{https://en.wikipedia.org/wiki/Saddle_point}}
}

@article{kingma2014adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}
@misc{wiki_newton,

    title = {Newton's method in optimization},
    year = {2024},
    howpublished = {\url{https://en.wikipedia.org/wiki/Newton%27s_method_in_optimization}},
}

@misc{gfg_newton_optimization,
    author = {GeeksforGeeks},
    title = {Optimization in Neural Networks and Newton's Method},
    year = {2024},
    howpublished = {\url{https://www.geeksforgeeks.org/optimization-in-neural-networks-and-newtons-method/}},
}

@misc{gfg_ml_optimization,
    author = {GeeksforGeeks},
    title = {Optimization Algorithms in Machine Learning},
    year = {2024},
    howpublished = {\url{https://www.geeksforgeeks.org/optimization-algorithms-in-machine-learning/}},
}

@misc{d2l_adam,
    author = {D2L.ai},
    title = {Adam},
    year = {2024},
    howpublished = {\url{https://d2l.ai/chapter_optimization/adam.html}},
}

@misc{d2l_momentum,
    author = {D2L.ai},
    title = {Momentum},
    year = {2024},
    howpublished = {\url{https://d2l.ai/chapter_optimization/momentum.html}},
}

@misc{umam2017newton,
  author       = {A. Umam},
  title        = {Newton's Method Optimization: Derivation and How It Works},
  year         = {2017},
  url          = {https://ardianumam.wordpress.com/2017/09/27/newtons-method-optimization-derivation-and-how-it-works/},
}

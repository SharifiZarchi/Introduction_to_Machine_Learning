{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce6397ab",
   "metadata": {
    "id": "ce6397ab"
   },
   "source": [
    "<img src=\"./pic/sharif-main-logo.png\" alt=\"SUT logo\" width=345 height=345 align=left class=\"saturate\">\n",
    "\n",
    "\n",
    "<br>\n",
    "<font>\n",
    "<div dir=ltr align=center>\n",
    "<font color=0F5298 size=7>\n",
    "    Machine Learning <br>\n",
    "<font color=2565AE size=5>\n",
    "    Computer Engineering Department <br>\n",
    "    Fall 2024<br>\n",
    "<font color=3C99D size=5>\n",
    "    Practical Assignment 2 - Unsupervised Learning<br>\n",
    "<font color=696880 size=4>\n",
    "    Assignment Supervisor: Niki Sepasian <br>\n",
    "<font color=696880 size=5>\n",
    "    Asemaneh Nafe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73dbbf28",
   "metadata": {
    "id": "73dbbf28"
   },
   "outputs": [],
   "source": [
    "student_number = 'Asemaneh'\n",
    "full_name = 'Nafe'\n",
    "assert student_number and full_name is not None, 'please input your information'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce5d0a7",
   "metadata": {
    "id": "4ce5d0a7"
   },
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "from sklearn.metrics import silhouette_score\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from scipy.cluster.hierarchy import fcluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3551f3e4",
   "metadata": {
    "id": "3551f3e4"
   },
   "source": [
    "<font color=red size=3>\n",
    "notice that you can not use sklearn.decomposition and sklearn.cluster libary in this home work! you should implement pca and kmeans from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f15b487",
   "metadata": {
    "id": "2f15b487"
   },
   "source": [
    "## Overview\n",
    "In this assignment, you will perform PCA and K-Means clustering on credit card customer data. dataset contains information about customer’s use of credit cards. The goal is to reduce the dataset’s dimensionality using PCA and then apply clustering to segment customers. You will compare the clustering performance both before and after PCA. Additionally, you'll be asked to explain the theory and decisions behind each step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d27dee",
   "metadata": {
    "id": "26d27dee"
   },
   "source": [
    "## Data Preprocessing\n",
    "Read the cc_general.CSV file and display a few samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb73370d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bb73370d",
    "outputId": "7443237c-6918-4745-9e1c-47a18911b020"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('dataset.csv')\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f92d77",
   "metadata": {
    "id": "21f92d77"
   },
   "source": [
    "Display dataset information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b68cd6c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3b68cd6c",
    "outputId": "6e416dd9-e285-453d-d96b-e40bf8e5c7a9"
   },
   "outputs": [],
   "source": [
    "print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43f0af2",
   "metadata": {
    "id": "c43f0af2"
   },
   "source": [
    "Which column do you think might be the most irrelevant for PCA and clustering?\n",
    "Answer:\n",
    "Non-numeric data or identifiers, like CUST_ID, should be removed, as they don't carry useful information for these tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b471c9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e1b471c9",
    "outputId": "bc7dcee2-e001-43d5-db8b-e96bbc7b589d"
   },
   "outputs": [],
   "source": [
    "# Exclude irrelevant feature\n",
    "df = df.select_dtypes(include=[float, int])\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0a71f9",
   "metadata": {
    "id": "4c0a71f9"
   },
   "source": [
    "how do you handle missing data, and why did you choose this method?\n",
    "Answer:\n",
    "In this dataset, filling missing values with the median is a good choice because the median is robust to outliers, which are common in financial datasets. Imputing missing values this way helps to retain the overall distribution of the data without significantly distorting it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0105ae83",
   "metadata": {
    "id": "0105ae83"
   },
   "outputs": [],
   "source": [
    "#Fill missing data\n",
    "df.fillna(df.median(), inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371304a8",
   "metadata": {
    "id": "371304a8"
   },
   "source": [
    "plot the correlation matrix and identify redundant features.remove them from the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307fa5f6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "307fa5f6",
    "outputId": "1066ce7d-ead4-410d-9b9a-d16c336f09eb"
   },
   "outputs": [],
   "source": [
    "# Plot the correlation matrix\n",
    "plt.figure(figsize=(30, 28))\n",
    "correlation_matrix = df.corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae98d93",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4ae98d93",
    "outputId": "364430fa-579c-439d-9f9c-536898860858"
   },
   "outputs": [],
   "source": [
    "# Identify and remove redundant features. use 0.8 threshold.\n",
    "correlation_matrix = df.corr()\n",
    "def identify_redundant_features(correlation_matrix, threshold=1):\n",
    "    redundant_features = set()\n",
    "\n",
    "    for i in range(len(correlation_matrix.columns)):\n",
    "        for j in range(i):\n",
    "            if abs(correlation_matrix.iloc[i, j]) > threshold:\n",
    "                colname = correlation_matrix.columns[i]\n",
    "                redundant_features.add(colname)\n",
    "\n",
    "    return redundant_features\n",
    "\n",
    "redundant_features = identify_redundant_features(correlation_matrix, threshold=0.8)\n",
    "print(\"Redundant features:\", redundant_features)\n",
    "df = df.drop(columns=redundant_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c152dc",
   "metadata": {
    "id": "d1c152dc"
   },
   "source": [
    "## Standardize the Data\n",
    "Standardize the dataset using z-score normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c9de44",
   "metadata": {
    "id": "87c9de44"
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da254869",
   "metadata": {
    "id": "da254869"
   },
   "source": [
    "Why is it important to standardize the data before applying PCA?\n",
    "<br>\n",
    "Answer:\n",
    "Standardization is crucial before applying PCA because PCA is sensitive to the scale of the data. Features with larger scales will dominate the principal components, which can skew the analysis. Standardizing ensures that all features contribute equally to the PCA, enabling better dimensionality reduction and interpretation of the principal components.\n",
    "KMeans algorithm may end up giving disproportionately greater weight to features with higher magnitudes and may be unaware of features with lower magnitudes but equally important."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f617fe",
   "metadata": {
    "id": "37f617fe"
   },
   "source": [
    "What is differnce between Normalizer and StandardScaler classes. which is better for PCA?\n",
    "<br>\n",
    "Answer:\n",
    "Normalizer: Normalizes each sample (row) to have a unit norm (L1, L2, or max norm).\n",
    "StandardScaler: Standardizes each feature (column) to have a mean of 0 and a standard deviation of 1 (Z-score normalization).\n",
    "<br>\n",
    "If variance of one variable is higher than others we make the pca components biased in that direction. So, best thing to do is make the variance of all variables the same. One way of doing this is by standardizing all the variables. Normalization does not make all variables to have the same variance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6db37a4",
   "metadata": {
    "id": "c6db37a4"
   },
   "source": [
    "## Principal Component Analysis (PCA)\n",
    "Implement PCA from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7410b3a",
   "metadata": {
    "id": "a7410b3a"
   },
   "outputs": [],
   "source": [
    "class CustomPCA:\n",
    "    def __init__(self, n_components=None):\n",
    "        self.n_components = n_components\n",
    "        self.components = None\n",
    "        self.mean = None\n",
    "        self.explained_variance_ratio = None\n",
    "\n",
    "    def fit(self, X):\n",
    "        self.mean = np.mean(X, axis=0)\n",
    "        X_centered = X - self.mean\n",
    "\n",
    "        covariance_matrix = np.cov(X_centered, rowvar=False)\n",
    "\n",
    "        eigenvalues, eigenvectors = np.linalg.eigh(covariance_matrix)\n",
    "\n",
    "        sorted_indices = np.argsort(eigenvalues)[::-1]\n",
    "        eigenvalues = eigenvalues[sorted_indices]\n",
    "        eigenvectors = eigenvectors[:, sorted_indices]\n",
    "\n",
    "        if self.n_components is None:\n",
    "            self.n_components = X.shape[1]\n",
    "\n",
    "        self.components = eigenvectors[:, :self.n_components]\n",
    "        total_variance = np.sum(eigenvalues)\n",
    "        self.explained_variance_ratio = eigenvalues[:self.n_components] / total_variance\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_centered = X - self.mean\n",
    "        return np.dot(X_centered, self.components)\n",
    "\n",
    "    def get_explained_variance_ratio(self):\n",
    "        return self.explained_variance_ratio\n",
    "\n",
    "    def get_components(self):\n",
    "        return self.components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2507b3",
   "metadata": {
    "id": "0e2507b3"
   },
   "source": [
    "### Visualizing the Cumulative Variance\n",
    "\n",
    "Plot the cumulative explained variance to visualize the selection of components.  How many components are needed to explain 75% of the variance?\n",
    "answer: 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f6e77b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 599
    },
    "id": "f6f6e77b",
    "outputId": "23e65675-b709-4f37-d851-5a1f2069dda5"
   },
   "outputs": [],
   "source": [
    "pca = CustomPCA()\n",
    "pca.fit(scaled_data)\n",
    "per_var = np.round(pca.get_explained_variance_ratio()*100, decimals = 1)\n",
    "print(per_var.cumsum())\n",
    "\n",
    "plt.figure(figsize = (10,6))\n",
    "plt.plot(range(1, len(per_var)+1), per_var.cumsum(), marker = \"o\", linestyle = \"--\")\n",
    "plt.grid()\n",
    "plt.ylabel(\"Percentage Cumulative of Explained Variance\")\n",
    "plt.xlabel(\"Number of Components\")\n",
    "plt.title(\"Explained Variance by Component\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238fb526",
   "metadata": {
    "id": "238fb526"
   },
   "source": [
    "Build a new DataFrame with the first slected components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b3b3ac",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f7b3b3ac",
    "outputId": "fdbbb529-a644-47de-d91f-acac9f18cdb1"
   },
   "outputs": [],
   "source": [
    "#Build a new DataFrame with the first slected components\n",
    "selected_components = 6\n",
    "pca = CustomPCA(selected_components)\n",
    "pca.fit(scaled_data)\n",
    "scores_pca = pca.transform(scaled_data)\n",
    "\n",
    "df_pca = pd.DataFrame(scores_pca, columns=[f'PC{i+1}' for i in range(selected_components)])\n",
    "print(df_pca.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed43b921",
   "metadata": {
    "id": "ed43b921"
   },
   "source": [
    "We expect these new features to be orthogonal to each other. Check this and show the correlation between the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b414fe5f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 699
    },
    "id": "b414fe5f",
    "outputId": "c483ef22-f88c-4218-fa6a-b9e5ce5a1920"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(df_pca.corr(), annot=True, cmap='coolwarm')\n",
    "plt.title('Correlation Matrix of PCA Components')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674be777",
   "metadata": {
    "id": "674be777"
   },
   "source": [
    "## KMeans\n",
    "Implement kmeans from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83a614e",
   "metadata": {
    "id": "f83a614e"
   },
   "outputs": [],
   "source": [
    "class CustomKMeans:\n",
    "    def __init__(self, n_clusters=3, max_iter=100, random_state=42):\n",
    "        self.n_clusters = n_clusters\n",
    "        self.max_iter = max_iter\n",
    "        self.random_state = random_state\n",
    "        self.centroids = None\n",
    "        self.inertia_ = None\n",
    "        self.labels_ = None\n",
    "\n",
    "    def fit(self, X):\n",
    "        np.random.seed(self.random_state)\n",
    "        # Randomly initialize centroids\n",
    "        random_indices = np.random.permutation(X.shape[0])\n",
    "        self.centroids = X[random_indices[:self.n_clusters]]\n",
    "\n",
    "        for i in range(self.max_iter):\n",
    "            # Assign clusters\n",
    "            distances = self._compute_distances(X)\n",
    "            self.labels_ = np.argmin(distances, axis=1)\n",
    "\n",
    "            # Recompute centroids\n",
    "            new_centroids = np.array([X[self.labels_ == j].mean(axis=0) for j in range(self.n_clusters)])\n",
    "\n",
    "            # Stop if centroids don't change\n",
    "            if np.all(self.centroids == new_centroids):\n",
    "                break\n",
    "            self.centroids = new_centroids\n",
    "\n",
    "        # Calculate inertia (WCSS)\n",
    "        self.inertia_ = self._calculate_inertia(X)\n",
    "        return self\n",
    "\n",
    "    def _compute_distances(self, X):\n",
    "        # Compute distances between each point and the centroids\n",
    "        return np.linalg.norm(X[:, np.newaxis] - self.centroids, axis=2)\n",
    "\n",
    "    def _calculate_inertia(self, X):\n",
    "        # Sum of squared distances of samples to their closest cluster center\n",
    "        inertia = 0\n",
    "        for i in range(self.n_clusters):\n",
    "            cluster_points = X[self.labels_ == i]\n",
    "            inertia += np.sum((cluster_points - self.centroids[i])**2)\n",
    "        return inertia"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f5b851",
   "metadata": {
    "id": "d8f5b851"
   },
   "source": [
    "### Elbow Method\n",
    "Apply the elbow method to determine the optimal number of clusters for K-Means. what is the best number of clusters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07562698",
   "metadata": {
    "id": "07562698"
   },
   "outputs": [],
   "source": [
    "#put the seed state 42\n",
    "WCSS = []\n",
    "\n",
    "for i in range(1,30):\n",
    "  kmeans_pca = CustomKMeans(n_clusters = i, random_state = 42)\n",
    "  kmeans_pca.fit(scores_pca)\n",
    "  WCSS.append(kmeans_pca.inertia_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b090a1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "id": "82b090a1",
    "outputId": "83f62fec-77c9-46c4-80db-b412442cf743"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10,6))\n",
    "plt.plot(range(1,30), WCSS, marker = \"o\", linestyle = \"--\")\n",
    "plt.grid()\n",
    "plt.title(\"Cluster using PCA Scores\")\n",
    "plt.ylabel(\"WCSS\")\n",
    "plt.xlabel(\"N Clusters\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e3b3664",
   "metadata": {
    "id": "9e3b3664"
   },
   "source": [
    "Apply the optimal KMeans clustering on the PCA-transformed data, and assign cluster labels to each observation. Add a new column named segment to the df_pca DataFrame to store these labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525a14e2",
   "metadata": {
    "id": "525a14e2"
   },
   "outputs": [],
   "source": [
    "# Apply KMeans on PCA-reduced data\n",
    "kmeans_pca = CustomKMeans(n_clusters = 5, random_state = 42)\n",
    "kmeans_pca.fit(scores_pca)\n",
    "\n",
    "# Seting the cluster label to each observation, using the atribute .labels_\n",
    "df_pca[\"segment\"] = kmeans_pca.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c2ba55",
   "metadata": {
    "id": "f1c2ba55"
   },
   "source": [
    " visualize the clustering by plotting the pairwise relationships of the PCA-reduced features, color-coded by the cluster assignments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d166a9b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "7d166a9b",
    "outputId": "85e65f12-d6d4-4dca-ff8a-08bf6a7e5eb8"
   },
   "outputs": [],
   "source": [
    "sns.pairplot(df_pca[0:], hue='segment', palette='Set2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32bec536",
   "metadata": {
    "id": "32bec536"
   },
   "source": [
    "So, when we employ PCA prior to using K-means we can visually separate almost the entire data set. That was one of the biggest goals of PCA - to reduce the number of variables by combining them into bigger, more meaningful features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8bb61a",
   "metadata": {
    "id": "eb8bb61a"
   },
   "source": [
    "### Hierarchical Clustering\n",
    "Perform hierarchical clustering on the reduced dataset after PCA. use complete linkage method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777cd9d4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 645
    },
    "id": "777cd9d4",
    "outputId": "92d31eb5-92fe-4bed-f177-cf8c7451c16d"
   },
   "outputs": [],
   "source": [
    "# Perform Hierarchical Clustering on the original dataset (first four attributes)\n",
    "Z = linkage(scores_pca, method='complete')\n",
    "\n",
    "# Visualize the dendrogram\n",
    "plt.figure(figsize=(10, 7))\n",
    "dendrogram(Z)\n",
    "plt.title('Hierarchical Clustering Dendrogram (Complete Linkage)')\n",
    "plt.xlabel('Sample Index')\n",
    "plt.ylabel('Distance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc0f296",
   "metadata": {
    "id": "fbc0f296"
   },
   "source": [
    "Use scipy.cluster.hierarchy.fcluster to assign clusters from the dendrogram(use 5 cluster). then visualize the results using pairplots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334fd853",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "334fd853",
    "outputId": "785a4ed0-1a52-46d5-aadf-92a6736af5c5"
   },
   "outputs": [],
   "source": [
    "# Choose distance threshold and assign clusters\n",
    "clusters_hierarchical_pca = fcluster(Z, t=5, criterion='maxclust')\n",
    "\n",
    "# Assign cluster labels to PCA DataFrame\n",
    "df_pca[\"hierarchical_segment\"] = clusters_hierarchical_pca\n",
    "\n",
    "# Visualize using PCA components\n",
    "sns.pairplot(df_pca, hue='hierarchical_segment', palette='Set2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc5b8c5",
   "metadata": {
    "id": "fbc5b8c5"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

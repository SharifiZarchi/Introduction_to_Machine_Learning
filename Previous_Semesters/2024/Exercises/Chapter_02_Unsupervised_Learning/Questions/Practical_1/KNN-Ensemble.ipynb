{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./pic/sharif-main-logo.png\" alt=\"SUT logo\" width=345 height=345 align=left class=\"saturate\">\n",
    "\n",
    "\n",
    "<br>\n",
    "<font>\n",
    "<div dir=ltr align=center>\n",
    "<font color=0F5298 size=7>\n",
    "    Machine Learning <br>\n",
    "<font color=2565AE size=5>\n",
    "    Computer Engineering Department <br>\n",
    "    Fall 2024<br>\n",
    "<font color=3C99D size=5>\n",
    "    Practical Assignment 2 - Unsupervised Learning<br>\n",
    "<font color=696880 size=4>\n",
    "    Assignment Supervisor: Niki Sepasian <br>\n",
    "<font color=696880 size=5>\n",
    "    Sarina Heshmati\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_number = None\n",
    "full_name = None\n",
    "assert student_number and full_name is not None, 'please input your information'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8LgQp-QRPSd3"
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, BaggingClassifier, AdaBoostClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Configure visualizations\n",
    "sns.set(style=\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, you will explore a dataset containing information about employees and use that data to train models capable of predicting whether those employees have left the company (attrited) or not. You will start by exploring the dataset, analyzing its features, and performing necessary preprocessing steps (such as label encoding and scaling). Then, you will build and evaluate implementations of K-Nearest Neighbors (KNN) as well as Random Forest, Bagging, and AdaBoost classifiers. After training and tuning each model, you’ll compare their performance using metrics like accuracy and feature importance to identify the best approach for accurate attrition prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dxij84ZZMICb"
   },
   "source": [
    "# Load and Explore Dataset (10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "# \"Attrition\" is our target columm\n",
    "\n",
    "df = pd.read_csv('dataset.csv')\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explore the dataset and get familiar with its features and statistics. (don't worry about the 'masked values' in our target column. They are simply used to automatically test your model later on.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KKtCXW57PS-k"
   },
   "outputs": [],
   "source": [
    "# TODO: Check the basic structure of the dataset using .info() and .describe()\n",
    "# Use: df.info() to check data types and missing values\n",
    "# Use: df.describe() to get summary statistics of numeric features\n",
    "\n",
    "# TODO: Check for any missing values in the dataset\n",
    "# Use: df.isnull().sum() to find if any column has missing values\n",
    "\n",
    "# TODO: Explore the target variable (binary classification)\n",
    "# Use value_counts() to see the distribution of our target (Attrition) column and then visualize it (bar plot).\n",
    "\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is generally better to remove columns with only one unique value from a DataFrame when preparing data for a decision tree. <br>\n",
    "Such columns do not provide any useful information for splitting the data and can lead to unnecessary complexity in the model. Remove the said columns from the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Measure and print the number of unique values for each column. \n",
    "# Check if there are any columns with less than 2 unique values. If so, remove them. \n",
    "\n",
    "unique_values= {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the DataFrame and try to gather insight into people's monthly income and things that generally affect this number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Plot (lineplot) the average MonthlyIncome against the YearsAtCompany. \n",
    "# TODO: Then find which departments have the highest and lowest incomes on average."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nViqoo1oMNPN"
   },
   "source": [
    "# Data Preprocessing (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Label Encode categorical columns and create a new DataFrame. Then split this data into train and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Label encode all categorical columns\n",
    "encoded_df= None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "joQQVv4TPTXY"
   },
   "outputs": [],
   "source": [
    "# Split into features and target variable\n",
    "X = df.drop(columns=['Attrition'])\n",
    "y = df['Attrition']\n",
    "\n",
    "# TODO: Perform a train-test split using train_test_split() from sklearn\n",
    "# Split the dataset into training and test sets with a test size of 30%\n",
    "\n",
    "# TODO: Scale the features using StandardScaler\n",
    "# Fit the scaler on the training data and transform both the training and test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WoDFp4Y8MR2H"
   },
   "source": [
    "# K-Nearest Neighbors (KNN) Model (15 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement KNN model from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "class CustomKNN:\n",
    "    def __init__(self, k):\n",
    "        \"\"\"\n",
    "        Initialize the KNN classifier.\n",
    "\n",
    "        Parameters:\n",
    "        - k (int): Number of neighbors to consider.\n",
    "        \"\"\"\n",
    "        # Store the number of neighbors (k)\n",
    "        self.k = k\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        \"\"\"\n",
    "        Fit the KNN classifier to the training data.\n",
    "\n",
    "        Parameters:\n",
    "        - X_train (numpy array): Training feature vectors.\n",
    "        - y_train (numpy array): Training labels.\n",
    "        \"\"\"\n",
    "        # Store training data\n",
    "        self.X_train = np.array(X_train)\n",
    "        self.y_train = np.array(y_train)\n",
    "\n",
    "    def euclidean_distance(self, x1, x2):\n",
    "        \"\"\"\n",
    "        Calculate the Euclidean distance between two data points.\n",
    "\n",
    "        Parameters:\n",
    "        - x1 (numpy array): First data point.\n",
    "        - x2 (numpy array): Second data point.\n",
    "\n",
    "        Returns:\n",
    "        - float: Euclidean distance between x1 and x2.\n",
    "        \"\"\"\n",
    "        # TODO: Calculate and return the Euclidean distance\n",
    "        pass\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        \"\"\"\n",
    "        Predict labels for test data.\n",
    "\n",
    "        Parameters:\n",
    "        - X_test (numpy array): Test feature vectors.\n",
    "\n",
    "        Returns:\n",
    "        - numpy array: Predicted labels.\n",
    "        \"\"\"\n",
    "        # TODO: Predict label for each test instance and return the array of predictions\n",
    "        pass\n",
    "\n",
    "    def _predict(self, x):\n",
    "        \"\"\"\n",
    "        Predict label for a single data point.\n",
    "\n",
    "        Parameters:\n",
    "        - x (numpy array): Test data point.\n",
    "\n",
    "        Returns:\n",
    "        - int: Predicted label.\n",
    "        \"\"\"\n",
    "        # TODO: Compute distances from x to all training points.\n",
    "        # Find the indices and labels of k nearest neighbors.\n",
    "        # Perform mafority vote and return the most common label among them.\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit and test your model using different k values and then choose the best one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional. You can choose any range of k values that you want.\n",
    "k_values = [1, 3, 5, 7, 9, 11, 13, 15]\n",
    "accuracies = []\n",
    "\n",
    "\n",
    "\n",
    "for k in k_values:\n",
    "    y_pred_custom= []\n",
    "    \n",
    "    # TODO: Fit the model using the scaled training data\n",
    "    # TODO: Make predictions on the scaled test data\n",
    "    # TODO: Evaluate the model's accuracy for each value of k and choose the best one\n",
    "    \n",
    "    print(f'k: {k} - Accuracy: {accuracy_score(y_test, y_pred_custom)}')\n",
    "    \n",
    "\n",
    "Best_custom_model= None\n",
    "\n",
    "# Keep the best k value (needed later on with bagging)\n",
    "bestk= None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jbb7yn8QPTt0"
   },
   "outputs": [],
   "source": [
    "# TODO: Print the accuracy and classification report using sklearn's metrics for your best model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the confusion matrix for KNN predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JVEF9tO2PUEb"
   },
   "outputs": [],
   "source": [
    "# TODO: Create a confusion matrix for KNN predictions\n",
    "# Use confusion_matrix from sklearn.metrics\n",
    "\n",
    "# TODO: Visualize the confusion matrix using seaborn's heatmap\n",
    "# Add annotations and a title for better readability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation (30 points)\n",
    "In this part, we are going to evaluate your model's performance on another set of unseen data. Load test.csv (this data is already encoded), use your best_custom_model to predict and save the results in a DataFrame called 'result.csv'. The DataFrame should contain one column called 'target' that contains your model's predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test.csv\n",
    "eval_df= pd.read_csv('test.csv')\n",
    "\n",
    "# TODO: Use your old scaler to scale the data\n",
    "# TODO: Predict using your model\n",
    "\n",
    "y_pred_eval= None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the results in a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the results as a csv file\n",
    "result_df= pd.DataFrame()\n",
    "result_df['target']=pd.Series(y_pred_eval)\n",
    "result_df.to_csv('result.csv', index= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vH3_BUvKMbBG"
   },
   "source": [
    "# Random Forest Model (12 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement a random forest model using sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "17m_-ltrPUbg"
   },
   "outputs": [],
   "source": [
    "# TODO: Implement the Random Forest model\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# TODO: Set up a hyperparameter tuning process for Random Forest using GridSearchCV\n",
    "# Suggested parameter grid: {'n_estimators': [100, 200, 300], 'max_depth': [10, 20, None], 'min_samples_split': [2, 5, 10]}\n",
    "\n",
    "# GridSearchCV with Random Forest\n",
    "rf_cv = GridSearchCV(rf, param_grid_rf, cv=5)\n",
    "\n",
    "# TODO: Fit the GridSearchCV on the training data to find the best parameters\n",
    "# Use rf_cv.fit() with the training data\n",
    "\n",
    "# TODO: Use the best Random Forest model for predictions on the test data\n",
    "# Use rf_cv.best_estimator_ and predict()\n",
    "\n",
    "# TODO: Print the Random Forest model accuracy and classification report using sklearn's metrics\n",
    "# Use accuracy_score and classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4ZxEb6pgMfCC"
   },
   "source": [
    "Visualize the confusion matrix for Random Forest predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EDP8A0miPU3N"
   },
   "outputs": [],
   "source": [
    "# TODO: Create a confusion matrix for Random Forest predictions\n",
    "# Use confusion_matrix from sklearn.metrics\n",
    "\n",
    "# TODO: Visualize the confusion matrix using seaborn's heatmap\n",
    "# Add annotations and a title for better readability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A feature importance plot is a visual representation that illustrates the significance of each feature (or variable) in a machine learning model, particularly in the context of supervised learning tasks like classification and regression. Plot the feature importances using a bar plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the feature importances from the trained Random Forest model\n",
    "imortances= best_rf.feature_importances_\n",
    "\n",
    "# TODO: Sort the indices of the importance values in descending order\n",
    "# TODO: Create a DataFrame that contains the feature names and their corresponding importance scores\n",
    "# TODO: Create a bar plot to visualize feature importances using Seaborn\n",
    "\n",
    "# use tick_params and Rotate the x-axis labels for better readability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F2tDZYAuMk5l"
   },
   "source": [
    "# Bagging with KNN (12 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ie5KQ50dPVQY"
   },
   "outputs": [],
   "source": [
    "# TODO: Implement Bagging with KNN\n",
    "# Use BaggingClassifier with KNeighborsClassifier as the base estimator\n",
    "# Here we use the bestk value we found before\n",
    "\n",
    "bagging_knn = BaggingClassifier(KNeighborsClassifier(n_neighbors=bestk), n_estimators=50, random_state=42)\n",
    "\n",
    "# TODO: Fit the BaggingClassifier on the scaled training data\n",
    "# Use bagging_knn.fit() with the training data\n",
    "\n",
    "# TODO: Use the trained Bagging model for predictions on the test data\n",
    "# Use bagging_knn.predict()\n",
    "\n",
    "# TODO: Print the Bagging KNN model accuracy and classification report\n",
    "# Use accuracy_score and classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cDXp4LBDMpVL"
   },
   "source": [
    "Visualize the confusion matrix for Baggin KNN predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_2_fnxpaPVps"
   },
   "outputs": [],
   "source": [
    "# TODO: Create a confusion matrix for Bagging KNN predictions\n",
    "# Use confusion_matrix from sklearn.metrics\n",
    "\n",
    "# TODO: Visualize the confusion matrix using seaborn's heatmap\n",
    "# Add annotations and a title for better readability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pTXD8Ke6MsXI"
   },
   "source": [
    "# AdaBoost Model (12 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QDayPaLTPWDp"
   },
   "outputs": [],
   "source": [
    "# TODO: Implement AdaBoost model\n",
    "adaboost = AdaBoostClassifier(random_state=42)\n",
    "\n",
    "# TODO: Set up a hyperparameter tuning process for AdaBoost using GridSearchCV\n",
    "# Suggested parameter grid: {'n_estimators': [50, 100, 150], 'learning_rate': [0.01, 0.1, 1.0]}\n",
    "\n",
    "# GridSearchCV with AdaBoost\n",
    "adaboost_cv = GridSearchCV(adaboost, param_grid_ada, cv=5)\n",
    "\n",
    "# TODO: Fit the GridSearchCV on the training data to find the best parameters\n",
    "# Use adaboost_cv.fit() with the training data\n",
    "\n",
    "# TODO: Use the best AdaBoost model for predictions on the test data\n",
    "# Use adaboost_cv.best_estimator_ and predict()\n",
    "\n",
    "# TODO: Print the AdaBoost model accuracy and classification report using sklearn's metrics\n",
    "# Use accuracy_score and classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hZkdWW-oMzis"
   },
   "source": [
    "Visualize the confusion matrix for AdaBoost predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xi6bMH4JPWZD"
   },
   "outputs": [],
   "source": [
    "# TODO: Create a confusion matrix for AdaBoost predictions\n",
    "# Use confusion_matrix from sklearn.metrics\n",
    "\n",
    "# TODO: Visualize the confusion matrix using seaborn's heatmap\n",
    "# Add annotations and a title for better readability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ORGr2kDzM3FS"
   },
   "source": [
    "# Model Comparison (4 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v3o7CNF7PXFY"
   },
   "outputs": [],
   "source": [
    "# TODO: Compare model accuracies for KNN, Random Forest, Bagging KNN, and AdaBoost\n",
    "# Create a DataFrame with model names and their respective accuracies\n",
    "\n",
    "# TODO: Visualize the model comparison using a line plot\n",
    "# Use seaborn's lineplot to plot model names vs. accuracies"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Introduction**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, you will work with a dataset that includes information about the cost of treatment of different patients. The cost of treatment depends on many factors: diagnosis, type of clinic, city of residence, age and so on. We have no data on the diagnosis of patients. \n",
    "\n",
    "Columns\n",
    "\n",
    "age: age of primary beneficiary\n",
    "\n",
    "sex: insurance contractor gender, female, male\n",
    "\n",
    "bmi: Body mass index, providing an understanding of body, weights that are relatively high or low relative to height,\n",
    "objective index of body weight (kg / m ^ 2) using the ratio of height to weight, ideally 18.5 to 24.9\n",
    "\n",
    "children: Number of children covered by health insurance / Number of dependents\n",
    "\n",
    "smoker: Smoking\n",
    "\n",
    "region: the beneficiary's residential area in the US, northeast, southeast, southwest, northwest.\n",
    "\n",
    "charges: Individual medical costs billed by health insurance\n",
    "\n",
    "First, you will start by fitting a basic regression model using scikit-learn (sklearn) to establish a baseline for comparison. This basic regression model will serve as a reference point for evaluating the performance of more sophisticated models incorporating regularization techniques.\n",
    "\n",
    "Furthermore, you will apply L1 (Lasso) and L2 (Ridge) regularization techniques to refine your predictions and evaluate the impact of these methods on the accuracy of your results. Regularization is a technique used to prevent overfitting by adding a penalty term to the loss function, encouraging simpler models with smaller coefficients. L1 regularization (Lasso) encourages sparsity by penalizing the absolute values of coefficients, while L2 regularization (Ridge) penalizes the square of coefficients. By incorporating these regularization techniques, you aim to improve the generalization performance of your regression models and obtain more robust predictions of house prices in the Boston area."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Personal Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your student number\n",
    "student_number = ''\n",
    "Name = ''\n",
    "Last_Name = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_percentage_error\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, PolynomialFeatures\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from joblib import dump, load\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Explore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the dataset (as a dataframe) using pandas and display the top 5 rows of the dataframe and then check for missing values and impute missing values with mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"./InsuranceData.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get a brief description and do some EDA to get familiar with the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: you can use .info() and .description()\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['region'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: apply any pre processing method you think is necessary, drop salary and convert to numpy array\n",
    "# Options: Normalization, Standardization, Outlier Detection, Imputation, etc.\n",
    "\n",
    "df['sex'] = LabelEncoder().fit_transform(df['sex'])\n",
    "df['smoker'] = LabelEncoder().fit_transform(df['smoker'])\n",
    "\n",
    "df['sex'] = df['sex'].astype(bool)\n",
    "df['smoker'] = df['smoker'].astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region = pd.get_dummies(df['region'])\n",
    "df = pd.concat([df, region], axis = 1)\n",
    "df.drop('region', axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.fillna(df.mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 10))\n",
    "plt.title(\"Boxplot of the Columns (Features)\")\n",
    "\n",
    "X = df[['bmi', 'age', 'children']]\n",
    "\n",
    "X.boxplot()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "Q1 = X.quantile(0.25)\n",
    "Q3 = X.quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "outliers = ((X < (Q1 - 1.5 * IQR)) | (X > (Q3 + 1.5 * IQR))).any(axis=1).sum()\n",
    "\n",
    "\n",
    "if outliers > 0:\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    print(f\"Total of {outliers} outliers detected. Data has been scaled.\")\n",
    "    print(X_scaled[:5])\n",
    "else:\n",
    "    print(\"No outliers detected.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Split the dataset into two parts such that the training set contains 80% of the samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('charges', axis = 1)\n",
    "y = df['charges']\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, train_size=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = StandardScaler()\n",
    "x_train = ss.fit_transform(x_train)\n",
    "x_test = ss.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit a linear regressor to the data. (Use sklearn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Use sklearn\n",
    "lr = LinearRegression()\n",
    "lr.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the coefficients of the variables and visualize it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: \n",
    "def visualize_coef(model, label, color):\n",
    "    print(\"Coefficients of the variables in the sklearn: \", model.coef_)\n",
    "\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    plt.bar(range(len(model.coef_)), model.coef_, label=label, color=color, alpha=0.5)\n",
    "    plt.xlabel(\"Coefficient Index\")\n",
    "    plt.ylabel(\"Coefficient Value\")\n",
    "    plt.title(\"Comparison of Coefficients\")\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "visualize_coef(lr, \"lr\", \"r\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the score value of sklearn regressor on train dataset</br>\n",
    "if you are not familiar with R-squared concept see the link below:\n",
    "[R-squared](https://statisticsbyjim.com/regression/interpret-r-squared-regression/)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Calculate R² score and MSE on the training dataset\n",
    "# TODO: Calculate R² score and MSE on the training dataset\n",
    "def calc_scores(model, X, y):\n",
    "    y_pred = model.predict(X)\n",
    "    r2 = r2_score(y, y_pred)\n",
    "    mse = mean_squared_error(y, y_pred)\n",
    "    return r2, mse\n",
    "\n",
    "r2, mse = calc_scores(lr, x_train, y_train)\n",
    "\n",
    "print(\"R² score on the training dataset: \", r2)\n",
    "print(\"MSE on the training dataset: \", mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L1 regularization, also known as Lasso (Least Absolute Shrinkage and Selection Operator) regularization, is a technique used in regression models that encourages simplicity and sparsity in the model coefficients. This is achieved by adding a penalty equal to the absolute value of the magnitude of coefficients to the loss function.\\\n",
    "</br>\n",
    "Train a regression model using L1 regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Use Lasso from sklearn library\n",
    "\n",
    "lasso_lr = Lasso(alpha=0.5)\n",
    "lasso_lr.fit(x_train, y_train)\n",
    "\n",
    "\n",
    "lasso_lr_low_alpha = Lasso(alpha=0.1)\n",
    "lasso_lr_low_alpha.fit(x_train, y_train)\n",
    "\n",
    "\n",
    "lasso_lr_high_alpha = Lasso(alpha=2)\n",
    "lasso_lr_high_alpha.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the coefficients of the variables and visualize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_coef(lasso_lr, \"lasso, alpha=0.5\", \"b\")\n",
    "visualize_coef(lasso_lr_low_alpha, \"lasso, alpha=0.1\", \"b\")\n",
    "visualize_coef(lasso_lr_high_alpha, \"lasso, alpha=2\", \"b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a regression model using L2 regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Use Ridge from sklearn library\n",
    "Ridge_lr = Ridge(alpha=0.5)\n",
    "Ridge_lr.fit(x_train, y_train)\n",
    "\n",
    "\n",
    "Ridge_lr_low_alpha = Ridge(alpha=0.1)\n",
    "Ridge_lr_low_alpha.fit(x_train, y_train)\n",
    "\n",
    "\n",
    "Ridge_lr_high_alpha = Ridge(alpha=2)\n",
    "Ridge_lr_high_alpha.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_coef(Ridge_lr, \"Ridge, alpha=0.5\", \"b\")\n",
    "visualize_coef(Ridge_lr_low_alpha, \"Ridge, alpha=0.1\", \"b\")\n",
    "visualize_coef(Ridge_lr_high_alpha, \"Ridge, alpha=2\", \"b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test different regularization parameters (alpha) using cross validation. Use MAPE for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Use folding methods from sklearn library\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "alphas = [0.1, 1, 10, 100]\n",
    "results = {}\n",
    "\n",
    "for alpha in alphas:\n",
    "    model = Ridge(alpha=alpha)\n",
    "    scores = cross_val_score(model, X, y, cv=kf, scoring='neg_mean_absolute_percentage_error')\n",
    "    results[alpha] = np.mean(np.abs(scores))\n",
    "\n",
    "print(\"Cross-validated MSE for different alphas:\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "alphas = [0.1, 1, 10, 100]\n",
    "results = {}\n",
    "\n",
    "for alpha in alphas:\n",
    "    model = Lasso(alpha=alpha)\n",
    "    scores = cross_val_score(model, X, y, cv=kf, scoring='neg_mean_absolute_percentage_error')\n",
    "    results[alpha] = np.mean(np.abs(scores))\n",
    "\n",
    "print(\"Cross-validated MSE for different alphas:\", results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add extra variables to dataset to make model more complex, then compare the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Increase No. of dimensions using PolynomialFeatures from sklearn \n",
    "poly_features = PolynomialFeatures(degree=2)\n",
    "X_poly = poly_features.fit_transform(X)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X_poly, y, train_size=0.8)\n",
    "ss = StandardScaler()\n",
    "x_train = ss.fit_transform(x_train)\n",
    "x_test = ss.transform(x_test)\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(x_train, y_train)\n",
    "mean_absolute_percentage_error(y_test, lr.predict(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:     \n",
    "visualize_coef(lr, \"lr\", \"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso = Lasso(alpha=10)\n",
    "lasso.fit(x_train, y_train)\n",
    "mean_absolute_percentage_error(y_test, lasso.predict(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_coef(lasso, \"lasso\", \"b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ridge = Ridge(alpha=10)\n",
    "Ridge.fit(x_train, y_train)\n",
    "mean_absolute_percentage_error(y_test, Ridge.predict(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_coef(Ridge, \"Ridge\", \"b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Report your best model with its evaluated results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Compare the weight distribution when applying L1/L2 regularization and their sparsity?\n",
    "\n",
    "2. How does the regularization parameter (alpha) affect each feature? Does it help to model's explainability?\n",
    "\n",
    "3. How does the regularization affect dimension expansion?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. As we know L1 regularization's main feature is sparsity. It means that it pushes the coefficients of the linear regression to be zero. We can see that feature just by comparing the plots of the normal LR and the Lasso. Many of the coefficients in the Lasso are zero or at least near zero (As we are adding a penalty equal to the absolute value of the coefficients, our model tends to have smaller coefficients and thus we'll have many zero values between them). \n",
    "\n",
    "    In the first glance we can see the difference between their sparsity. When we use the Lasso our coefficients have more sparsity and most of them are zero or at least so close to zero, but in the Ridge our coefficients are not forced to be exactly zero, they're just pushed to be small and not necessarily zero. This also indicates a feature selection act that is done by the Lasso. Lasso tends to kinda ignore the less important features and gives them zero coefficients and by that it acts as a feature selector, but the Ridge doesn't act like that and gives all of the features their corresponding coefficients to just ensure that the L2 norm is small.\n",
    "\n",
    "    So basically their key differences are the sparsity that Lasso has compared to the Ridge (Lasso pushes the coefficients to be small and mostly zero, but Ridge just pushes them to be small) and also the feature selection act that Lasso has (and Ridge doesn't have) when dealing w/ the features.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. As alpha increases, the coefficients of features are penalized more heavily. In Ridge regression (L2 regularization), all coefficients are shrunk but remain non-zero, while in Lasso regression (L1 regularization), some coefficients can be completely reduced to zero, effectively performing feature selection. This means that different features can be affected differently based on their relevance to the target variable and their correlation with other features.\n",
    "\n",
    "    By shrinking coefficients, especially in Lasso regression, regularization can enhance model interpretability. A model with fewer non-zero coefficients is easier to explain and understand since it highlights only the most relevant features. This is particularly beneficial in high-dimensional datasets where many features may contribute little to the predictive power.\n",
    "\n",
    "    Models with appropriate regularization tend to produce more stable coefficient estimates across different datasets or samples. This stability aids in understanding which features consistently contribute to predictions, enhancing overall explainability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Regularization helps control overfitting by penalizing large coefficients in the model. In high-dimensional spaces, models can easily become overly complex and fit noise rather than the underlying data structure. By introducing a penalty term (controlled by alpha), regularization restricts the complexity of the model, effectively managing the dimensionality of the feature space.\n",
    "\n",
    "    Ridge regression (L2 regularization) does not eliminate coefficients but shrinks them towards zero. This shrinkage stabilizes coefficient estimates, particularly in high-dimensional settings where multicollinearity may be present. Stable coefficients mean that small changes in the data do not lead to large fluctuations in the model, which is crucial when dealing with expanded dimensions.\n",
    "\n",
    "    Regularization introduces bias into the model but reduces variance by limiting how much coefficients can change based on fluctuations in the training data. This tradeoff is essential when working with high-dimensional data, as it helps maintain predictive performance while controlling for overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

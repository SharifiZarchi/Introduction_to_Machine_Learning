{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./pic/sharif-main-logo.png' alt=\"SUT logo\" width=300 height=300 align=left class=\"saturate\" > <br>\n",
    "\n",
    "<div dir=ltr align=center>\n",
    "<font color=0F5298 size=7>\n",
    "    Machine Learning <br>\n",
    "<font color=2565AE size=5>\n",
    "    Computer Engineering Department <br>\n",
    "    Fall 2024 <br>\n",
    "<font color=3C99D size=5>\n",
    "    Practical Assignment 1 - Supervised Learning <br>\n",
    "<font color=696880 size=4>\n",
    "    Erfan Salima, Nikan Vasei, Romina Shiri, Sarina Heshmati\n",
    "\n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Polynomial Regression & Bias-Variance Tradeoff (100 Points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "Author: Nikan Vasei"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "In this part, we will explore the **bias-variance tradeoff** using polynomial regression models. Polynomial regression allows us to model complex relationships between an independent variable $X$ and a dependent variable $y$ by introducing higher-degree polynomial terms. However, increasing the complexity of the model can lead to **overfitting** or **underfitting**, which are key concepts in the bias-variance tradeoff.\n",
    "\n",
    "## Plan\n",
    "1. **Create a synthetic dataset**: Use the provided quadratic function to generate $X$ and $y$ with noise.\n",
    "2. **Split the dataset**: Divide the data into training and test sets to evaluate model generalization.\n",
    "3. **Train polynomial models**: Fit polynomial regression models with degrees ranging from 1 to 10 (or more).\n",
    "4. **Evaluate performance**: Calculate the MSE for both training and test sets and compare the errors for different polynomial degrees.\n",
    "5. **Analyze the results**: Use plots to visualize how the error on the training and test sets changes as the model complexity increases.\n",
    "\n",
    "## Objective\n",
    "The objective is to determine the degree of the polynomial that best models the relationship between $X$ and $y$ and balances both underfitting and overfitting, i.e., the degree that achieves the best generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Create a synthetic dataset (15 Points)\n",
    "\n",
    "Your first task is to define a function that generates a dataset with 1000 samples. The input values for the independent variable $X$ should be randomly selected within the range of **-3 to 3**. To make the dataset more realistic, add noise to the output using a normal distribution with a mean of 0 and a standard deviation of 10.\n",
    "\n",
    "The relationship between the independent variable $X$ and the dependent variable $y$ is modeled by the following equation:\n",
    "\n",
    "$$\n",
    "y = 0.2X^6 - 0.5X^5 + 2X^4 - 5X^3 + 3X^2 - 10X + 5 + \\text{noise}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset(range_start, range_end, num_samples=1000, noise_std=10.0):\n",
    "    # TODO: Generate dataset\n",
    "    X = np.random.uniform(range_start, range_end, num_samples)\n",
    "    noise = np.random.normal(0, noise_std, num_samples)\n",
    "    y = 0.2 * X**6 - 0.5 * X**5 + 2 * X**4 - 5 * X**3 + 3 * X**2 - 10 * X + 5 + noise\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use a scatter plot to visualize the relationship between $X$ and $y$. This will help you understand the structure of the generated data before moving on to fitting models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = generate_dataset(-3, 3)\n",
    "\n",
    "# TODO: Visualize the dataset using scatter (or other plots)\n",
    "plt.scatter(X, y, color='b', alpha=0.25, label='Generated data')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.title('Synthetic Polynomial Data with Noise')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Splitting the Dataset (5 Points)\n",
    "\n",
    "In this task, you should split the dataset into three parts:\n",
    "1. **Training Set**\n",
    "2. **In-Range Test Set**: This set should contain data within the same range as the training set.\n",
    "3. **Out-of-Range Test Set**: This set should contain data in the range **[5, 7]**.\n",
    "\n",
    "Ensure that the two test sets (In-Range and Out-of-Range) have the same length. You can use the `train_test_split` function from the `sklearn` library for convenience in splitting the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Split dataset into the training and the in-range testing sets\n",
    "X_train, X_test_in_range, y_train, y_test_in_range = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# TODO: Generate the out-of-range testing set\n",
    "X_out, y_out = generate_dataset(5, 7)\n",
    "_, X_test_out_of_range, _, y_test_out_of_range = train_test_split(X_out, y_out, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, you can use a scatter plot (or similar plots) to visualize the different sets and see their overall distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Visualize the sets using scatter (or other plots)\n",
    "plt.scatter(X_train, y_train, color='blue', alpha=0.25, label='Training Set [-3, 3]')\n",
    "plt.scatter(X_test_in_range, y_test_in_range, color='green', alpha=0.25, label='In-Range Test Set [-3, 3]')\n",
    "plt.scatter(X_test_out_of_range, y_test_out_of_range, color='red', alpha=0.25, label='Out-of-Range Test Set 2 [5, 7]')\n",
    "plt.legend()\n",
    "plt.title('Training Set vs. Test Sets')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Polynomial Regression Training (25 Points)\n",
    "\n",
    "In this task, you should train polynomial regression models with varying degrees, ranging from degree = 2 to degree = 9. You can use the `preprocessing`, `linear_model`, and `pipeline` classes from the `sklearn` library to create and evaluate these models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_models(X, y, degree):\n",
    "    # TODO: Create and train a model based on the given degree\n",
    "    features = PolynomialFeatures(degree=degree)\n",
    "    lr = LinearRegression()\n",
    "    \n",
    "    model = make_pipeline(features, lr)    \n",
    "    model.fit(X.reshape(-1, 1), y)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {}\n",
    "degree = 9\n",
    "\n",
    "# TODO: Add the trained models with different degrees to the dict\n",
    "for d in range(2, degree + 1):\n",
    "    models[d] = train_models(X_train, y_train, d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Evaluation (20 Points)\n",
    "\n",
    "In this task, you should evaluate the polynomial regression models on both test sets (In-Range Test Set and Out-of-Range Test Set) using Mean Squared Error (MSE). This evaluation will help you understand how well each model generalizes to different data ranges.\n",
    "\n",
    "For each polynomial degree, compute the MSE on both the in-range test set and the out-of-range test set using the `mean_squared_error` function from the `sklearn` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_models(models, X_test, y_test):\n",
    "    losses = {}\n",
    "    # TODO: Calculate MSE of each model (on the input test set).\n",
    "    for degree, model in models.items():\n",
    "        y_pred = model.predict(X_test.reshape(-1, 1))\n",
    "        losses[degree] = mean_squared_error(y_test, y_pred)\n",
    "    \n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Evaluate models on both the train and the test sets.\n",
    "losses_train = evaluate_models(models, X_train, y_train)\n",
    "losses_in_range = evaluate_models(models, X_test_in_range, y_test_in_range)\n",
    "losses_out_of_range = evaluate_models(models, X_test_out_of_range, y_test_out_of_range)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Analyzing Model Scores (20 Points)\n",
    "\n",
    "In this task, you will visualize and analyze the performance of polynomial regression models by plotting the **logarithm** (to better visualize differences) of losses (Mean Squared Error, MSE) for both training and test set and also by printing the losses related to each degree.\n",
    "\n",
    "Make sure to train each model a few times to get a sense of variability in the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Calculate the log MSE for both the train and the test sets based on the model degrees from 2 to 9\n",
    "log_losses_train = np.log1p(list(losses_train.values()))\n",
    "log_losses_in_range = np.log1p(list(losses_in_range.values()))\n",
    "log_losses_out_of_range = np.log1p(list(losses_out_of_range.values()))\n",
    "\n",
    "degrees = range(2, degree + 1)\n",
    "\n",
    "# TODO: Plot the log MSE results\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(degrees, list(log_losses_train), marker='o', linestyle='-', label='Training Set Log MSE')\n",
    "plt.plot(degrees, list(log_losses_in_range), marker='o', linestyle='--', label='In-Range Test Set Log MSE')\n",
    "plt.plot(degrees, list(log_losses_out_of_range), marker='o', linestyle='-.', label='Out-of-Range Test Set Log MSE')\n",
    "plt.xlabel('Polynomial Degree')\n",
    "plt.ylabel('Logarithm of Mean Squared Error (MSE)')\n",
    "plt.title('Logarithm of MSE vs. Polynomial Degree')\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.xticks(degrees)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Print each model loss on the training and the two test sets\n",
    "print(f\"{'Degree':<10}{'Train MSE':<20}{'In-Range Test MSE':<20}{'Out-of-Range Test MSE':<20}\")\n",
    "for i in range(degree - 1):\n",
    "    print(f\"{i + 2:<10}{log_losses_train[i]:<20.2f}{log_losses_in_range[i]:<20.2f}{log_losses_out_of_range[i]:<20.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion (15 Points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Based on your analysis of the training set loss, in-range test set loss, and out-of-range test set loss across different polynomial degrees, summarize what you have learned about the bias-variance tradeoff. How do these losses illustrate the tradeoff between model complexity and performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <span style=\"color:cyan;\">**Answer:** As the polynomial degree increases, the model tends to overfit the training data, leading to lower training loss but higher test loss, especially in the out-of-range test set. This demonstrates the bias-variance tradeoff: lower bias (better training fit) results in higher variance (poor generalization to unseen data). The challenge is finding a balance where the model performs well on both the training and test sets.\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Reflecting on the trends observed in the losses as polynomial degree increases, what conclusions can you draw about the effects of model complexity on overfitting and underfitting? How does this understanding help in selecting the appropriate polynomial degree for a model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <span style=\"color:cyan;\">**Answer:** With increasing complexity (higher-degree polynomials), models tend to capture noise in the training data, resulting in overfitting. The in-range test set may still show reasonable performance, but the out-of-range test set performance drops significantly. This highlights that overly complex models generalize poorly, while simpler models can underfit. The best degree is one that balances complexity, fitting well without overfitting.\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization (50 Points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "Author: Romina Shiri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=FF0000>\n",
    "Will be merged."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

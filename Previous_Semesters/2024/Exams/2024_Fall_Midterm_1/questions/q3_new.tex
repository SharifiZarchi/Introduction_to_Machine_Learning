\textbf{خلاف شیب}

در بسیاری از سناریوهای دنیای واقعی، داده‌های ما دارای میلیون‌ها بُعد هستند، اما یک نمونه خاص فقط دارای صدها ویژگی غیرصفر است. به عنوان مثال، در تحلیل اسناد با تعداد کلمات به عنوان ویژگی‌ها، ممکن است فرهنگ لغت ما میلیون‌ها کلمه داشته باشد، اما یک سند خاص فقط دارای صدها کلمه منحصر به فرد است. در این سؤال، می‌خواهیم نرم \lr{\(\ell_2\)-Regularized Stochastic Gradient Descent (SGD)} برای زمانی که داده‌های ورودی ما sparse است را کارا کنیم. به خاطر داشته باشید که در Regression Logistic \(\ell_2\) ، می‌خواهیم تابع هدف زیر را کمینه کنیم (در این مسئله برای سادگی \(w_0\) حذف شده است):

\begin{latin}
\[
F(\mathbf{w}) = \frac{1}{N} \sum_{j=1}^N l(\mathbf{x}^{(j)}, y^{(j)}, \mathbf{w}) + \frac{\lambda}{2} \sum_{i=1}^d w_i^2
\]
\end{latin}

که در آن \(l(\mathbf{x}^{(j)}, y^{(j)}, \mathbf{w})\) تابع هدف است:

\begin{latin}
    
\[
l(\mathbf{x}^{(j)}, y^{(j)}, \mathbf{w}) =\ln \left( 1 + \exp \left( \sum_{i=1}^d w_i x_i^{(j)} \right)\right) - y^{(j)} \left( \sum_{i=1}^d w_i x_i^{(j)} \right) 
\]
\end{latin}

و باقی مانده‌ی جمع، میزان Penalty Regularization خواهد بود.

وقتی روی نقطه \((\mathbf{x}^{(j)}, y^{(j)})\) \lr{SGD} انجام می‌دهیم، تابع هدف را به صورت زیر تقریب می‌زنیم:

\begin{latin}
    
\[
F(\mathbf{w}) \approx l(\mathbf{x}^{(j)}, y^{(j)}, \mathbf{w}) + \frac{\lambda}{2} \sum_{i=1}^d w_i^2
\]
\end{latin}

% \textbf{تریف :sparse} فرض کنید داده‌های ورودی ما دارای \(d\) ویژگی هستند، به عبارت دیگر \(\mathbf{x}^{(j)} \in \mathbb{R}^d\). در این مسئله، سناریویی را در نظر خواهیم گرفت که \(\mathbf{x}^{(j)}\) تنک باشد. به عبارت دیگر، فرض کنید \(s\) میانگین تعداد عناصر غیرصفر در هر نمونه باشد. ما زمانی که \(s \ll d\) باشد، می‌گوییم داده‌ها sparse هستند. در سوالات بعدی، \textbf{پاسخ شما باید در صورت امکان sparse \(\mathbf{x}^{(j)}\) را در نظر بگیرد}. (نکته: هنگامی که از ساختار داده sparse استفاده می‌کنیم، می‌توانیم در زمان \(O(s)\) عناصر غیرصفر را بررسی کنیم، در حالی که یک ساختار داده متراکم به زمان \(O(d)\) نیاز دارد.)

\begin{enumerate}
    \item 

 ابتدا حالت \(\lambda = 0\) را در نظر بگیرید. قانون به‌روزرسانی SGD برای \(w_i\) را هنگامی که \(\lambda = 0\) است، با استفاده از اندازه گام \(\eta\) و با توجه به نمونه \((\mathbf{x}^{(j)}, y^{(j)})\) بنویسید.

\vspace{5cm}

\item
اگر از یک ساختار داده متراکم استفاده کنیم، میانگین پیچیدگی زمانی برای به‌روزرسانی \(w_i\) هنگامی که \(\lambda = 0\) است، چقدر است؟ اگر از یک ساختار داده تنک استفاده کنیم، چطور؟ پاسخ خود را در یک یا دو جمله توضیح دهید.

\vspace{5cm}

\item
 اکنون حالت کلی را که \(\lambda > 0\) در نظر بگیرید. قانون به‌روزرسانی SGD برای \(w_i\) را هنگامی که \(\lambda > 0\) است، با استفاده از اندازه گام \(\eta\) و با توجه به نمونه \((\mathbf{x}^{(j)}, y^{(j)})\) بنویسید.

\vspace{5cm}

\item
  اگر از یک ساختار داده متراکم استفاده کنیم، میانگین پیچیدگی زمانی برای به‌روزرسانی \(w_i\) هنگامی که \(\lambda > 0\) است، چقدر است؟

\vspace{5cm}

\item
 فرض کنید \(\mathbf{w}^{(t)}_i\) بردار وزن بعد از به‌روزرسانی \(t\)ام باشد. اکنون فرض کنید که \(k\) به‌روزرسانی SGD روی \(\mathbf{w}\) با استفاده از نمونه‌های \((\mathbf{x}^{(t+1)}, y^{(t+1)}), \cdots , (\mathbf{x}^{(t+k)}, y^{(t+k)})\) انجام می‌دهیم، که در آن \(x_i^{(j)} = 0\) برای هر نمونه در دنباله باشد (یعنی ویژگی \(i\)ام برای تمام نمونه‌ها در دنباله صفر است). وزن جدید \(\mathbf{w}^{(t+k)}_i\) را بر حسب \(\mathbf{w}^{(t)}_i\)، \(k\)، \(\eta\)، و \(\lambda\) حساب کنید.

\vspace{5cm}

\item
با استفاده از پاسخ خود در قسمت قبل، یک الگوریتم کارا برای \lr{Regularized SGD} ارائه دهید زمانی که از ساختار داده sparse استفاده می‌کنیم. میانگین پیچیدگی زمانی به‌ازای هر نمونه چقدر است؟\\ \textbf{راهنمایی:} چه زمانی نیاز به به‌روزرسانی \(w_i\) دارید؟
\end{enumerate}

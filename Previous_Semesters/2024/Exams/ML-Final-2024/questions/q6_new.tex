

 مساله embed کردن متن را در نظر بگیرید. فرض کنید تعداد واژگان دیکشنری برابر ۱۰۰۰۰ باشد و تعداد توکن‌های متن ورودی حداکثر ۵۰۰ در نظر گرفته شده باشد و واژه‌ها به صورت \lr{one-hot} به‌ عنوان ورودی به شبکه داده شوند.

اگر از معماری ترنسفورمر استفاده شود، فرض کنید که بُعد  embedding هر لایه self-attention برابر ۷۶۸، بعد مخفی یا همان تعداد گره‌های لایه مخفی MLP تک لایه مخفی برابر ۳۰۰۰ و تعداد head لایه self-attention برابر ۴ و بعد متناظر با هر head برابر ۱۹۲ باشد. فرض کنید از \lr{positional embedding} با استفاده از توبع سینوسی و کسینوسی ایجاد شده است. شمای کلی شبکه‌ی 
embedding
و یک بلوک انکدر را رسم کنید و سپس تعداد پارامترهای آن را به دست آورید.
در هر بخش، درستی یا نادرستی گزینه را مشخص کرده و کامل توضیح دهید.
\begin{itemize}

    \item[الف)]
    در هنگام آموزش هرگونه مدل روی هر نوع مساله، از زمانی که تعداد پارامترها متناسب با تعداد ورودی‌ها شود به حداقل خطای تست می‌رسیم و پس از آن با افزایش تعداد پارامترها همیشه شاهد روند افزایشی خطای تست خواهیم بود.
    \item[ب)]
    معماری RNN سریع‌تر از Transformer است ولی دقت آن نسبت به Transformer کم‌تر است.

    \item[ج)]
    ابعاد فضای Embedding متن و تصویر در CLIP می‌تواند متفاوت باشد.
    
    \item[د)]
در یادگیری انتقالی با \lr{CNN}، زمانی که توزیع داده‌های هدف مشابه توزیع داده‌های پیش‌آموزش باشد، معمولاً چند لایه‌ی \lr{Fully Connected} آخر جایگزین شده و دوباره آموزش داده می‌شود، در حالی که وزن لایه‌های قبلی ثابت می‌مانند.

\item[ه)]
اگر یک مدل شبکه‌ی عصبی ۳ لایه برای رسیدن به دقت ۹۰ درصد روی یک مساله‌ی ورودی نیاز به ۱ میلیارد پارامتر داشته باشد، یک شبکه‌ی عصبی ۱۰ لایه برای رسیدن به همان سطح دقت روی همان مساله نیاز به تقریبا همان تعداد پارامتر دارد که البته عرض هر لایه کاهش می‌یابد.

\item[و)]
اگر از یک میلیارد تصویر داده شده، فقط یک میلیون تصویر برچسب داشته باشد برای خوشه‌بندی (Clustering) از کل یک میلیارد تصویر می‌توان استفاده کرد ولی برای دسته‌بندی (Classification) صرفا می‌توانیم از یک میلیون تصویر برچسب‌دار استفاده کنیم.
\item[ز)]
برای یک CNN با ابعاد کرنل $5\times 5$ و استراید ۱ و روی تصاویر $1000\times 1000$ داشتن سه لایه برای استخراج ویژگی‌های سراسری از تصویر کافی است.
\item[ح)]
استفاده از Patch ها به صورت انکد شده به عنوان ورودی ترانسفورمر ViT کافی نیست و حتما باید از 
\lr{Positional Embedding} استفاده کنیم.
\end{itemize}

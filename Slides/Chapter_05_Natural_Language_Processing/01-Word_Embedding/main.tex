%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% A Beamer template for University of Wollongong     %
% Based on THU beamer theme                          %
% Author: Qiuyu Lu                                   %
% Date: July 2024                                    %
% LPPL Licensed.                                     %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Customized for Sharif University of Technology     %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\documentclass[serif, aspectratio=169]{beamer}
%\documentclass[serif]{beamer}  % for 4:3 ratio
\usepackage[T1]{fontenc} 
\usepackage{fourier} % see "http://faq.ktug.org/wiki/uploads/MathFonts.pdf" for other options
\usepackage{hyperref}
\usepackage{latexsym,amsmath,xcolor,multicol,booktabs,calligra}
\usepackage{graphicx,pstricks,listings,stackengine}
\usepackage{lipsum}
\usepackage{caption}

% For writing clean pseudocodes
\usepackage{algorithm, algpseudocode, mathtools, needspace}
% To justify the items
\usepackage{ragged2e}
% To draw diagrams
\usepackage{tikz}
% To include urls
\usepackage{url}
% To make clean tables
\usepackage{color, tabularray}

\author{Ali Sharifi-Zarchi}
\title{Machine Learning (CE 40717)}
\subtitle{Fall 2025}
\institute{
    CE Department \\
    Sharif University of Technology
}
\usepackage{SUTstyle}

% defs
\def\cmd#1{\texttt{\color{red}\footnotesize $\backslash$#1}}
\def\env#1{\texttt{\color{blue}\footnotesize #1}}
\definecolor{deepblue}{rgb}{0,0,0.5}
\definecolor{deepred}{RGB}{153,0,0}
\definecolor{deepgreen}{rgb}{0,0.5,0}
\definecolor{halfgray}{gray}{0.55}
\definecolor{silver}{rgb}{0.752,0.752,0.752}

\lstset{
    basicstyle=\ttfamily\small,
    keywordstyle=\bfseries\color{deepblue},
    emphstyle=\ttfamily\color{deepred},    % Custom highlighting style
    stringstyle=\color{deepgreen},
    numbers=left,
    numberstyle=\small\color{halfgray},
    rulesepcolor=\color{red!20!green!20!blue!20},
    frame=shadowbox,
}



\begin{document}

\begin{frame}
    \titlepage
    \vspace*{-0.6cm}
    \begin{figure}[htpb]
        \begin{center}
            \includegraphics[keepaspectratio, scale=0.25]{pic/sharif-main-logo.png}
        \end{center}
    \end{figure}
\end{frame}

\begin{frame}    
\tableofcontents[sectionstyle=show,
subsectionstyle=show/shaded/hide,
subsubsectionstyle=show/shaded/hide]
\end{frame}



\section{Introduction}


% Slide 1: Introduction to NLP and Language Importance
\begin{frame}{Natural Language Processing}
	\begin{itemize}
		\item Language is central to human interaction; many of our daily activities revolve around text and language.
		\item Natural Language Processing (NLP) enables computers to understand and generate human language.
	\end{itemize}
\end{frame}

% Slide 2: Translation
\begin{frame}{Translation}
	\begin{figure}
		\centering
		\includegraphics[width=0.6\textwidth]{pic/translation_image.png}
	\end{figure}
	\begin{itemize}
		\item NLP helps translate text from one language to another.
	\end{itemize}
	\vspace{1.6cm}
	\hspace{-1.0cm}
	{\tiny \textcolor{gray}{Figure adapted from geeksforgeeks.org/machine-translation-of-languages-in-artificial-intelligence/}}
\end{frame}

% Slide 3: Sentiment Analysis
\begin{frame}{Sentiment Analysis}
	\begin{figure}
		\centering
		\includegraphics[width=0.6\textwidth]{pic/5.png}
	\end{figure}
	\begin{itemize}
		\item Determines the sentiment (e.g., positive or negative) expressed in a text.
	\end{itemize}
	\vspace{2.6cm}
	\hspace{-1.0cm}
	{\tiny \textcolor{gray}{Figure adapted from www.mdpi.com/2079-9292/9/3/483}}
\end{frame}

% Slide 4: Text Summarization
\begin{frame}{Text Summarization}
	\begin{figure}
		\centering
		\includegraphics[width=0.6\textwidth]{pic/summarization_image.png}
	\end{figure}
	\begin{itemize}
		\item Automatically generates a concise summary of longer text.
	\end{itemize}
	\vspace{0.6cm}
	\hspace{-1.0cm}
	{\tiny \textcolor{gray}{Figure adapted from turbolab.in/types-of-text-summarization-extractive-and-abstractive-summarization-basics/}}
\end{frame}

% Slide 5: Named Entity Recognition (NER)
\begin{frame}{Named Entity Recognition (NER)}
	\begin{figure}
		\centering
		\includegraphics[width=0.6\textwidth]{pic/ner_image.jpg}
	\end{figure}
	\begin{itemize}
		\item Identifies and classifies entities (e.g., names, dates) in text.
	\end{itemize}
	\vspace{1.6cm}
	\hspace{-1.0cm}
	{\tiny \textcolor{gray}{Figure adapted from analyticsvidhya.com/blog/2021/11/a-beginners-introduction-to-ner-named-entity-recognition/}}
\end{frame}

% Slide 6: Speech Recognition
\begin{frame}{Speech Recognition}
	\begin{figure}
		\centering
		\includegraphics[width=0.6\textwidth]{pic/speech_recognition_image.png}
	\end{figure}
	\begin{itemize}
		\item Converts spoken language into text.
	\end{itemize}
\end{frame}

% Slide 7: Text Classification
\begin{frame}{Text Classification}
	\begin{figure}
		\centering
		\includegraphics[width=0.6\textwidth]{pic/classification_image.png}
	\end{figure}
	\begin{itemize}
		\item Categorizes text into predefined groups or topics.
	\end{itemize}
	\vspace{1.6cm}
	\hspace{-1.0cm}
	{\tiny \textcolor{gray}{Figure adapted from towardsdatascience.com/machine-learning-nlp-text-classification-using-scikit-learn-python-and-nltk-c52b92a7c73a}}
\end{frame}

% Slide 8: Question Answering
\begin{frame}{Question Answering}
	\begin{figure}
		\centering
		\includegraphics[width=0.6\textwidth]{pic/question_answering_image.png}
	\end{figure}
	\begin{itemize}
		\item Answers questions based on a given text or dataset.
	\end{itemize}
	\vspace{0.6cm}
	\hspace{-1.0cm}
	{\tiny \textcolor{gray}{Figure adapted from nlplanet.org/course-practical-nlp/02-practical-nlp-first-tasks/17-question-answering}}
\end{frame}

% Slide 9: Chatbots and Dialogue Systems
\begin{frame}{Chatbots and Dialogue Systems}
	\begin{figure}
		\centering
		\includegraphics[width=0.6\textwidth]{pic/chatbot_image.jpg}
	\end{figure}
	\begin{itemize}
		\item NLP powers chatbots that can interact with users through text or speech.
	\end{itemize}
	\vspace{0.3cm}
	\hspace{-1.0cm}
	{\tiny \textcolor{gray}{Figure adapted from chatgpt.org/}}
\end{frame}


% Slide 11: Transition to Word Representation
\begin{frame}{The Importance of Word Representation}
	\begin{itemize}
		\item To process text effectively, the first step is to represent words in a way that models can understand.
		\item We need to transform words into vectors or dense representations to capture their meaning and relationships.
		\item This is crucial for enabling machines to understand and use language as humans do.
	\end{itemize}
\end{frame}



\begin{frame}{Motivation and One-Hot Encoding}
	\begin{itemize}
		\item Traditional models like one-hot encoding lack semantic understanding and fail to capture word relationships.
		\item One-hot encoding represents words as binary vectors where only one position is 1, making the representation sparse and non-semantic.
		\item We need dense, semantic word representations to improve natural language processing tasks.
	\end{itemize}
	\begin{figure}
		\centering
		\includegraphics[width=0.7\textwidth]{pic/one_hot_example.png}
	\end{figure}
	\hspace{-1.0cm}
	{\tiny \textcolor{gray}{Figure adapted from medium.com/@kalyan45/natural-language-processing-one-hot-encoding-5b31f76b09a0}}
\end{frame}


\begin{frame}{Example of One-Hot Encoding}
	\begin{itemize}
		\item For example, given a vocabulary of 5 words:
		\begin{itemize}
			\item \texttt{apple} = [1, 0, 0, 0, 0]
			\item \texttt{banana} = [0, 1, 0, 0, 0]
			\item \texttt{cherry} = [0, 0, 1, 0, 0]
			\item \texttt{date} = [0, 0, 0, 1, 0]
			\item \texttt{elderberry} = [0, 0, 0, 0, 1]
		\end{itemize}
		\item The length of the one-hot vector depends on the number of unique words in the vocabulary.
	\end{itemize}
\end{frame}

\begin{frame}{Strengths and Limitations of One-Hot Encoding}
	\begin{itemize}
		\item \textbf{Strengths:}
		\begin{itemize}
			\item One-hot encoding is a simple and intuitive representation that can be effective in certain models, especially smaller neural networks.
			\item It requires minimal computation and works well for small vocabularies or categorical features in simpler tasks.
		\end{itemize}
		\item \textbf{Limitations:}
		\begin{itemize}
			\item One-hot encoding does not capture semantic relationships, so similar words (e.g., \texttt{hotel} and \texttt{motel}) appear unrelated.
			\item The vectors are sparse, containing mostly zeros, making it inefficient for large vocabularies and prone to overfitting due to the large number of parameters in downstream models.
		\end{itemize}
	\end{itemize}
\end{frame}

\begin{frame}{Example: Similar Words, Zero Cosine Similarity}
	\begin{itemize}
		\item Consider the following one-hot vectors:
		\begin{itemize}
			\item \texttt{hotel} = [0, 0, 0, 1, 0]
			\item \texttt{motel} = [0, 0, 0, 0, 1]
		\end{itemize}
		\item Even though \texttt{hotel} and \texttt{motel} are semantically similar, their cosine similarity is 0 because their vectors are orthogonal.
		\item \textbf{Cosine Similarity:}
		\[
		\cos(\theta) = \frac{\mathbf{v}_1 \cdot \mathbf{v}_2}{\|\mathbf{v}_1\| \|\mathbf{v}_2\|}
		\]
		\item In this case, the dot product of the one-hot vectors is zero, leading to a cosine similarity of zero.
	\end{itemize}
\end{frame}

\begin{frame}{Example: One-Hot Encoding for Different Words}
	\begin{figure}
		\centering
		\includegraphics[width=0.8\textwidth]{pic/onehot encode_space.png}
	\end{figure}
	\begin{itemize}
		\item Different words can be perpendicular to each other in space regardless of their meanings.
	\end{itemize}
	\vspace{1.6cm}
	\hspace{-1.0cm}
	{\tiny \textcolor{gray}{Figure adapted from andreaperlato.com/theorypost/introduction-to-word2vec/}}
\end{frame}





\section{Word2Vec}

\begin{frame}{Word Embedding} 
	
	\begin{itemize} \item To process text data, we need to represent words in a form that a machine can understand—numerical vectors. \item Word2Vec uses a neural network to learn \textbf{word embedding}s that capture semantic similarities. \item These embeddings allow words with similar meanings to be represented by vectors close to each other in a high-dimensional space. 
	\end{itemize} 
\end{frame}

\begin{frame}{Example: Word Embedding}
	\begin{figure}
		\centering
		\includegraphics[width=0.8\textwidth]{pic/word_embedding.png}
	\end{figure}
%	\begin{itemize}
%		\item Representation of the words in 2D space using word embedding.
%	\end{itemize}
%	\vspace{1.6cm}
	\hspace{-1.0cm}
	{\tiny \textcolor{gray}{Figure adapted from classes.engr.oregonstate.edu/eecs/fall2023/ai534-400/unit4/word\_embeddings.html}}
\end{frame}


\begin{frame}{Word2Vec as a Neural Network}
	 \begin{itemize} \item Word2Vec is a shallow neural network, with an input, hidden, and output layer. \item It takes in a target word and learns to predict either the surrounding context words or the target word from a set of context words. \item Through training, the network adjusts weights to create meaningful vector representations of words. 
	 \end{itemize} 
\end{frame}


\begin{frame}{Word2Vec as a Neural Network}
	\begin{figure}
		\centering
		\includegraphics[width=0.7\textwidth]{pic/word2vec_nn.png}
	\end{figure}
	\vspace{0.4cm}
	\hspace{-1.0cm}
	{\tiny \textcolor{gray}{Figure adapted from machinelearningcoban.com/tabml\_book/ch\_embedding/word2vec.html}}
\end{frame}

\begin{frame}{Expected Outcome of Word2Vec}
	\begin{itemize}
		\item Word2Vec aims to create a vector space where words with similar meanings or contexts are located close to each other.
		\item \textbf{Expected Result:} Semantically related words—such as “king” and “queen” or “dog” and “puppy”—should have similar vector representations.
		\item This proximity allows for various NLP tasks, such as:
		\begin{itemize}
			\item \textbf{Synonym detection:} Identifying words with similar meanings.
			\item \textbf{Analogy tasks:} Solving analogies by vector arithmetic (e.g., “king” - “man” + “woman” $\approx$ “queen”).
			\item \textbf{Clustering of concepts:} Grouping related concepts together in the embedding space.
		\end{itemize}
		\item By representing words in this way, Word2Vec enables models to make use of semantic relationships between words.
	\end{itemize}
\end{frame}



\begin{frame}{Expected Outcome of Word2Vec}
	\begin{figure}
		\centering
		\includegraphics[width=1.0\textwidth]{pic/word2vec_expected.png}
		\caption*{Hypothetical features to understand word embeddings}
	\end{figure}
	\vspace{0.6cm}
	\hspace{-1.0cm}
	{\tiny \textcolor{gray}{Figure adapted from medium.com/@manansuri/a-dummys-guide-to-word2vec-456444f3c673}}
\end{frame}


\begin{frame}{Word2Vec: Contextual Word Representation}
    \begin{itemize}
        \item The core idea is based on distributional semantics: "You shall know a word by the company it keeps."
        \item Word2Vec uses two main algorithms for learning word vectors:
        \begin{itemize}
            \item Continuous Bag of Words (CBOW)
            \item Skip-gram Model
        \end{itemize}
    \end{itemize}
\end{frame}

\subsection{Continuous Bag of Words (CBOW)}

\begin{frame}{CBOW: How It Works}
    \begin{itemize}
        \item CBOW predicts the target word using the context (surrounding words) in a fixed window.
        \item For each word in the corpus, CBOW takes a set of context words and predicts the center word.
        \item Example: Given the context words \{“the”, “brown”, “fox”, “over”\}, CBOW predicts the center word “jumps.”
        \item CBOW tends to perform better on smaller datasets and is computationally more efficient.
    \end{itemize}
\end{frame}

\begin{frame}{Example: Continuous Bag of Words}
	\begin{figure}
		\centering
		\includegraphics[width=0.6\textwidth]{pic/cbow.png}
	\end{figure}
	\vspace{0.4cm}
	\hspace{-1.0cm}
	{\tiny \textcolor{gray}{Figure adapted from geeksforgeeks.org/continuous-bag-of-words-cbow-in-nlp/}}
\end{frame}

\subsection{Skip-gram Model}

\begin{frame}{Skip-gram: How It Works}
    \begin{itemize}
        \item Skip-gram is the reverse of CBOW. It predicts the surrounding context words given a target word.
        \item For each word \(w_t\), the model predicts the words in the window of size \(m\) around it (e.g., words \(w_{t-2}, w_{t-1}, w_{t+1}, w_{t+2}\)).
        \item Example: If the center word is “jumps,” Skip-gram predicts the context words “the,” “brown,” “fox,” and “over.”
        \item Skip-gram is better suited for larger datasets and can capture rare words more effectively.
    \end{itemize}
\end{frame}

\begin{frame}{Why Skip-gram?}
	\begin{itemize}
		\item Skip-gram is preferred for large datasets because it handles rare words more effectively than CBOW.
		\item It captures detailed information about the surrounding words, leading to better word representations in the vector space.
		\item Word2Vec embeddings from Skip-gram have been widely adopted in various NLP tasks such as machine translation, sentiment analysis, and document classification.
	\end{itemize}
\end{frame}


\begin{frame}{Skip-gram Example}
    \begin{figure}
        \centering
        \includegraphics[width=0.4\textwidth]{pic/10.png}
        \caption*{Different window sizes and samples drawn from context words and their target}
    \end{figure}
    \vspace{0.1cm}
    \hspace{-1.0cm}
    {\tiny \textcolor{gray}{Figure adapted from tensorflow.org/text/tutorials/word2vec}}
\end{frame}

\begin{frame}{CBOW vs. Skip-gram}
	\begin{itemize}
		\item CBOW and Skip-gram are the two primary architectures for Word2Vec.
		\item \textbf{CBOW} predicts a target word given its surrounding context, making it efficient and effective for smaller datasets.
		\item \textbf{Skip-gram}, on the other hand, predicts the surrounding words for a given target word. It is well-suited for larger datasets and can handle rare words more effectively.
		\item In essence, the Skip-gram model captures more detailed word relationships and is robust in large vocabularies.
	\end{itemize}
\end{frame}


\begin{frame}{CBOW vs. Skip-gram}
	\begin{figure}
		\centering
		\includegraphics[width=0.7\textwidth]{pic/skip_vs_cbow1.png}
		\caption*{Difference between CBOW and Skip-gram}
	\end{figure}
	\vspace{0.4cm}
	\hspace{-1.0cm}
	{\tiny \textcolor{gray}{Figure adapted from kavita-ganesan.com/comparison-between-cbow-skipgram-subword/}}
\end{frame}
\begin{frame}{Skip-gram: Objective Function}
    \begin{itemize}
        \item The Skip-gram model learns word representations by predicting context words \(w_o\) given a center word \(w_c\).
        \item Each word has two embeddings:
        \begin{itemize}
            \item \textbf{Input (center) embedding}: \(\mathbf{v}_w\)
            \item \textbf{Output (context) embedding}: \(\mathbf{u}_w\)
        \end{itemize}
        \item The conditional probability of a context word is:
        \[
        P(w_o \mid w_c) =
        \frac{\exp(\mathbf{u}_{w_o}^\top \mathbf{v}_{w_c})}
        {\sum_{x \in V} \exp(\mathbf{u}_x^\top \mathbf{v}_{w_c})}
        \]
        \item Intuition: words that co-occur should have large dot products.
    \end{itemize}
\end{frame}

\begin{frame}{Skip-gram as a 2-Layer Neural Network}
    \begin{itemize}
        \item Word2Vec is a shallow neural network:
        \[
        \text{one-hot}(w_c)
        \;\rightarrow\;
        \mathbf{W}_{\text{in}}
        \;\rightarrow\;
        \mathbf{h}
        \;\rightarrow\;
        \mathbf{W}_{\text{out}}
        \;\rightarrow\;
        \text{softmax}
        \]
        \item Weight matrices:
        \[
        \mathbf{W}_{\text{in}} \in \mathbb{R}^{|V| \times d},
        \quad
        \mathbf{W}_{\text{out}} \in \mathbb{R}^{d \times |V|}
        \]
        \item Because the input is one-hot:
        \[
        \mathbf{h} = \mathbf{W}_{\text{in}}^\top \mathbf{x} = \mathbf{v}_{w_c}
        \]
        \item \textbf{Key point:}
        \begin{itemize}
            \item \(\mathbf{v}_w\) = row of \(\mathbf{W}_{\text{in}}\)
            \item \(\mathbf{u}_w\) = column of \(\mathbf{W}_{\text{out}}\)
        \end{itemize}
        \item Updating embeddings = updating network weights.
    \end{itemize}
\end{frame}


\begin{frame}{Skip-gram as a Neural Network}
	\begin{figure}
		\centering
		\includegraphics[width=0.8\textwidth]{pic/skip_nn.png}
		\caption*{Skip-gram Model as Neural Network}
	\end{figure}
	\vspace{1.0cm}
	\hspace{-1.0cm}
	{\tiny \textcolor{gray}{Figure adapted from towardsdatascience.com/word2vec-explained-49c52b4ccb71}}
\end{frame}

\begin{frame}{Skip-gram: Loss Function}
    \begin{itemize}
        \item Training maximizes likelihood of observed center--context pairs.
        \item Equivalent objective (negative log-likelihood):
        \[
        J(\theta)
        = -\frac{1}{T}
        \sum_{t=1}^{T}
        \sum_{\substack{-m \leq j \leq m \\ j \neq 0}}
        \log P(w_{t+j} \mid w_t)
        \]
        \item \(T\): corpus size, \quad \(m\): window size.
    \end{itemize}
\end{frame}

\begin{frame}{Skip-gram: Gradient Calculation}
    \begin{itemize}
        \item For one training pair \((w_c, w_o)\):
        \[
        \log P(w_o \mid w_c)
        =
        \mathbf{u}_{w_o}^\top \mathbf{v}_{w_c}
        - \log \sum_x \exp(\mathbf{u}_x^\top \mathbf{v}_{w_c})
        \]
        \item Gradient w.r.t. center embedding:
        \[
        \frac{\partial \log P(w_o \mid w_c)}{\partial \mathbf{v}_{w_c}}
        =
        \mathbf{u}_{w_o}
        -
        \sum_x P(w_x \mid w_c)\, \mathbf{u}_x
        \]
        \item Same result as backprop through \(\mathbf{W}_{\text{in}}\).
    \end{itemize}
\end{frame}

\begin{frame}{Skip-gram: Gradient Update (Intuition)}
    \begin{itemize}
        \item Gradient ascent update:
        \[
        \mathbf{v}_{w_c}
        \leftarrow
        \mathbf{v}_{w_c}
        + \eta
        \left(
        \mathbf{u}_{w_o}
        -
        \sum_x P(w_x \mid w_c)\, \mathbf{u}_x
        \right)
        \]
        \item Interpretation:
        \begin{itemize}
            \item Pull center word toward true context word
            \item Push it away from other words
        \end{itemize}
        \item Output embeddings \(\mathbf{u}_x\) are updated analogously.
    \end{itemize}
\end{frame}


\begin{frame}{Skip-gram Example with Gradient Intuition}
    \begin{itemize}
        \item Sentence: ``Cats chase mice in the dark night.''
        \item Center word: \(w_c = \text{``chase''}\)
        \item Context word: \(w_o = \text{``cats''}\)
        \item Example embeddings (2D for illustration):
        \[
        \mathbf{v}_{\text{chase}} = [0.1, 0.2],
        \quad
        \mathbf{u}_{\text{cats}} = [0.3, 0.4]
        \]
        \item Dot product:
        \[
        \mathbf{u}_{\text{cats}}^\top \mathbf{v}_{\text{chase}} = 0.11
        \]
        \item If \(P(\text{cats} \mid \text{chase}) = 0.493\), the error is:
        \[
        1 - 0.493 = 0.507
        \]
        \item The update moves \(\mathbf{v}_{\text{chase}}\) closer to \(\mathbf{u}_{\text{cats}}\).
    \end{itemize}
\end{frame}

\begin{frame}{Skip-gram Pseudocode}
    \begin{algorithm}[H]
        \caption{Skip-gram Model}
        \begin{algorithmic}
            \fontsize{6pt}{7.2}\selectfont
            \Require Corpus $D$, window size $w$, embedding dimension $d$, learning rate $\eta$, epochs $n$
            \State Initialize input embeddings $\mathbf{V}$ and output embeddings $\mathbf{U}$ randomly
            \For{epoch $= 1$ to $n$}
                \For{each sentence $S$ in $D$}
                    \For{each center word $w_c$ in $S$}
                        \For{each context word $w_o$ within window $w$}
                            \State Compute score $\mathbf{u}_{w_o}^\top \mathbf{v}_{w_c}$
                            \State Compute $P(w_o \mid w_c)$ using softmax
                            \State Update $\mathbf{v}_{w_c}$ and $\mathbf{u}_{w_o}$ using gradients
                        \EndFor
                    \EndFor
                \EndFor
            \EndFor
            \State \Return input embeddings $\mathbf{V}$
        \end{algorithmic}
    \end{algorithm}
\end{frame}

\begin{frame}{Why Do We Need Negative Sampling?}
    \begin{itemize}
        \item The softmax denominator sums over the entire vocabulary \(V\).
        \item For large vocabularies, this is computationally expensive.
        \item \textbf{Key idea}: update only a few words instead of all words.
        \item \textbf{Negative sampling} provides an efficient approximation.
    \end{itemize}
\end{frame}

\begin{frame}{Skip-gram: Negative Sampling}
    \begin{itemize}
        \item For each positive pair \((w_c, w_o)\), sample \(k\) negative words.
        \item The objective becomes binary classification:
        \begin{itemize}
            \item Maximize similarity for true context words
            \item Minimize similarity for randomly sampled words
        \end{itemize}
        \item Example:
        \begin{itemize}
            \item Center word: ``cat''
            \item Positive context: ``cute''
            \item Negative samples: ``table'', ``sky'', ``computer''
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}{Skip-gram: Negative Sampling}
    \begin{figure}
        \centering
        \includegraphics[width=0.8\textwidth]{pic/skip_negative.png}
        \caption*{Negative sampling}
    \end{figure}
    \vspace{0.1cm}
    {\tiny \textcolor{gray}{Figure adapted from amitness.com/posts/fasttext-embeddings}}
\end{frame}



\subsection{Word Embedding Visualization}

\begin{frame}{Visualizing Words in 2D}
    \begin{itemize}
        \item After training the model, words are mapped into a high-dimensional vector space.
        \item Using techniques like PCA or t-SNE, these vectors can be reduced to 2D for visualization, where similar words appear closer together.
    \end{itemize}
\end{frame}

\begin{frame}{Visualizing Words in 2D}
	\begin{figure}
		\centering
		\includegraphics[width=0.6\textwidth]{pic/word2vec_embedding.png}
		\caption*{Words represented in a 2D space after dimensionality reduction}
	\end{figure}
	\vspace{0.6cm}
	\hspace{-1.0cm}
	{\tiny \textcolor{gray}{Figure adapted from jramkiss.github.io/2019/08/21/word-embeddings/}}
\end{frame}

\subsection{Word Analogy}

\begin{frame}{Word Analogy: Vector Arithmetic in Word2Vec}
    \begin{itemize}
        \item Word2Vec embeddings can solve analogy tasks by performing vector arithmetic.
        \item The analogy task takes the form:
        \[
        \text{king} - \text{man} + \text{woman} \approx \text{queen}
        \]
        \item The analogy is solved by finding the word vector closest to \( \mathbf{v}_{king} - \mathbf{v}_{man} + \mathbf{v}_{woman} \).
    \end{itemize}
\end{frame}


\begin{frame}{Word Analogy Formula}
	\begin{itemize}
		\item The formal formula for solving word analogies is:
		\[
		d = \arg\max_i \frac{(x_b - x_a + x_c)^T x_i}{\|x_b - x_a + x_c\|}
		\]
		\item Definitions:
		\begin{itemize}
			\item \(x_a\): Vector for the first word (e.g., "man").
			\item \(x_b\): Vector for the second word (e.g., "woman").
			\item \(x_c\): Vector for the third word (e.g., "king").
			\item \(x_i\): Candidate word vectors in the vocabulary.
		\end{itemize}
		\item Example: To solve "man is to woman as king is to ?", the formula computes a vector closest to \(x_b - x_a + x_c\) (e.g., "queen").
	\end{itemize}
\end{frame}


\begin{frame}{Word Analogy Example}
    \begin{itemize}
        \item Example:
        \[
        \text{king}[0.30, 0.70] - \text{man}[0.20, 0.20] + \text{woman}[0.60, 0.30] \approx \text{queen}[0.70, 0.80]
        \]
        \item This means the vector difference between "king" and "man" is similar to the difference between "queen" and "woman."
    \end{itemize}
\end{frame}


\begin{frame}{Word Analogy Example}
	\begin{figure}
		\centering
		\includegraphics[width=0.5\textwidth]{pic/4.png}
		\caption*{Word analogy example}
	\end{figure}
\end{frame}


\section{References}

\begin{frame}{Contribution}
	\begin{itemize}
		\item \textbf{These slides were prepared with contribution from:}
		\begin{itemize}
			\item Omid Daliran
			\item Armin Geramirad
		\end{itemize}
	\end{itemize}
\end{frame}



\begin{frame}{References}
    \bibliography{ref}
    \bibliographystyle{ieeetr}
    \nocite{*}
\end{frame}

\end{document}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hWmyKlSBjVqc"
   },
   "source": [
    "<br>\n",
    "<font>\n",
    "<!-- <img src=\"https://sina.sharif.edu/~m_salehi/images/logo_sharif.png\" alt=\"SUT logo\" width=300 height=300 align=left class=\"saturate\"> -->\n",
    "<div dir=ltr align=center>\n",
    "<img src=\"https://sina.sharif.edu/~m_salehi/images/logo_sharif.png\" width=160 height=180>\n",
    "<br>\n",
    "<font color=0F5298 size=6>\n",
    "Introduction to Machine Learning <br>\n",
    "<font color=696880 size=5>\n",
    "<!-- <br> -->\n",
    "Computer Engineering Department\n",
    "<br>\n",
    "Sharif University of Technology\n",
    "\n",
    "<font color=696880 size=5>\n",
    "<br>\n",
    "CE 40477 - Fall 2025\n",
    "\n",
    "<font color=GREEN size=5>\n",
    "<br>\n",
    "Mahdi Aghaei & Farzan Rahmani\n",
    "<!-- <br> -->\n",
    "\n",
    "____\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7rpECnsLL5DX"
   },
   "source": [
    "# Table of Contents\n",
    "\n",
    "\n",
    "*   ## [Regression](#scrollTo=iucKtjV-JUQb&line=1&uniqifier=1)\n",
    "    *   ### [Linear Regression](#scrollTo=4Ip2SVWzJoBk&line=1&uniqifier=1)\n",
    "    *  ### [Polynomial Regression](#scrollTo=mlZPhd-BUAyj&line=1&uniqifier=1)\n",
    "    * ### [Root Mean Square Error(RMSE)](#scrollTo=uZUPXpRHUX80&line=1&uniqifier=1)\n",
    "    * ### [Gradient Descent](#scrollTo=-WQ6j36_Y2cS&line=1&uniqifier=1)\n",
    "    * ### [Regularization: Ridge and Lasso Regression](#scrollTo=QNHOkmgtI-pt&line=1&uniqifier=1)\n",
    "\n",
    "*  ### [Real-World Example 1: California House Price Prediction](#scrollTo=tMkQJeyKI0Gq&line=1&uniqifier=1)\n",
    "*   ### [Real-World Example 2: Tehran House Price Prediction](#scrollTo=i-g63Weeisqj&line=1&uniqifier=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iucKtjV-JUQb"
   },
   "source": [
    "# Regression\n",
    "In this section, we will try to solve the problem of **Regression**. In our first step, we will atack the problem from the analytical view."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Ck3wWUWTADI"
   },
   "source": [
    "### Generate Synthetic Data\n",
    "\n",
    "We will use rather simple line of $ y = 3x + 8 $ with noise of $ \\epsilon = 5 $ to generate test and train data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "o5hs1H0zEG6s",
    "outputId": "bf338dd7-4251-46b7-84ae-17d1e6fc4cf2"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def generate_data(n_samples=50, noise=5.0):\n",
    "    np.random.seed(42)\n",
    "    X = np.linspace(-10, 10, n_samples)\n",
    "    # Ground truth line: y = 3x + 8\n",
    "    true_slope = 3\n",
    "    true_intercept = 8\n",
    "    noise = np.random.randn(n_samples) * noise\n",
    "    y = true_slope * X + true_intercept + noise\n",
    "    return X, y\n",
    "\n",
    "#plot\n",
    "X, y = generate_data(n_samples=50, noise=5.0)\n",
    "plt.scatter(X, y, color='blue', label='Data Points')\n",
    "plt.title(\"Generated Data (Univariate)\")\n",
    "plt.xlabel(\"$x$\")\n",
    "plt.ylabel(\"$y$\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Ip2SVWzJoBk"
   },
   "source": [
    "## Linear Regression: Analytical Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8FOoAeLWT38E"
   },
   "source": [
    "### Implement the Closed-Form Solution\n",
    "In the analytical approach, we directly compute the optimal parameters using the normal equation:\n",
    "\n",
    "$$\n",
    "\\mathbf{w} = (\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{y}\n",
    "$$\n",
    "\n",
    "This method works perfectly for small datasets, but can become computationally expensive for large-scale problems.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gJ7Ns6Z_EMh8",
    "outputId": "2650ff7a-0dde-4a68-aa8e-9c97f1039577"
   },
   "outputs": [],
   "source": [
    "# Hypothesis: h_w(x) = w_0 + w_1 * x_1\n",
    "def h_w(x, w):\n",
    "    return w[0] + w[1] * x  # equivalent to w_0 + w_1 * x\n",
    "\n",
    "# Linear Regression using closed-form solution\n",
    "def linear_regression_closed_form(X, y):\n",
    "    # Adding bias term (x_0 = 1) to input vector X\n",
    "    X_b = np.c_[np.ones((len(X), 1)), X]  # X_b is now the full input vector with bias term\n",
    "    # Closed-form solution: w = (X^T * X)^-1 * X^T * y\n",
    "    w = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)\n",
    "    return w\n",
    "\n",
    "# Get parameter vector w\n",
    "w = linear_regression_closed_form(X, y)\n",
    "print(f\"Parameters (w): \")\n",
    "print(f\"w_1 = {w[1]:.2f}, w_0 = {w[0]:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "5jPFrAPvEVC4",
    "outputId": "59f68327-07bc-4c96-a11e-34792fa97799"
   },
   "outputs": [],
   "source": [
    "y_pred = h_w(X, w)\n",
    "\n",
    "plt.scatter(X, y, color='blue', label='Actual Data')\n",
    "plt.plot(X, y_pred, color='red', label='Prediction (Closed Form)')\n",
    "plt.title(\"Linear Regression - Closed Form Solution\")\n",
    "plt.xlabel(\"$x$\")\n",
    "plt.ylabel(\"$y$\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mlZPhd-BUAyj"
   },
   "source": [
    "## Polynomial Regression: Analytical Solution\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jHboZxPvK7bp"
   },
   "source": [
    "### Implement Polynomial Regression\n",
    "Linear models are simple, but they fail to capture nonlinear relationships.Linear regression can be extended to model nonlinear relationships by introducing polynomial terms.\n",
    "\n",
    "$$\n",
    "y = w_0 + w_1x + w_2x^2 + ... + w_mx^m\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_2Q55cbtEYmI",
    "outputId": "fa4bc7b2-56d6-4891-c493-8f7bba4eef1c"
   },
   "outputs": [],
   "source": [
    "# Function to generate polynomial features (input matrix X')\n",
    "def polynomial_features(X, degree):\n",
    "    X_poly = np.c_[np.ones(len(X))]\n",
    "    for i in range(1, degree + 1):\n",
    "        X_poly = np.c_[X_poly, X**i]\n",
    "    return X_poly\n",
    "\n",
    "def polynomial_regression(X, y, degree):\n",
    "    X_poly = polynomial_features(X, degree)\n",
    "    # Closed-form solution: w = (X'^T * X')^-1 * X'^T * y\n",
    "    w = np.linalg.inv(X_poly.T.dot(X_poly)).dot(X_poly.T).dot(y)\n",
    "    return w\n",
    "\n",
    "m = 5  # Degree of the polynomial regression\n",
    "w_poly = polynomial_regression(X, y, m)  # Parameter vector w\n",
    "\n",
    "print(f\"Parameters (w) for Degree {m}: {w_poly}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rMEmdZGHUQVa"
   },
   "source": [
    "### Visualize the Polynomial Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "-x2FvSOMEnaJ",
    "outputId": "a3f85a24-062c-4bf0-a959-662a81d08ec9"
   },
   "outputs": [],
   "source": [
    "X_fit = np.linspace(X.min(), X.max(), 200)\n",
    "X_fit_poly = polynomial_features(X_fit, m)\n",
    "y_poly_pred = X_fit_poly.dot(w_poly)  # h_w(x) = X' * w\n",
    "\n",
    "# Plot the actual data and the polynomial fit\n",
    "plt.scatter(X, y, color='blue', label='Actual Data')\n",
    "plt.plot(X_fit, y_poly_pred, color='green', label=f'Polynomial Fit (Degree {m})')\n",
    "plt.title(f\"Polynomial Regression (Degree {m})\")\n",
    "plt.xlabel(\"$x$\")\n",
    "plt.ylabel(\"$y$\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uZUPXpRHUX80"
   },
   "source": [
    "## Visualizing $E_{rms}$\n",
    "The **Root Mean Squared Error (RMSE)** measures the average magnitude of prediction errors and helps us understand how well our model fits the data.\n",
    "We will analyze how the polynomial degree affects both training and testing error.\n",
    "We could visualize the $E_{rms}$ better if we split generated data into train and test datasets\n",
    "\n",
    "‚≠ê Increasing model complexity typically reduces training error but may increase testing error (overfitting).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "4jNwnTpCEtRh",
    "outputId": "ed075c03-f621-4f2f-c164-1211d0a515a3"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define Root Mean Square Error compute function\n",
    "def compute_rms_error(y_true, y_pred):\n",
    "    return np.sqrt(np.mean((y_true - y_pred) ** 2))\n",
    "\n",
    "\n",
    "# Split the data into 80% training and 20% testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "degrees = range(0, 9)\n",
    "train_rms_errors = []\n",
    "test_rms_errors = []\n",
    "\n",
    "# Create subplots for 9 polynomial degrees (3 rows x 3 columns)\n",
    "fig, axs = plt.subplots(3, 3, figsize=(15, 12))\n",
    "axs = axs.flatten()  # Flatten to make it easier to index\n",
    "\n",
    "for i, d in enumerate(degrees):\n",
    "    # Train the model\n",
    "    w_poly = polynomial_regression(X_train, y_train, d)\n",
    "\n",
    "    # Make predictions on training set\n",
    "    X_train_poly = polynomial_features(X_train, d)\n",
    "    y_train_pred = X_train_poly.dot(w_poly)\n",
    "\n",
    "    # Make predictions on test set\n",
    "    X_test_poly = polynomial_features(X_test, d)\n",
    "    y_test_pred = X_test_poly.dot(w_poly)\n",
    "\n",
    "    # Compute RMSE\n",
    "    train_rms_error = compute_rms_error(y_train, y_train_pred)\n",
    "    test_rms_error = compute_rms_error(y_test, y_test_pred)\n",
    "\n",
    "    # Store RMSE values\n",
    "    train_rms_errors.append(train_rms_error)\n",
    "    test_rms_errors.append(test_rms_error)\n",
    "\n",
    "    # Plot in the corresponding subplot\n",
    "    ax = axs[i]\n",
    "    ax.scatter(X_train, y_train, color='blue', label=\"Training Data\")\n",
    "    ax.scatter(X_test, y_test, color='red', label=\"Test Data\", alpha=0.6)\n",
    "    X_fit = np.linspace(X.min(), X.max(), 200)\n",
    "    X_fit_poly = polynomial_features(X_fit, d)\n",
    "    y_fit_pred = X_fit_poly.dot(w_poly)\n",
    "    ax.plot(X_fit, y_fit_pred, label=f\"Degree {d} Fit\", color='green')\n",
    "    ax.set_title(f\"Degree {d} - Train RMSE: {train_rms_error:.2f}, Test RMSE: {test_rms_error:.2f}\")\n",
    "    ax.set_xlabel(\"$x$\")\n",
    "    ax.set_ylabel(\"$y$\")\n",
    "    ax.legend()\n",
    "\n",
    "# Adjust spacing between subplots\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Create a new figure for RMSE comparison (centered in 4th row visually)\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "ax.plot(degrees, train_rms_errors, marker='o', linestyle='-', color='blue', label='Train RMSE')\n",
    "ax.plot(degrees, test_rms_errors, marker='o', linestyle='-', color='red', label='Test RMSE')\n",
    "ax.set_title(\"Train vs Test RMSE vs Polynomial Degree\")\n",
    "ax.set_xlabel(\"Polynomial Degree\")\n",
    "ax.set_ylabel(\"RMSE\")\n",
    "ax.set_xticks(degrees)\n",
    "ax.grid(True)\n",
    "ax.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kqwHXRLOVTnT"
   },
   "source": [
    "As you can see from the RMSE plot above, by increasing the degree polynomial, the training error never increases **(Why ?)**. However; testing error can increase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-WQ6j36_Y2cS"
   },
   "source": [
    "## Gradient Descent:\n",
    "In this section, we will use the popular iterative method called **Gradient Descent** to solve the regression problem.\n",
    "\n",
    "Assuming we need to find $ w_0\\ and\\ w_1 $ in the problem of linear regression, update rule using gradinet descent will be:\n",
    "$$\n",
    "\\begin{array}{l}\n",
    "w_0 \\leftarrow w_0 - \\eta \\frac{\\partial J}{\\partial w_0} = w_0 - \\eta \\sum_{i=1}^{m} (h_w(x^{(i)}) - y^{(i)}) \\\\\n",
    "w_1 \\leftarrow w_1 - \\eta \\frac{\\partial J}{\\partial w_1} = w_1 - \\eta \\sum_{i=1}^{m} (h_w(x^{(i)}) - y^{(i)}) (x^{(i)}) \\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "In which $ \\eta $ is the learning rate. To overcome the problem of overflow, assume cost function is $ J(\\mathbf{w}) = \\frac{SSE}{training \\ \\ size} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y9VT8Smtalqs"
   },
   "source": [
    "### Implementing GD for linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "A9mZQFSXaaBd",
    "outputId": "e384ef32-0851-425d-f099-699c055b9100"
   },
   "outputs": [],
   "source": [
    "# SSE cost function\n",
    "def cost_function(X, y, w):\n",
    "    return np.sum((h_w(X, w) - y)**2) / len(X)\n",
    "\n",
    "# Gradient descent\n",
    "def gradient_descent(X, y, w, alpha, num_iters):\n",
    "    m = len(X)\n",
    "    cost_history = []\n",
    "    w_history = [w.copy()]\n",
    "\n",
    "    for i in range(num_iters):\n",
    "        # updates\n",
    "        gradient_w0 = np.sum(h_w(X, w) - y) / m\n",
    "        gradient_w1 = np.sum((h_w(X, w) - y) * X) / m\n",
    "        w[0] -= alpha * gradient_w0\n",
    "        w[1] -= alpha * gradient_w1\n",
    "\n",
    "        cost_history.append(cost_function(X, y, w))\n",
    "        w_history.append(w.copy())  # Store a copy of w, not the reference\n",
    "\n",
    "    return w, cost_history, w_history\n",
    "\n",
    "X, y = generate_data(n_samples=50, noise=5.0)\n",
    "w_initial = [0, 0]  # Start with w0 = 0, w1 = 0\n",
    "eta = 0.05  # Learning rate\n",
    "num_iters = 500\n",
    "\n",
    "# Run Gradient Descent\n",
    "w_final, cost_history, w_history = gradient_descent(X, y, w_initial, eta, num_iters)\n",
    "\n",
    "# Visualize cost function (log of J(w))\n",
    "w0_vals = np.linspace(-10, 20, 100)\n",
    "w1_vals = np.linspace(-1, 5, 100)\n",
    "J_vals = np.zeros((len(w0_vals), len(w1_vals)))\n",
    "\n",
    "for i in range(len(w0_vals)):\n",
    "    for j in range(len(w1_vals)):\n",
    "        w = [w0_vals[i], w1_vals[j]]\n",
    "        J_vals[i, j] = cost_function(X, y, w)\n",
    "\n",
    "# Plot GD Progression (without labels for lines, different alphas)\n",
    "plt.scatter(X, y, color='blue', label='Actual Data')\n",
    "\n",
    "# Plot lines for every 50th step with increasing alpha\n",
    "for idx, w in enumerate(w_history[::num_iters // 100]):\n",
    "    alpha = 0.15 + 0.85*(idx) / 100  # Gradually increase alpha for each line\n",
    "    plt.plot(X, h_w(X, w), color='red', alpha=alpha)\n",
    "\n",
    "# Final line in bold\n",
    "plt.plot(X, h_w(X, w_final), color='red', lw=2, label='Final Line')\n",
    "\n",
    "plt.title(\"GD Progression - Linear Regression\")\n",
    "plt.xlabel(\"$x$\")\n",
    "plt.ylabel(\"$y$\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2YuQ-dfdeCuI"
   },
   "source": [
    "As you can see from the lines above, we first started by a neutral hypothesis which was a simple line $ y = 0 $. The update at each iteration tries to minimize cost function thus improving weights. As you can see, the final line after 500 iterations is the line best describing datapoints. **But how can we be sure that cost function is optimizable ?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Vv5zacveny5"
   },
   "source": [
    "### Plotting cost function\n",
    "To get a better sense of SSE cost function, let's visualize it for univariate linear regression discussed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 598
    },
    "id": "d0Ha9m2HdfZb",
    "outputId": "5c16246d-9278-4182-e344-16eab17058c5"
   },
   "outputs": [],
   "source": [
    "# 3D Plot of J(w)\n",
    "fig = plt.figure(figsize=(10, 7))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "W0, W1 = np.meshgrid(w0_vals, w1_vals)\n",
    "ax.plot_surface(W0, W1, J_vals.T, cmap='viridis')\n",
    "ax.set_xlabel('w0')\n",
    "ax.set_ylabel('w1')\n",
    "ax.set_zlabel('log(J(w))')\n",
    "plt.title(\"Cost Function Surface\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TTo1t_rzxBB6"
   },
   "source": [
    "The plot above can be somewhat confusing. Let us visualize $ log J(\\mathbf{w}) $ instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 598
    },
    "id": "barPTPqpxKpS",
    "outputId": "e87f0f3a-3589-4889-874f-066d77628cfc"
   },
   "outputs": [],
   "source": [
    "# 3D Plot of log J(w)\n",
    "fig = plt.figure(figsize=(10, 7))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "W0, W1 = np.meshgrid(w0_vals, w1_vals)\n",
    "ax.plot_surface(W0, W1, np.log(J_vals.T), cmap='viridis')\n",
    "ax.set_xlabel('w0')\n",
    "ax.set_ylabel('w1')\n",
    "ax.set_zlabel('log(J(w))')\n",
    "plt.title(\"Cost Function Surface (Log Scale)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-A384rwRxUDv"
   },
   "source": [
    "Gradient descent tries to reach minimum point of the plot above in each step. But are we actually reaching our goal ? Let's visualize it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 598
    },
    "id": "LwtmhsqByRbr",
    "outputId": "84a681bc-65ce-44c9-9712-755b315a521b"
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "fig = plt.figure(figsize=(10, 7))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "W0, W1 = np.meshgrid(w0_vals, w1_vals)\n",
    "ax.plot_surface(W0, W1, np.log(J_vals.T), cmap='viridis', alpha=0.25)\n",
    "ax.set_xlabel('w0')\n",
    "ax.set_ylabel('w1')\n",
    "ax.set_zlabel('log(J(w))')\n",
    "plt.title(\"Cost Function Surface (Log Scale)\")\n",
    "\n",
    "# Plot the points on the 3D surface for each GD iteration\n",
    "w_history_array = np.array(w_history)  # Convert list to array for easier slicing\n",
    "w0_history = w_history_array[:, 0]\n",
    "w1_history = w_history_array[:, 1]\n",
    "cost_history_log = np.log(np.array(cost_history))  # Log of the cost history\n",
    "\n",
    "# Plot the path of gradient descent in 3D\n",
    "ax.plot(w0_history[:num_iters], w1_history[:num_iters], cost_history_log, marker='o', color='r', label='GD Path', markersize=3)\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9arBxt2qy7OT"
   },
   "source": [
    "As shown in the plot above, GD is trying to reach optimal point at each step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YzzhUJuJzWkC"
   },
   "source": [
    "### Effect of learning rate ($ \\eta $)\n",
    "Choosing $ \\eta $ could be tricky.\n",
    "\n",
    "**Large learning rates** can lead to divergence.\n",
    "\n",
    "**Small learning rates** on the other hand could slow down the convergence by requiring more iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "yENS5XjH0J7r",
    "outputId": "ba1eefd6-99b4-45d1-9799-5f14efdd1b48"
   },
   "outputs": [],
   "source": [
    "# Define different learning rates to test\n",
    "learning_rates = [0.1, 0.02, 0.001]\n",
    "\n",
    "# Number of iterations for gradient descent\n",
    "num_iters = 100\n",
    "\n",
    "# Initial weights (w0 and w1)\n",
    "w_initial = [0, 0]\n",
    "\n",
    "# Colors for plotting each learning rate's results\n",
    "colors = ['purple', 'green', 'orange']\n",
    "\n",
    "# Create a grid of possible w0 and w1 values for visualizing the cost function\n",
    "w0_vals = np.linspace(-10, 20, 100)\n",
    "w1_vals = np.linspace(-1, 5, 100)\n",
    "J_vals = np.zeros((len(w0_vals), len(w1_vals)))\n",
    "\n",
    "# Compute the cost function for each pair of (w0, w1)\n",
    "for i in range(len(w0_vals)):\n",
    "    for j in range(len(w1_vals)):\n",
    "        w = [w0_vals[i], w1_vals[j]]\n",
    "        J_vals[i, j] = cost_function(X, y, w)\n",
    "\n",
    "# To store the cost history for each learning rate\n",
    "cost_histories = []\n",
    "\n",
    "# Perform Gradient Descent for each learning rate (eta)\n",
    "for idx, eta in enumerate(learning_rates):\n",
    "\n",
    "    # Run gradient descent starting from the same initial weights\n",
    "    w_final, cost_history, w_history = gradient_descent(X, y, w_initial.copy(), eta, num_iters)\n",
    "    cost_histories.append(cost_history)\n",
    "\n",
    "    # Plot the regression lines as they evolve during gradient descent\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    # Plot intermediate lines (100 samples across the iterations)\n",
    "    for step_idx, w in enumerate(w_history[::num_iters // 100]):\n",
    "        # Set transparency for each line\n",
    "        alpha_val = 0.15 + 0.85*(idx) / 100\n",
    "        plt.plot(X, h_w(X, w), color=colors[idx], alpha=alpha_val)\n",
    "\n",
    "    plt.plot(X, h_w(X, w_final), lw=2, label=f'Final Line (eta={eta})', color=colors[idx])\n",
    "    plt.title(f\"Lines during Gradient Descent (Learning Rate {eta})\")\n",
    "    plt.xlabel(\"$x$\")\n",
    "    plt.ylabel(\"$y$\")\n",
    "    plt.legend()\n",
    "    plt.scatter(X, y, color='blue', label='Actual Data')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rc90d9vW7qPX"
   },
   "source": [
    "As shown in the plots above, choosing a large learning rate leads to divergence. In this example, the update rule keeps making weights larger and larger and the weights will never converge. Choosing a small learning rate on the other hand, leads to slow convergence. In this example, learning $ w_0 $ is happening at a slow time because the update rule is being changed almost minimially !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "5PMs-qu_86WB",
    "outputId": "9d62c0bb-6333-4171-98f8-ceff48578641"
   },
   "outputs": [],
   "source": [
    "plt.title(\"Cost Function (log scale) over Iterations for Different Learning Rates\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"log(J(w))\")\n",
    "for idx in range(len(cost_histories)):\n",
    "  plt.plot(np.log(cost_histories[idx]), label=f'eta={learning_rates[idx]}', color=colors[idx])\n",
    "plt.ylim(bottom=2, top=10)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QNHOkmgtI-pt"
   },
   "source": [
    "## Regularization: Ridge and Lasso Regression\n",
    "In this section, we will try to visualize the effect of regularization using **L1 norm (Lasso regression)** and **L2 norm (Ridge regression)**.\n",
    " Let us have a small number of datapoints and try to fit a complex model to it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jUOv5aW-Jg-i"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "def generate_data(n=100, noise=10.0):\n",
    "    np.random.seed(42)\n",
    "    X = np.random.uniform(-10, 10, n)\n",
    "    y = X**2 - 2 * X + np.random.randn(n) * noise  # x**2 - 2*x + noise\n",
    "    return X, y\n",
    "\n",
    "X, y = generate_data(n=15)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "emmh2wbz-gdJ",
    "outputId": "9075c753-13a1-4dc7-9fe8-6bbe4a558958"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "\n",
    "degrees = [2, 6, 8]\n",
    "lambdas = [1e4, 1, 1e-4, 1e-8]\n",
    "\n",
    "ridge_rmse_train = np.zeros((len(degrees), len(lambdas)))\n",
    "ridge_rmse_test = np.zeros((len(degrees), len(lambdas)))\n",
    "lasso_rmse_train = np.zeros((len(degrees), len(lambdas)))\n",
    "lasso_rmse_test = np.zeros((len(degrees), len(lambdas)))\n",
    "\n",
    "for degree_idx, degree in enumerate(degrees):\n",
    "    fig, axs = plt.subplots(1, 4, figsize=(20, 5))  # Create a 1x4 grid of subplots\n",
    "    for lambda_idx, lambda_val in enumerate(lambdas):\n",
    "        poly_features = PolynomialFeatures(degree=degree)\n",
    "        X_train_poly = poly_features.fit_transform(X_train[:, np.newaxis])\n",
    "        X_test_poly = poly_features.transform(X_test[:, np.newaxis])\n",
    "\n",
    "        # Ridge Regression using scikit-learn\n",
    "        ridge_model = Ridge(alpha=lambda_val)\n",
    "        ridge_model.fit(X_train_poly, y_train)\n",
    "        y_train_pred_ridge = ridge_model.predict(X_train_poly)\n",
    "        y_test_pred_ridge = ridge_model.predict(X_test_poly)\n",
    "\n",
    "        # Lasso Regression using scikit-learn\n",
    "        lasso_model = Lasso(alpha=lambda_val, max_iter=10000)\n",
    "        lasso_model.fit(X_train_poly, y_train)\n",
    "        y_train_pred_lasso = lasso_model.predict(X_train_poly)\n",
    "        y_test_pred_lasso = lasso_model.predict(X_test_poly)\n",
    "\n",
    "        ridge_rmse_train[degree_idx, lambda_idx] = compute_rms_error(y_train, y_train_pred_ridge)\n",
    "        ridge_rmse_test[degree_idx, lambda_idx] = compute_rms_error(y_test, y_test_pred_ridge)\n",
    "        lasso_rmse_train[degree_idx, lambda_idx] = compute_rms_error(y_train, y_train_pred_lasso)\n",
    "        lasso_rmse_test[degree_idx, lambda_idx] = compute_rms_error(y_test, y_test_pred_lasso)\n",
    "\n",
    "        # Plot the fitted curves for both Ridge and Lasso\n",
    "        X_plot = np.linspace(-10, 10, 100)\n",
    "        X_plot_poly = poly_features.transform(X_plot[:, np.newaxis])\n",
    "\n",
    "        y_plot_ridge = ridge_model.predict(X_plot_poly)\n",
    "        y_plot_lasso = lasso_model.predict(X_plot_poly)\n",
    "\n",
    "        ax = axs[lambda_idx]\n",
    "        ax.scatter(X_train, y_train, color='blue', label='Train Data')\n",
    "        ax.scatter(X_test, y_test, color='green', label='Test Data')\n",
    "        ax.plot(X_plot, y_plot_ridge, color='red', label=f'Ridge (Œª={lambda_val})')\n",
    "        ax.plot(X_plot, y_plot_lasso, color='orange', linestyle='--', label=f'Lasso (Œª={lambda_val})')\n",
    "        ax.set_title(f'Polynomial Degree {degree} - Œª={lambda_val}')\n",
    "        ax.set_xlabel('X')\n",
    "        ax.set_ylabel('y')\n",
    "        ax.legend()\n",
    "\n",
    "    plt.suptitle(f'Polynomial Degree {degree} - Regularization Comparison')\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "UkDo0fRxKida",
    "outputId": "9864bf4e-a664-41f2-eb04-251512c6798e"
   },
   "outputs": [],
   "source": [
    "# plot RMSE\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "for degree_idx, degree in enumerate(degrees):\n",
    "    plt.plot(lambdas, ridge_rmse_test[degree_idx], marker='x', label=f'Ridge - Degree {degree}')\n",
    "plt.xscale('log')\n",
    "plt.xlabel('Regularization Parameter (Œª)')\n",
    "plt.ylabel('RMSE')\n",
    "plt.title('RMSE for Ridge at Different Polynomial Degrees')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "for degree_idx, degree in enumerate(degrees):\n",
    "  plt.plot(lambdas, lasso_rmse_test[degree_idx], marker='x', label=f'Lasso - Degree {degree}')\n",
    "\n",
    "plt.xscale('log')\n",
    "plt.xlabel('Regularization Parameter (Œª)')\n",
    "plt.ylabel('RMSE')\n",
    "plt.title('RMSE for Lasso at Different Polynomial Degrees')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tMkQJeyKI0Gq"
   },
   "source": [
    "# Real-World Example 1: California House Price Prediction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5xmDnCi5T4Qu"
   },
   "source": [
    "## Dataset\n",
    "We'll use the California Housing Dataset available in scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wpXdXvlhI1Va",
    "outputId": "ca61821b-47cc-4531-cc41-c2df8ab4d394"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "X, y = housing.data, housing.target\n",
    "\n",
    "feature_names = housing.feature_names\n",
    "print(\"Feature names:\", feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "laP4V9RyXoP3"
   },
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JD03BaDdJDSJ"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iQj4xj_bXrt2"
   },
   "source": [
    "## Implement Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zUabEaciJGMZ",
    "outputId": "689f2d2b-43e5-4ce4-a5cb-eadb90f536ee"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(f\"Test RMSE: {rmse:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XtD8-Os7Xx_I"
   },
   "source": [
    "## Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tOIwdqQ9JI7p",
    "outputId": "4c27cabb-7350-4be7-ab3e-f00be7205e97"
   },
   "outputs": [],
   "source": [
    "coefficients = model.coef_\n",
    "for name, coef in zip(feature_names, coefficients):\n",
    "    print(f\"{name}: {coef:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SwuIdGmnXt3x"
   },
   "source": [
    "## Visualize Actual vs Predicted Prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "id": "I-ootgVLJKMK",
    "outputId": "4ff2daf3-c598-449a-c25d-e814b995b106"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(y_test, y_pred, alpha=0.5)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2)\n",
    "plt.xlabel(\"Actual Prices\")\n",
    "plt.ylabel(\"Predicted Prices\")\n",
    "plt.title(\"Actual vs Predicted House Prices\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i-g63Weeisqj"
   },
   "source": [
    "# Real-World Example 2: Tehran House Price Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B30LdQchH7FQ"
   },
   "source": [
    "Fot this section, we will use regression to predict hourse prices in different regions of Tehran. The [Dataset](https://www.kaggle.com/datasets/mokar2001/house-price-tehran-iran) used is records on Divar website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_oKsn_Pfe9qk"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "qOZjgUxjbi6z",
    "outputId": "4dca2eb6-1162-4c79-c38e-f84d4efb2e3e"
   },
   "outputs": [],
   "source": [
    "file_path = './assets/housePrice.xlsx'\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jquORkhAehW1",
    "outputId": "25ed26a2-0915-4a42-a9a2-57d996c6fa76"
   },
   "outputs": [],
   "source": [
    "print(df['Area'].describe())\n",
    "print(df[df['Area'] > 1e6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "Hh1fvdcEpFcP",
    "outputId": "44d391c3-c2d3-423e-fcf8-6f31c5b36b66"
   },
   "outputs": [],
   "source": [
    "# Data cleaning - removing outliers based on IQR\n",
    "Q1_area = df['Area'].quantile(0.25)\n",
    "Q3_area = df['Area'].quantile(0.75)\n",
    "IQR_area = Q3_area - Q1_area\n",
    "\n",
    "Q1_price = df['PriceUSD'].quantile(0.25)\n",
    "Q3_price = df['PriceUSD'].quantile(0.75)\n",
    "IQR_price = Q3_price - Q1_price\n",
    "\n",
    "lower_bound_area = Q1_area - 1.5 * IQR_area\n",
    "upper_bound_area = Q3_area + 1.5 * IQR_area\n",
    "\n",
    "lower_bound_price = Q1_price - 1.5 * IQR_price\n",
    "upper_bound_price = Q3_price + 1.5 * IQR_price\n",
    "\n",
    "df_cleaned = df[(df['Area'] >= lower_bound_area) & (df['Area'] <= upper_bound_area) &\n",
    "                (df['PriceUSD'] >= lower_bound_price) & (df['PriceUSD'] <= upper_bound_price)]\n",
    "\n",
    "# Check the cleaned dataset\n",
    "df_cleaned.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 468
    },
    "id": "IYTsJY3npKRi",
    "outputId": "3a42387e-485f-4a7b-82b8-949ce7c21d1d"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(df_cleaned['Area'], bins=20, color='blue', alpha=0.7)\n",
    "plt.title('Distribution of Area')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(df_cleaned['PriceUSD'], bins=20, color='green', alpha=0.7)\n",
    "plt.title('Distribution of PriceUSD')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-Z2sD9bdbbqI"
   },
   "outputs": [],
   "source": [
    "# Function to compute the Root Mean Squared Error (RMSE)\n",
    "def compute_rms_error(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "# Function to create polynomial features\n",
    "def polynomial_features(X, degree):\n",
    "    poly = PolynomialFeatures(degree=degree)\n",
    "    return poly.fit_transform(X)\n",
    "\n",
    "# Function to perform polynomial regression\n",
    "def polynomial_regression(X, y, degree):\n",
    "    poly = PolynomialFeatures(degree=degree)\n",
    "    X_poly = poly.fit_transform(X)\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_poly, y)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "fJcLWxUqbdvG",
    "outputId": "3ebaa7cf-03fa-4724-f4f2-d2513aeedd05"
   },
   "outputs": [],
   "source": [
    "# Group by 'Address' and perform analysis for each region\n",
    "addresses = df_cleaned['Address'].unique()\n",
    "\n",
    "for address in addresses:\n",
    "    df_address = df_cleaned[df_cleaned['Address'] == address]\n",
    "\n",
    "    # Skip if not enough data points\n",
    "    if len(df_address) < 2:\n",
    "        print(f\"Skipping address {address} due to insufficient samples.\")\n",
    "        continue\n",
    "\n",
    "    X = df_address[['Area']]\n",
    "    y = df_address['PriceUSD']\n",
    "\n",
    "    # Perform train-test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Skip if train or test sets are empty\n",
    "    if len(X_train) < 50 or len(X_test) < 25:\n",
    "        # print(f\"Skipping address {address} due to train-test split issues.\")\n",
    "        continue\n",
    "    else:\n",
    "      print(f\"Processing address: {address}\")\n",
    "\n",
    "    # Set polynomial degrees to evaluate\n",
    "    degrees = [2, 3, 5, 8, 10]\n",
    "\n",
    "    train_rms_errors = []\n",
    "    test_rms_errors = []\n",
    "\n",
    "    for degree in degrees:\n",
    "        model = polynomial_regression(X_train, y_train, degree)\n",
    "\n",
    "        X_train_poly = polynomial_features(X_train, degree)\n",
    "        y_train_pred = model.predict(X_train_poly)\n",
    "\n",
    "        X_test_poly = polynomial_features(X_test, degree)\n",
    "        y_test_pred = model.predict(X_test_poly)\n",
    "\n",
    "        train_rms_error = compute_rms_error(y_train, y_train_pred)\n",
    "        test_rms_error = compute_rms_error(y_test, y_test_pred)\n",
    "\n",
    "        train_rms_errors.append(train_rms_error)\n",
    "        test_rms_errors.append(test_rms_error)\n",
    "\n",
    "        # print(f\"Address: {address}, Degree {degree}: Train RMSE = {train_rms_error:.2f}, Test RMSE = {test_rms_error:.2f}\")\n",
    "\n",
    "        # Visualize Polynomial Regression for each degree\n",
    "    fig, axs = plt.subplots(1, len(degrees), figsize=(20, 5))  # Create a grid for subplots\n",
    "\n",
    "    for idx, degree in enumerate(degrees):\n",
    "        model = polynomial_regression(X_train, y_train, degree)\n",
    "\n",
    "        X_train_poly = polynomial_features(X_train, degree)\n",
    "        y_train_pred = model.predict(X_train_poly)\n",
    "\n",
    "        X_test_poly = polynomial_features(X_test, degree)\n",
    "        y_test_pred = model.predict(X_test_poly)\n",
    "\n",
    "        # Scatter plot of actual data and polynomial fit\n",
    "        axs[idx].scatter(X_train, y_train, color='blue', label=\"Training Data\")\n",
    "        axs[idx].scatter(X_test, y_test, color='red', label=\"Test Data\", alpha=0.6)\n",
    "        X_fit = np.linspace(X.min(), X.max(), 200).reshape(-1, 1)\n",
    "        X_fit_poly = polynomial_features(X_fit, degree)\n",
    "        y_fit_pred = model.predict(X_fit_poly)\n",
    "        axs[idx].plot(X_fit, y_fit_pred, label=f\"Degree {degree} Fit\", color='green')\n",
    "        axs[idx].set_title(f\"{address} - Degree {degree}\")\n",
    "        axs[idx].set_xlabel(\"Area\")\n",
    "        axs[idx].set_ylabel(\"PriceUSD\")\n",
    "        axs[idx].legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    # Plot Train RMSE and Test RMSE vs Polynomial Degree for each address\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(degrees, train_rms_errors, marker='o', label='Train RMSE', color='blue')\n",
    "    plt.plot(degrees, test_rms_errors, marker='o', label='Test RMSE', color='red')\n",
    "    plt.title(f\"RMSE vs Degree of Polynomial for {address}\")\n",
    "    plt.xlabel(\"Polynomial Degree\")\n",
    "    plt.ylabel(\"RMSE\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2MLMISYpIUIP"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "tMkQJeyKI0Gq",
    "i-g63Weeisqj"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

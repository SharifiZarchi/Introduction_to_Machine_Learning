{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yEhXpMmWKBSB"
   },
   "source": [
    "<br>\n",
    "<font>\n",
    "<!-- <img src=\"https://sina.sharif.edu/~m_salehi/images/logo_sharif.png\" alt=\"SUT logo\" width=300 height=300 align=left class=\"saturate\"> -->\n",
    "<div dir=ltr align=center>\n",
    "<img src=\"https://sina.sharif.edu/~m_salehi/images/logo_sharif.png\" width=160 height=180>\n",
    "<br>\n",
    "<font color=0F5298 size=6>\n",
    "Introduction to Machine Learning <br>\n",
    "<font color= 6C3BAA size=6>\n",
    "Linear Classification <br>\n",
    "<font color=696880 size=5>\n",
    "<!-- <br> -->\n",
    "Computer Engineering Department\n",
    "<br>\n",
    "Sharif University of Technology\n",
    "\n",
    "<font color=696880 size=5>\n",
    "<br>\n",
    "CE 40477 - Fall 2025\n",
    "\n",
    "<font color=GREEN size=5>\n",
    "<br>\n",
    "Mahdi Aghaei & Farzan Rahmani\n",
    "<!-- <br> -->\n",
    "\n",
    "____\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zHSx2ScuHEjT"
   },
   "source": [
    "# Table of Contents\n",
    "\n",
    "\n",
    "*   ## [Example: Linear Regression fails in Linear Classification](#scrollTo=4yJjsFY8WHg2)\n",
    "*   ## [Linearly Separable and Non-linearly Separable Data](#scrollTo=O4TzTi2JkYZs&line=1&uniqifier=1)\n",
    "    *   ### [Linearly Separable Data](#scrollTo=NRAV7FDF9b92&line=1&uniqifier=1)\n",
    "    *  ### [Non-Linearly Separable Data (XOR Problem)](#scrollTo=_-jBpjjo_0tz)\n",
    "    * ### [Non-Linearly Separable Data (Circular data points)](#scrollTo=v8xUV1aXflch&line=1&uniqifier=1)\n",
    "\n",
    "*  ## [Perceptron Classifier from scratch](#scrollTo=dl7K_EQrakrj&line=1&uniqifier=1)\n",
    "*  ## [Imbalanced Data](#scrollTo=_cFA0yEGqluU&line=1&uniqifier=1)\n",
    "    *   ### [Oversampling(SMOTE)](#scrollTo=oV5KIwGN1xFE&line=1&uniqifier=1)\n",
    "    *  ### [Undersampling](#scrollTo=3oDp8ldz17RP&line=1&uniqifier=1)\n",
    "*   ## [Real-World Example: Breast Cancer](#scrollTo=SaNNKHinmSxQ&line=1&uniqifier=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4yJjsFY8WHg2"
   },
   "source": [
    "# Example: Linear Regression fails in Linear Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xnVz-DVsWKaf"
   },
   "source": [
    "In this section, we would like to visualize why using techniques of linear regression (using SSE cost function) fails in linear classification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 573
    },
    "id": "4IZXKdfdWUdq",
    "outputId": "316f1a7f-3c9c-4194-d602-601c5233600d"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate 100 samples for Class A from a Gaussian distribution centered at (4, 4) with low variance\n",
    "class_A = np.random.normal(loc=(4, 4), scale=0.25, size=(100, 2))\n",
    "labels_A = np.ones(class_A.shape[0])  # Label for Class A is 1\n",
    "\n",
    "# Generate 50 samples for Class B from a Gaussian distribution centered at (-1, -1) with higher variance\n",
    "class_B = np.random.normal(loc=(-1, -1), scale=1.5, size=(50, 2))\n",
    "labels_B = np.zeros(class_B.shape[0])  # Label for Class B is 0\n",
    "\n",
    "# Combine features and labels of both classes\n",
    "X = np.vstack((class_A, class_B))  # Combine data\n",
    "y = np.hstack((labels_A, labels_B))  # Combine labels\n",
    "\n",
    "# Plot the generated data points\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(class_A[:, 0], class_A[:, 1], label='Class A', color='blue')\n",
    "plt.scatter(class_B[:, 0], class_B[:, 1], label='Class B', color='red')\n",
    "plt.title(\"Generated Data: Class A and Class B\")\n",
    "plt.xlabel(\"x1\")\n",
    "plt.ylabel(\"x2\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KxRdPIUcWeed"
   },
   "source": [
    "As you can see, the data points are linearly separable in many ways. Now let's fit a line to them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 590
    },
    "id": "jWDwiVKHWlgG",
    "outputId": "43f80041-9975-4878-b0d8-b96a9d5ad683"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Train a linear regression model to classify the data\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "# Get model parameters: weights (w1, w2) and bias (w0)\n",
    "w1, w2 = model.coef_\n",
    "w0 = model.intercept_\n",
    "\n",
    "print(f\"Model weights: w1 = {w1:.2f}, w2 = {w2:.2f}, bias = {w0:.2f}\")\n",
    "\n",
    "# Calculate decision boundary: set model output to 0.5 and solve for x2\n",
    "x_vals = np.linspace(-4, 6, 100)\n",
    "decision_boundary = (-w1 * x_vals - w0 + 0.5) / w2\n",
    "\n",
    "# Plot the data and the decision boundary\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(class_A[:, 0], class_A[:, 1], label='Class A', color='blue')\n",
    "plt.scatter(class_B[:, 0], class_B[:, 1], label='Class B', color='red')\n",
    "plt.plot(x_vals, decision_boundary, label='Decision Boundary (SSE)', color='green')\n",
    "plt.title(\"Linear Classifier Using SSE for Classification\")\n",
    "plt.xlabel(\"x1\")\n",
    "plt.ylabel(\"x2\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HqCsxUmSWw8o"
   },
   "source": [
    "Even the best fitted line fails to classify points. **Can you explain why ?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O4TzTi2JkYZs"
   },
   "source": [
    "# Linearly Separable and Non-linearly Separable Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jk9NyHbQ9t6C"
   },
   "source": [
    "In this section, we would visualize linearly separable and non-linearly separable data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NRAV7FDF9b92"
   },
   "source": [
    "## Linearly Separable Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 727
    },
    "id": "UjaANj2tmSd2",
    "outputId": "4090a5a8-2281-4604-a912-2dcd03aada2e"
   },
   "outputs": [],
   "source": [
    "# Function to generate two classes based on the condition y > x and y < x\n",
    "def generate_ab_class(n_points=100):\n",
    "    class_A = []  # Class A: points where y > x\n",
    "    class_B = []  # Class B: points where y < x\n",
    "    while len(class_A) < n_points or len(class_B) < n_points:\n",
    "        x = np.random.uniform(0, 10)  # Random x in [0, 10]\n",
    "        y = np.random.uniform(0, 10)  # Random y in [0, 10]\n",
    "        if y > x and len(class_A) < n_points:\n",
    "            class_A.append([x, y])\n",
    "        elif y < x and len(class_B) < n_points:\n",
    "            class_B.append([x, y])\n",
    "    return np.array(class_A), np.array(class_B)\n",
    "\n",
    "# Generate the points for both classes\n",
    "class_A, class_B = generate_ab_class()\n",
    "\n",
    "# Plot the points and the decision boundary y = x\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.scatter(class_A[:, 0], class_A[:, 1], color='green', label='Class A (y > x)')\n",
    "plt.scatter(class_B[:, 0], class_B[:, 1], color='orange', label='Class B (y < x)')\n",
    "plt.plot([0, 10], [0, 10], color='black', linestyle='--', label='Decision Boundary (y = x)')\n",
    "plt.xlabel('X-axis')\n",
    "plt.ylabel('Y-axis')\n",
    "plt.title('Classification Based on y > x and y < x')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_-jBpjjo_0tz"
   },
   "source": [
    "## Non-Linearly Separable Data (XOR Problem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 727
    },
    "id": "3zFLNdIw-2v7",
    "outputId": "92b219e2-2dc2-45ae-b232-3098781ca9f7"
   },
   "outputs": [],
   "source": [
    "# Function to generate XOR problem data (not linearly separable)\n",
    "def generate_xor_data(n_points=200, seed=42):\n",
    "    np.random.seed(seed)\n",
    "    class_A = []  # Class A: points in same quadrants (top-right or bottom-left)\n",
    "    class_B = []  # Class B: points in opposite quadrants (top-left or bottom-right)\n",
    "    while len(class_A) < n_points or len(class_B) < n_points:\n",
    "        x = np.random.uniform(0, 1)  # Random x in [0, 1]\n",
    "        y = np.random.uniform(0, 1)  # Random y in [0, 1]\n",
    "        # XOR condition:\n",
    "        if (x > 0.5 and y > 0.5) or (x < 0.5 and y < 0.5):  # same side of threshold\n",
    "            if len(class_A) < n_points:\n",
    "                class_A.append([x, y])\n",
    "        else:  # opposite sides of threshold\n",
    "            if len(class_B) < n_points:\n",
    "                class_B.append([x, y])\n",
    "    return np.array(class_A), np.array(class_B)\n",
    "\n",
    "# Generate XOR data\n",
    "class_A_xor, class_B_xor = generate_xor_data()\n",
    "\n",
    "# Plot XOR data with an attempted linear decision boundary (y = x)\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.scatter(class_A_xor[:, 0], class_A_xor[:, 1], color='green', label='Class A (XOR)')\n",
    "plt.scatter(class_B_xor[:, 0], class_B_xor[:, 1], color='orange', label='Class B (XOR)')\n",
    "plt.plot([0, 1], [0, 1], color='black', linestyle='--', label='Attempted Decision Boundary (y = x)')\n",
    "plt.xlabel('X-axis')\n",
    "plt.ylabel('Y-axis')\n",
    "plt.title('XOR Classification and Linear Decision Boundary')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v8xUV1aXflch"
   },
   "source": [
    "## Non-Linearly Separable Data (Circular data points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 727
    },
    "id": "N06MLwbtffco",
    "outputId": "1dff0717-82b3-44ef-de07-1a200b96e5ac"
   },
   "outputs": [],
   "source": [
    "# Function to generate two classes of 2D points:\n",
    "# - Class 0: points within a circle (radius 5)\n",
    "# - Class 1: points in a ring/annulus (radius between 8 and 10)\n",
    "def generate_data(n_points=200, seed=42):\n",
    "    np.random.seed(seed)  # Set seed for reproducibility\n",
    "\n",
    "    # Class 0: Points inside a circle of radius 5\n",
    "    radius_0 = 5\n",
    "    theta_0 = np.random.uniform(0, 2 * np.pi, n_points)  # Random angles [0, 2π]\n",
    "    r_0 = radius_0 * np.sqrt(np.random.uniform(0, 1, n_points))  # Uniform distribution over circle area\n",
    "    x0 = r_0 * np.cos(theta_0)  # Convert polar to Cartesian (x-coordinates)\n",
    "    y0 = r_0 * np.sin(theta_0)  # Convert polar to Cartesian (y-coordinates)\n",
    "    class_0 = np.vstack((x0, y0)).T  # Stack into a 2D array of shape (n_points, 2)\n",
    "\n",
    "    # Class 1: Points in an annulus (ring) between radii 8 and 10\n",
    "    inner_radius_1 = 8\n",
    "    outer_radius_1 = 10\n",
    "    theta_1 = np.random.uniform(0, 2 * np.pi, n_points)  # Random angles [0, 2π]\n",
    "    # Radius values uniformly sampled from the area of the annulus\n",
    "    r_1 = np.sqrt(np.random.uniform(inner_radius_1**2, outer_radius_1**2, n_points))\n",
    "    x1 = r_1 * np.cos(theta_1)  # Convert to x-coordinates\n",
    "    y1 = r_1 * np.sin(theta_1)  # Convert to y-coordinates\n",
    "    class_1 = np.vstack((x1, y1)).T  # Stack into shape (n_points, 2)\n",
    "\n",
    "    return class_0, class_1  # Return both classes as numpy arrays\n",
    "\n",
    "# Generate the dataset\n",
    "class_0, class_1 = generate_data()\n",
    "\n",
    "# Plot the generated points\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.scatter(class_0[:, 0], class_0[:, 1], color='blue', label='Class 0 (Circle)')\n",
    "plt.scatter(class_1[:, 0], class_1[:, 1], color='red', label='Class 1 (Annulus)')\n",
    "plt.xlabel('X-axis')\n",
    "plt.ylabel('Y-axis')\n",
    "plt.title('Class 0 and Class 1 Distribution')\n",
    "plt.legend()\n",
    "plt.axis('equal')  # Keep aspect ratio 1:1\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dl7K_EQrakrj"
   },
   "source": [
    "# Perceptron Classifier from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kE3P9L30amiy"
   },
   "source": [
    "In this section, we will implement a perceptron classifier from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7_dmlWigmavx"
   },
   "outputs": [],
   "source": [
    "class Percep:\n",
    "    def __init__(self, learning_rate=0.01, n_epochs=1000):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_epochs = n_epochs\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "        self.errors_ = []  # Track number of misclassifications per epoch\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Train the Perceptron model on the training data.\n",
    "\n",
    "        Parameters:\n",
    "        - X: array-like, shape = [n_samples, n_features]\n",
    "          Input features.\n",
    "        - y: array-like, shape = [n_samples]\n",
    "          Target labels: +1 or -1.\n",
    "        \"\"\"\n",
    "        n_samples, n_features = X.shape\n",
    "        self.weights = np.zeros(n_features)  # Initialize weights to zero\n",
    "        self.bias = 0.0  # Initialize bias to zero\n",
    "\n",
    "        for epoch in range(self.n_epochs):\n",
    "            errors = 0\n",
    "            for idx in range(n_samples):\n",
    "                # Calculate linear output (dot product + bias)\n",
    "                linear_output = np.dot(X[idx], self.weights) + self.bias\n",
    "                y_pred = self._unit_step(linear_output)\n",
    "\n",
    "                # If misclassified, update weights and bias\n",
    "                if y[idx] != y_pred:\n",
    "                    update = self.learning_rate * y[idx]\n",
    "                    self.weights += update * X[idx]\n",
    "                    self.bias += update\n",
    "                    errors += 1\n",
    "            self.errors_.append(errors)\n",
    "\n",
    "            # Stop early if no misclassifications\n",
    "            if errors == 0:\n",
    "                print(f\"Converged after {epoch+1} epochs\")\n",
    "                break\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict class labels using the learned weights.\n",
    "\n",
    "        Parameters:\n",
    "        - X: array-like, shape = [n_samples, n_features]\n",
    "\n",
    "        Returns:\n",
    "        - Array of predicted labels (+1 or -1)\n",
    "        \"\"\"\n",
    "        linear_output = np.dot(X, self.weights) + self.bias\n",
    "        return self._unit_step(linear_output)\n",
    "\n",
    "    def _unit_step(self, x):\n",
    "        # Unit step function: returns 1 if x >= 0, else -1\n",
    "        return np.where(x >= 0, 1, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dFITOIX48NUN"
   },
   "source": [
    "Prepare Dataset (Combine Classes A and B from Previous Cells)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z_v9nQlTTD4r",
    "outputId": "4ff538ea-c19e-4ce4-9866-0deaf0e3526e"
   },
   "outputs": [],
   "source": [
    "# Stack data from class_A and class_B\n",
    "X_ab = np.vstack((class_A, class_B))\n",
    "\n",
    "# Create labels: +1 for class A, -1 for class B\n",
    "y_ab = np.hstack((np.ones(class_A.shape[0]), -np.ones(class_B.shape[0])))\n",
    "\n",
    "# Shuffle the data and labels together to avoid ordered data\n",
    "shuffle_idx = np.random.permutation(len(X_ab))\n",
    "X_ab, y_ab = X_ab[shuffle_idx], y_ab[shuffle_idx]\n",
    "\n",
    "# Display sample data and labels\n",
    "print(\"Combined Data Sample Points:\\n\", X_ab[:5])\n",
    "print(\"Combined Labels:\\n\", y_ab[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YzFPb6zP8EMO"
   },
   "source": [
    "Train the Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HcCPCmSrTHOV",
    "outputId": "d9589bc3-7c5f-405f-f4af-c5e66cfd02e9"
   },
   "outputs": [],
   "source": [
    "# Create an instance of Perceptron with learning rate and max epochs\n",
    "percept = Percep(learning_rate=0.01, n_epochs=1000)\n",
    "\n",
    "# Train the perceptron model on the dataset\n",
    "percept.fit(X_ab, y_ab)\n",
    "\n",
    "# Display the final weights and bias\n",
    "print(f\"Final Weights: {percept.weights}\")\n",
    "print(f\"Final Bias: {percept.bias}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v0bYWdKKc9Iy"
   },
   "source": [
    "Visualize the Decision Boundary and Decision Regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 573
    },
    "id": "_hDTaxVrcqxA",
    "outputId": "323baa27-0947-4e06-86ac-35da2ae3d269"
   },
   "outputs": [],
   "source": [
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "# Define range of the plot\n",
    "x_min, x_max = X_ab[:, 0].min() - 1, X_ab[:, 0].max() + 1\n",
    "y_min, y_max = X_ab[:, 1].min() - 1, X_ab[:, 1].max() + 1\n",
    "\n",
    "# Create a grid of points to classify\n",
    "xx, yy = np.meshgrid(np.linspace(x_min, x_max, 500),\n",
    "                     np.linspace(y_min, y_max, 500))\n",
    "\n",
    "grid = np.c_[xx.ravel(), yy.ravel()]  # Flatten meshgrid into (n_samples, 2)\n",
    "Z = percept.predict(grid)          # Predict each point in the grid\n",
    "Z = Z.reshape(xx.shape)               # Reshape prediction for contour plot\n",
    "\n",
    "# Define color maps for background and points\n",
    "cmap_light = ListedColormap(['#FFAAAA', '#AAAAFF'])  # Light red/blue for regions\n",
    "cmap_bold = ListedColormap(['#FF0000', '#0000FF'])   # Bold red/blue for points\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Fill background with decision regions\n",
    "plt.contourf(xx, yy, Z, alpha=0.3, cmap=cmap_light)\n",
    "\n",
    "# Draw decision boundary (line where prediction = 0)\n",
    "if percept.weights[1] != 0:\n",
    "    x_vals = np.array([x_min, x_max])\n",
    "    y_vals = -(percept.weights[0] * x_vals + percept.bias) / percept.weights[1]\n",
    "    plt.plot(x_vals, y_vals, 'k--', label='Decision Boundary')\n",
    "else:\n",
    "    # Vertical decision boundary if weight for y is zero\n",
    "    x_val = -percept.bias / percept.weights[0]\n",
    "    plt.axvline(x=x_val, color='k', linestyle='--', label='Decision Boundary')\n",
    "\n",
    "# Plot original data points\n",
    "plt.scatter(class_A[:, 0], class_A[:, 1], color='red', marker='o', label='Class A')\n",
    "plt.scatter(class_B[:, 0], class_B[:, 1], color='blue', marker='s', label='Class B')\n",
    "\n",
    "# Axis and title\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('Perceptron Decision Boundary and Decision Regions')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_cFA0yEGqluU"
   },
   "source": [
    "#Imbalanced Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 501
    },
    "id": "OzbAvB4YqouQ",
    "outputId": "e858afeb-c20d-401b-c2ad-a9059012168d"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "# Generate a synthetic 2D binary classification dataset with class imbalance (90% vs 10%)\n",
    "X, y = make_classification(\n",
    "    n_classes=2,            # Binary classification\n",
    "    class_sep=2,            # Distance between classes (for separability)\n",
    "    weights=[0.9, 0.1],     # Class imbalance (90% class 0, 10% class 1)\n",
    "    n_informative=2,        # Number of informative features\n",
    "    n_redundant=0,          # No redundant features\n",
    "    n_features=2,           # Only 2 features for easy visualization\n",
    "    n_clusters_per_class=1,\n",
    "    n_samples=500,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Split into training and test sets, maintaining class ratio (stratify)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)\n",
    "\n",
    "# Initialize and train a Perceptron on imbalanced data\n",
    "clf = Perceptron()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Function to plot the decision boundary of a classifier\n",
    "def plot_decision_boundary(ax, X, y, model=None, title=\"\"):\n",
    "    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 300),\n",
    "                         np.linspace(y_min, y_max, 300))\n",
    "\n",
    "    if model is not None:\n",
    "        # Predict class label for each point on the grid\n",
    "        Z = model.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
    "        # Show regions based on prediction\n",
    "        ax.contourf(xx, yy, Z, cmap=ListedColormap(['#FFCCCC', '#CCCCFF']), alpha=0.5)\n",
    "\n",
    "    # Plot training data\n",
    "    scatter = ax.scatter(X[:, 0], X[:, 1], c=y, cmap=ListedColormap(['red', 'blue']), edgecolor='k')\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlim(xx.min(), xx.max())\n",
    "    ax.set_ylim(yy.min(), yy.max())\n",
    "    ax.set_xlabel(\"Feature 1\")\n",
    "    ax.set_ylabel(\"Feature 2\")\n",
    "\n",
    "# Plot the raw imbalanced data and the decision boundary learned by Perceptron\n",
    "fig, axs = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Left: original data only\n",
    "plot_decision_boundary(axs[0], X_train, y_train, model=None, title=\"Imbalanced Data Only\")\n",
    "\n",
    "# Right: decision boundary learned on imbalanced data\n",
    "plot_decision_boundary(axs[1], X_train, y_train, model=clf, title=\"Perceptron on Imbalanced Data\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oV5KIwGN1xFE"
   },
   "source": [
    "##Oversampling(SMOTE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0mTeV2N4-tXv"
   },
   "source": [
    "Apply SMOTE (Synthetic Minority Oversampling Technique):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 501
    },
    "id": "T8imfietrAXo",
    "outputId": "ee75b466-6990-4a83-a5c3-06483e2abf39"
   },
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Apply SMOTE to generate synthetic samples for the minority class\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Train a new Perceptron on the resampled (balanced) data\n",
    "clf_smote = Perceptron()\n",
    "clf_smote.fit(X_resampled, y_resampled)\n",
    "\n",
    "# Plot decision boundaries before and after SMOTE\n",
    "fig, axs = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "plot_decision_boundary(axs[0], X_train, y_train, model=clf, title=\"Before SMOTE (Imbalanced)\")\n",
    "plot_decision_boundary(axs[1], X_resampled, y_resampled, model=clf_smote, title=\"After SMOTE (Oversampled)\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3oDp8ldz17RP"
   },
   "source": [
    "##Undersampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U9sn4PtY-obm"
   },
   "source": [
    "Apply Random Undersampling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 501
    },
    "id": "gZCCleB7rPFh",
    "outputId": "3b03534d-2c43-4e43-d66d-7e7ace9818ef"
   },
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "# Apply random undersampling to reduce the majority class\n",
    "undersample = RandomUnderSampler(random_state=42)\n",
    "X_under, y_under = undersample.fit_resample(X_train, y_train)\n",
    "\n",
    "# Train a new Perceptron on the undersampled (balanced) data\n",
    "clf_under = Perceptron()\n",
    "clf_under.fit(X_under, y_under)\n",
    "\n",
    "# Plot decision boundaries before and after undersampling\n",
    "fig, axs = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "plot_decision_boundary(axs[0], X_train, y_train, model=clf, title=\"Before Undersampling (Imbalanced)\")\n",
    "plot_decision_boundary(axs[1], X_under, y_under, model=clf_under, title=\"After Undersampling\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SaNNKHinmSxQ"
   },
   "source": [
    "# Real-World Example: Breast Cancer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "En76tSBWmNzQ"
   },
   "source": [
    "Let us visit the breast cancer dataset again:\n",
    "   - Select two features `mean radius` and `mean texture` for visualization purposes.\n",
    "   - Split the dataset into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 573
    },
    "id": "Ce-DqxM-mWbn",
    "outputId": "6c9134a6-650e-440c-9ba6-1df4c97f6927"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "import pandas as pd\n",
    "\n",
    "# Load breast cancer dataset from scikit-learn\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target  # Labels: 0 = malignant, 1 = benign\n",
    "\n",
    "# Convert to pandas DataFrame for easy handling\n",
    "df = pd.DataFrame(X, columns=data.feature_names)\n",
    "df['target'] = y\n",
    "\n",
    "# Select two features for visualization and classification\n",
    "selected_features = ['mean radius', 'mean texture']\n",
    "X_selected = df[selected_features].values\n",
    "y_selected = y  # Use full target array\n",
    "\n",
    "# Scatter plot of the selected features\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Class 0: Malignant (red)\n",
    "plt.scatter(X_selected[y_selected == 0, 0], X_selected[y_selected == 0, 1],\n",
    "            color='red', marker='o', edgecolor='k', label='Malignant')\n",
    "\n",
    "# Class 1: Benign (blue)\n",
    "plt.scatter(X_selected[y_selected == 1, 0], X_selected[y_selected == 1, 1],\n",
    "            color='blue', marker='s', edgecolor='k', label='Benign')\n",
    "\n",
    "# Plot styling\n",
    "plt.ylim(10, 40)\n",
    "plt.xlabel('Mean Radius')\n",
    "plt.ylabel('Mean Texture')\n",
    "plt.title('Breast Cancer Data Visualization (All Samples)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iGrj1tg_aZ2v"
   },
   "source": [
    "Split data to 80% train and 20% test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8Hsyyz548Nfl",
    "outputId": "13c78cd9-29fa-42b6-89b2-7d5d75241fb9"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data (80% train, 20% test), maintaining class distribution (stratified)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_selected, y_selected, test_size=0.2, random_state=42, stratify=y_selected\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {X_train.shape[0]}\")\n",
    "print(f\"Testing samples: {X_test.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "20C8MwbrDz55"
   },
   "source": [
    "Train Perceptron on the Training Set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9--kcQw7dZE5",
    "outputId": "2678b830-fd82-4fcc-90ce-98efe02e54c8"
   },
   "outputs": [],
   "source": [
    "# Convert class labels to Perceptron-friendly format: 0 -> -1, 1 -> +1\n",
    "y_train_perceptron = np.where(y_train == 0, -1, 1)\n",
    "y_test_perceptron = np.where(y_test == 0, -1, 1)\n",
    "\n",
    "# Initialize and train a custom Perceptron (your own implementation)\n",
    "percept = Percep(learning_rate=0.01, n_epochs=1000)\n",
    "percept.fit(X_train, y_train_perceptron)\n",
    "\n",
    "# Display final learned weights and bias\n",
    "print(f\"Final Weights: {percept.weights}\")\n",
    "print(f\"Final Bias: {percept.bias}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I86Qh2-OgLiQ"
   },
   "source": [
    "As you remember, samples are not linearly separable. We can expect number of misclassifications not to converge to 0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "id": "7zVpOL8EdcrL",
    "outputId": "2d0bd418-46f1-4757-e873-c12568e173bd"
   },
   "outputs": [],
   "source": [
    "# Plot number of misclassified samples every 20 epochs\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(range(1, len(percept.errors_) + 1, 20), percept.errors_[::20], marker='o')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Number of Misclassifications')\n",
    "plt.title('Perceptron Learning Progress')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6GORBqZaDuYK"
   },
   "source": [
    "Plot Perceptron Decision Boundary on Test Set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 573
    },
    "id": "vYEeOiaZdhP1",
    "outputId": "e9eee865-378d-410e-ac2c-9def2f2d3527"
   },
   "outputs": [],
   "source": [
    "# Create a mesh grid for plotting decision regions\n",
    "x_min, x_max = X_train[:, 0].min() - 1, X_train[:, 0].max() + 1\n",
    "y_min, y_max = X_train[:, 1].min() - 1, X_train[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.linspace(x_min, x_max, 500),\n",
    "                     np.linspace(y_min, y_max, 500))\n",
    "\n",
    "# Predict labels for each point on the grid\n",
    "grid = np.c_[xx.ravel(), yy.ravel()]\n",
    "Z = percept.predict(grid)\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "# Define color maps for regions and class markers\n",
    "cmap_light = ListedColormap(['#FFAAAA', '#AAAAFF'])  # Background colors\n",
    "cmap_bold = ListedColormap(['#FF0000', '#0000FF'])   # Point colors\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Draw decision regions\n",
    "plt.contourf(xx, yy, Z, alpha=0.3, cmap=cmap_light)\n",
    "\n",
    "# Draw decision boundary as a dashed line\n",
    "if percept.weights[1] != 0:\n",
    "    x_vals = np.array([x_min, x_max])\n",
    "    y_vals = -(percept.weights[0] * x_vals + percept.bias) / percept.weights[1]\n",
    "    plt.plot(x_vals, y_vals, 'k--', label='Decision Boundary')\n",
    "else:\n",
    "    x_val = -percept.bias / percept.weights[0]\n",
    "    plt.axvline(x=x_val, color='k', linestyle='--', label='Decision Boundary')\n",
    "\n",
    "# Plot test data\n",
    "plt.scatter(X_test[y_test == 0, 0], X_test[y_test == 0, 1],\n",
    "            color='red', marker='o', edgecolor='k', label='Malignant')\n",
    "plt.scatter(X_test[y_test == 1, 0], X_test[y_test == 1, 1],\n",
    "            color='blue', marker='s', edgecolor='k', label='Benign')\n",
    "\n",
    "plt.ylim(10, 40)\n",
    "plt.xlabel('Mean Radius')\n",
    "plt.ylabel('Mean Texture')\n",
    "plt.title('Perceptron Decision Boundary and Decision Regions (Breast Cancer Dataset)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AugP8bYOcTfc"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

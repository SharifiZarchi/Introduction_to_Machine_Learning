{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# CLIP: Connecting text and images\n"
      ],
      "metadata": {
        "id": "yHuG8A6NseEX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"CLIP\" or \"Contrastive Language-Image Pretraining\" is a powerful model developed by OpenAI that bridges the gap between natural language and image understanding. It can take both images and text as inputs and relate them in meaningful ways, allowing it to perform a variety of tasks such as zero-shot image classification, image search, and more.\n",
        "\n",
        "The model is trained by learning a joint embedding space where images and their corresponding text descriptions are closely aligned. In this notebook, we will explore how CLIP works, how to use it for various tasks, and how to implement it.\n",
        "\n",
        "## why CLIP\n",
        "Traditional image classification models are typically limited to the categories they were trained on. CLIP, on the other hand, can recognize a wide variety of objects and concepts in images without being explicitly trained on specific tasks. This capability is achieved by learning from a massive dataset of image-text pairs gathered from the internet. As a result, CLIP can generalize to many tasks without needing further fine-tuning.\n",
        "\n",
        "Some key use cases for CLIP include:\n",
        "\n",
        "- Zero-shot classification: Classify images based on new categories without additional training.\n",
        "- Image search: Find images related to specific text descriptions.\n",
        "- Text-to-image mapping: Generate embeddings for both images and text, enabling cross-modal understanding."
      ],
      "metadata": {
        "id": "N4-8jRH7vJV_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Basics\n",
        "\n",
        "The raw product of CLIP is a shared representation(embedding) between two modalities (text and images) by training on a large dataset of image-text pairs.\n",
        "\n",
        "1. Input Data: CLIP is trained on a large set of image-text pairs. Each image is accompanied by a textual description (e.g., a picture of a dog and the text \"a dog sitting in a park\").\n",
        "\n",
        "2. Dual Encoder Architecture:\n",
        "  - Image Encoder: CLIP uses a Vision Transformer (ViT) or a ResNet to process the images and generate an embedding vector for each image.\n",
        "\n",
        "  - Text Encoder: A Transformer model is used to process the text descriptions and generate an embedding vector for each description.\n",
        "\n",
        " - Image of the output of dual encoders:\n",
        "![dual encoders output](https://images.ctfassets.net/kftzwdyauwt9/fbc4f633-9ad4-4dc2-3809c22df5e0/0bd2d5abf90d052731538613e4a42668/overview-a.svg)\n",
        "\n",
        "3. Contrastive Loss: The optimization objective in CLIP is contrastive learning. After both the image and the text are passed through their respective encoders to produce embeddings, CLIP uses a contrastive loss that encourages the image and its matching text to have similar embeddings, while mismatched pairs (e.g., a dog image and \"a cat sitting on a tree\") are distinguished in the embedding space.\n",
        "  \n",
        "  This similarity is usually formalised as a distance in the embedding space that is closer to zero when embedded elements are more similar.\n",
        "\n",
        "4. Joint Embedding Space: After training, CLIP learns a joint embedding space where related images and text are close together, and unrelated ones are far apart. This allows CLIP to perform tasks as:\n",
        "\n",
        "  - Zero-Shot Classification: Given a new category (e.g., \"a cat\"), CLIP can classify images by computing the similarity between the image embeddings and the text embedding of the label.\n",
        "  - Text-Image Similarity: CLIP can rank images by their similarity to a textual description or rank text by its similarity to an image.\n"
      ],
      "metadata": {
        "id": "a4xslcEFwXh2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pretrained Model\n",
        "Lets First take a look at the pretrained implementation of CLIP from OpenAI."
      ],
      "metadata": {
        "id": "hc7K_VWqyxkG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install CLIP library\n",
        "!pip install git+https://github.com/openai/CLIP.git\n",
        "!pip install torch torchvision"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eoni2qi4y4yN",
        "outputId": "b891b9b9-c411-44e1-a5bd-9def869c7bf4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/openai/CLIP.git\n",
            "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-h6qtc6gm\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-h6qtc6gm\n",
            "  Resolved https://github.com/openai/CLIP.git to commit dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting ftfy (from clip==1.0)\n",
            "  Downloading ftfy-6.2.3-py3-none-any.whl.metadata (7.8 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (24.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (4.66.5)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2.4.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (0.19.1+cu121)\n",
            "Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /usr/local/lib/python3.10/dist-packages (from ftfy->clip==1.0) (0.2.13)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (2024.6.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (10.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->clip==1.0) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->clip==1.0) (1.3.0)\n",
            "Downloading ftfy-6.2.3-py3-none-any.whl (43 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m43.0/43.0 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: clip\n",
            "  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369490 sha256=e991f329038cdc4cf8748e3f937b632432d42eaf2a991c091d50c47ab181a476\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-iocpmn9k/wheels/da/2b/4c/d6691fa9597aac8bb85d2ac13b112deb897d5b50f5ad9a37e4\n",
            "Successfully built clip\n",
            "Installing collected packages: ftfy, clip\n",
            "Successfully installed clip-1.0 ftfy-6.2.3\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.4.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.19.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (10.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import torch\n",
        "import clip\n",
        "from PIL import Image\n",
        "import requests\n",
        "from io import BytesIO"
      ],
      "metadata": {
        "id": "QcFrrGjny_D1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the model and the preprocessing function\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model, preprocess = clip.load(\"ViT-B/32\", device=device)"
      ],
      "metadata": {
        "id": "vgE-msOCzTiy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20cc39b2-850c-426e-e2a4-7c5bf0e4f4b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 338M/338M [00:02<00:00, 146MiB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's create a simple function that will accept an image URL and a list of text descriptions. The function will then calculate the similarity between the image and each text description."
      ],
      "metadata": {
        "id": "3qMBZZpezJrD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to process image and text, and compute similarity\n",
        "def match_image_text(image_url, text_descriptions):\n",
        "    # Load and preprocess the image\n",
        "    response = requests.get(image_url)\n",
        "    img = Image.open(BytesIO(response.content))\n",
        "    image = preprocess(img).unsqueeze(0).to(device)\n",
        "\n",
        "    # Tokenize and encode the text\n",
        "    text = clip.tokenize(text_descriptions).to(device)\n",
        "\n",
        "    # Run the image and the text through the model\n",
        "    with torch.no_grad():\n",
        "        image_features = model.encode_image(image)\n",
        "        text_features = model.encode_text(text)\n",
        "\n",
        "    # Compute similarity\n",
        "    image_features /= image_features.norm(dim=-1, keepdim=True)\n",
        "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
        "    similarities = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
        "\n",
        "    return similarities"
      ],
      "metadata": {
        "id": "iazHy6XWzHsQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Interactive Section: Enter an image URL and text descriptions\n",
        "# image_url = input(\"Enter an image URL: \")\n",
        "# text_descriptions = input(\"Enter text descriptions (comma-separated): \").split(',')\n",
        "\n",
        "# # Compute similarities\n",
        "# similarities = match_image_text(image_url, text_descriptions)\n",
        "\n",
        "# # Show results\n",
        "# print(f\"\\nImage URL: {image_url}\")\n",
        "# print(\"Text Descriptions and Similarity Scores:\")\n",
        "# for i, desc in enumerate(text_descriptions):\n",
        "#     print(f\"Description: {desc.strip()} | Similarity: {similarities[0, i].item():.4f}\")"
      ],
      "metadata": {
        "id": "HJU3aZloznOV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This picture shows what an idea of happened in the above code:\n",
        "![similarity scoring text based on image](https://images.ctfassets.net/kftzwdyauwt9/d9d46e4b-6d6a-4f9e-59a242ea1441/c7b386880f1af005fd02f159de7f4d00/overview-b.svg)\n",
        "\n",
        "As you (hopefully) saw the model didnt need to be trained on a dataset of your provided text and image. Hence the term zero-shot prediction.\n",
        "\n",
        "But how do `model.encode_image()` and `model.encode_text()` output the same embedding space for image and text. we will see that soon.\n"
      ],
      "metadata": {
        "id": "BISz4-MHH2ot"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Contrastive Loss\n",
        "\n",
        "The contrastive loss encourages the model to bring the embeddings of matching image-text pairs \"closer\" together and push the embeddings of non-matching pairs \"further apart\".\n",
        "\n",
        "For notions of \"closer\" and \"furthur\" in the embeddings showing a quanititative similarity between original data pairs we can use **cosine similarity** between the image and text embeddings.\n",
        "\n",
        "The contrastive loss can be formalized using the softmax function applied over the cosine similarity between the image and text embeddings. Hereâ€™s the mathematical formula for the contrastive loss in CLIP:\n",
        "\n",
        "$$\n",
        "L = - \\frac{1}{N} \\sum_{i=1}^{N} \\log \\frac{\\exp(\\text{sim}(x_i,y_i))}{\\sum_{j=1}^{N} \\exp(\\text{sim}(x_j,y_j))}\n",
        "$$\n",
        "Where:\n",
        "- ð‘ is the number of image-text pairs in the batch.\n",
        "- $x_i$ is the image embedding for the ð‘–-th image.\n",
        "- $y_i$ is the text embedding for the corresponding ð‘–-th text.\n",
        "- $\\text{sim}(ð‘¥_ð‘–,ð‘¦_ð‘—)$ is the similarity (usually cosine similarity) between the image embedding $x_i$ and the text embedding $y_j$.\n",
        "\n",
        "The loss function penalizes when the similarity between matching pairs is low or when mismatching pairs have a high similarity."
      ],
      "metadata": {
        "id": "SYY77SxRyjlc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Having seen the contrastive loss function, now we can see the training process on $N$ sample image-text pairs with below psudoe-code from the paper:\n",
        "\n",
        "```\n",
        "# image_encoder - ResNet or Vision Transformer\n",
        "# text_encoder - CBOW or Text Transformer\n",
        "# I[n, h, w, c] - minibatch of aligned images\n",
        "# T[n, l] - minibatch of aligned texts\n",
        "# W_i[d_i, d_e] - learned proj of image to embed\n",
        "# W_t[d_t, d_e] - learned proj of text to embed\n",
        "# t - learned temperature parameter\n",
        "\n",
        "# extract feature representations of each modality\n",
        "I_f = image_encoder(I) #[n, d_i]\n",
        "T_f = text_encoder(T) #[n, d_t]\n",
        "\n",
        "# joint multimodal embedding [n, d_e]\n",
        "I_e = l2_normalize(np.dot(I_f, W_i), axis=1)\n",
        "T_e = l2_normalize(np.dot(T_f, W_t), axis=1)\n",
        "\n",
        "# scaled pairwise cosine similarities [n, n]\n",
        "logits = np.dot(I_e, T_e.T) * np.exp(t)\n",
        "\n",
        "# symmetric loss function\n",
        "labels = np.arange(n)\n",
        "loss_i = cross_entropy_loss(logits, labels, axis=0)\n",
        "loss_t = cross_entropy_loss(logits, labels, axis=1)\n",
        "loss = (loss_i + loss_t)/2\n",
        "```\n",
        "\n",
        "The logits created is the matrix or table u saw in the first picture, where on the diagonal is the cosine similarity of matching pairs and the off diagonal elements are missmatched pairs similarities.\n"
      ],
      "metadata": {
        "id": "6eL3W2i5Q0s6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualise Embeddings\n",
        "\n",
        "Weâ€™ll now use t-SNE to reduce the 512-dimensional embeddings from CLIP to 2D and 3D and visualize the relationship between images and their corresponding text descriptions. t-SNE helps in visualizing how similar or dissimilar image and text embeddings are in the shared embedding space."
      ],
      "metadata": {
        "id": "HDWuO-of8Yry"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "visualisation code with t-SNE and plotly.\n",
        "try adjusting the perplexity parameter."
      ],
      "metadata": {
        "id": "dBIBxoxhZPMr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install transformers torchvision plotly scikit-learn"
      ],
      "metadata": {
        "id": "UD8FoAhsoKFP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch\n",
        "# from transformers import CLIPProcessor, CLIPModel\n",
        "\n",
        "# # Load the pretrained CLIP model and processor\n",
        "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "# clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
        "# clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")"
      ],
      "metadata": {
        "id": "jiGVbs1CoN_n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.express as px\n",
        "import torch\n",
        "from sklearn.manifold import TSNE\n",
        "import requests\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "\n",
        "# Function to fetch and preprocess images\n",
        "def preprocess_images(image_urls):\n",
        "    images = []\n",
        "    for url in image_urls:\n",
        "        response = requests.get(url)\n",
        "        img = Image.open(BytesIO(response.content))\n",
        "        image = preprocess(img).unsqueeze(0).to(device)\n",
        "        images.append(image)\n",
        "    return torch.cat(images)\n",
        "\n",
        "# Function to extract embeddings for images and texts\n",
        "def extract_embeddings(images, text_descriptions):\n",
        "    # encode images\n",
        "    with torch.no_grad():\n",
        "        image_features = model.encode_image(images)\n",
        "\n",
        "    # Tokenize and encode texts\n",
        "    text = clip.tokenize(text_descriptions).to(device)\n",
        "    with torch.no_grad():\n",
        "        text_features = model.encode_text(text)\n",
        "\n",
        "    return image_features, text_features\n",
        "\n",
        "# Function to reduce embeddings to 2D or 3D using t-SNE\n",
        "def reduce_with_tsne(embeddings, dim = 2):\n",
        "    tsne = TSNE(n_components=dim, perplexity=30, learning_rate=200, n_iter=1000)\n",
        "    return tsne.fit_transform(embeddings.cpu())\n",
        "\n",
        "# Function to visualize image and text embeddings using Plotly (2D)\n",
        "def visualize_embeddings_plotly(images, text_descriptions, image_labels =None, dim = 2):\n",
        "    image_features, text_features = extract_embeddings(images, text_descriptions)\n",
        "\n",
        "    # Normalize embeddings\n",
        "    image_features /= image_features.norm(dim=-1, keepdim=True)\n",
        "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "    # Combine and reduce to 2D or 3D using t-SNE\n",
        "    all_embeddings = torch.cat([image_features, text_features], dim=0)\n",
        "    reduced_embeddings = reduce_with_tsne(all_embeddings, dim = dim)\n",
        "\n",
        "    # Create labels for plotting\n",
        "    if image_labels == None:\n",
        "      labels = [\"Image \" + str(i+1) for i in range(len(images))] + text_descriptions\n",
        "    else:\n",
        "      labels = [\"Image \" + text_descriptions[image_labels[i]] for i in range(len(image_labels))] + text_descriptions\n",
        "\n",
        "    # Create a DataFrame for Plotly\n",
        "    import pandas as pd\n",
        "    if dim == 2:\n",
        "      df = pd.DataFrame(reduced_embeddings, columns=[\"x\", \"y\"])\n",
        "    elif dim == 3:\n",
        "      df = pd.DataFrame(reduced_embeddings, columns=[\"x\", \"y\", \"z\"])\n",
        "    else:\n",
        "      raise ValueError(\"Invalid dimension. Must be 2 or 3.\")\n",
        "\n",
        "    df[\"label\"] = labels\n",
        "    df[\"type\"] = [\"Image\"] * len(images) + [\"Text\"] * len(text_descriptions)\n",
        "\n",
        "    # # Create an interactive 2D scatter plot using Plotly\n",
        "    if dim == 2:\n",
        "      fig = px.scatter(df, x=\"x\", y=\"y\", color=\"type\", text=\"label\", title=\"Interactive 2D t-SNE Visualization\")\n",
        "      fig.update_traces(textposition='top center')\n",
        "    else:\n",
        "      fig = px.scatter_3d(df, x=\"x\", y=\"y\", z=\"z\", color=\"type\", text=\"label\", title=\"Interactive 3D t-SNE Visualization\")\n",
        "      fig.update_traces(marker=dict(size=5), textposition='top center')\n",
        "\n",
        "    fig.show()"
      ],
      "metadata": {
        "id": "H9aaSxbJ9AND"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "visulise your own input images and text. make sure to set perplexity lower than the number of samples for this. dont forget to increase it again for next part ðŸ˜€."
      ],
      "metadata": {
        "id": "rdrW_LB2X-ma"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image_urls = [\n",
        "    \"https://t4.ftcdn.net/jpg/00/97/58/97/360_F_97589769_t45CqXyzjz0KXwoBZT9PRaWGHRk5hQqQ.jpg\",   # Cat image\n",
        "    \"https://cdn.pixabay.com/photo/2023/08/18/15/02/dog-8198719_640.jpg\", # Dog image\n",
        "    \"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRI2RLOBO8DYvk8aAUNEs6DJzCJzlgHT7HfAg&s\" # Car image\n",
        "]\n",
        "text_descriptions = [\"a cat\", \"a dog\", \"a car\"]\n",
        "\n",
        "images = preprocess_images(image_urls)\n",
        "\n",
        "# Visualize embeddings interactively with Plotly\n",
        "# visualize_embeddings_plotly(images, text_descriptions)"
      ],
      "metadata": {
        "id": "9dtBezO0YHMU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title CIFAR10 dataset\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets.cifar import CIFAR10\n",
        "\n",
        "torch.manual_seed(0)\n",
        "\n",
        "# Load the CIFAR-10 training dataset\n",
        "dataset = CIFAR10(root='./data', train=True, download=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jfTFLoAxX12H",
        "outputId": "bc76aefd-ef93-41cc-a2db-33650a275e78",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170498071/170498071 [00:03<00:00, 48588081.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "visulise on CIFAR10"
      ],
      "metadata": {
        "id": "OYuXOLozX3bB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "classes = ['a photo of a plane', 'a photo of a car', 'a photo of abird', 'a photo of a cat',\n",
        "           'a photo of a deer', 'a photo of a dog', 'a photo of a frog', 'a photo of a horse', 'a photo of a ship', 'a photo of a truck']\n",
        "\n",
        "random_indices = random.sample(range(len(dataset)), 200)\n",
        "preprocessed_images = torch.stack([preprocess(dataset[i][0]).to(device) for i in random_indices])\n",
        "\n",
        "image_labels = [dataset[i][1] for i in random_indices]  # Extract corresponding labels\n",
        "\n",
        "# Map the numeric labels to their corresponding class names\n",
        "text_descriptions = [classes[label] for label in image_labels]\n",
        "\n",
        "# random_lable_indices = random.choices(range(len(classes)), k=64)\n",
        "\n",
        "\n",
        "# image_labels = [classes[i] for i in random_lable_indices]  # Extract corresponding labels\n",
        "\n",
        "visualize_embeddings_plotly(preprocessed_images,text_descriptions, image_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "cOXpoaj6Z1qf",
        "outputId": "1f8ac7c2-36c0-46b7-ada2-4e2d29c472ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.24.1.min.js\"></script>                <div id=\"0b11d866-e97d-49d8-a541-bf3a57a19c50\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"0b11d866-e97d-49d8-a541-bf3a57a19c50\")) {                    Plotly.newPlot(                        \"0b11d866-e97d-49d8-a541-bf3a57a19c50\",                        [{\"hovertemplate\":\"type=Image\\u003cbr\\u003ex=%{x}\\u003cbr\\u003ey=%{y}\\u003cbr\\u003elabel=%{text}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"Image\",\"marker\":{\"color\":\"#636efa\",\"symbol\":\"circle\"},\"mode\":\"markers+text\",\"name\":\"Image\",\"orientation\":\"v\",\"showlegend\":true,\"text\":[\"Image a photo of a horse\",\"Image a photo of a truck\",\"Image a photo of abird\",\"Image a photo of a car\",\"Image a photo of a ship\",\"Image a photo of a car\",\"Image a photo of a ship\",\"Image a photo of a ship\",\"Image a photo of a truck\",\"Image a photo of a horse\",\"Image a photo of a plane\",\"Image a photo of a truck\",\"Image a photo of a horse\",\"Image a photo of a plane\",\"Image a photo of a truck\",\"Image a photo of a plane\",\"Image a photo of a ship\",\"Image a photo of a ship\",\"Image a photo of a horse\",\"Image a photo of a horse\",\"Image a photo of a truck\",\"Image a photo of a car\",\"Image a photo of a truck\",\"Image a photo of a ship\",\"Image a photo of a truck\",\"Image a photo of a truck\",\"Image a photo of a car\",\"Image a photo of abird\",\"Image a photo of a truck\",\"Image a photo of a plane\",\"Image a photo of a horse\",\"Image a photo of a truck\",\"Image a photo of abird\",\"Image a photo of a ship\",\"Image a photo of a car\",\"Image a photo of abird\",\"Image a photo of a car\",\"Image a photo of abird\",\"Image a photo of a truck\",\"Image a photo of a car\",\"Image a photo of a horse\",\"Image a photo of a plane\",\"Image a photo of a truck\",\"Image a photo of a truck\",\"Image a photo of a plane\",\"Image a photo of a plane\",\"Image a photo of a truck\",\"Image a photo of a plane\",\"Image a photo of a plane\",\"Image a photo of a horse\",\"Image a photo of a ship\",\"Image a photo of a truck\",\"Image a photo of a plane\",\"Image a photo of a truck\",\"Image a photo of a plane\",\"Image a photo of a horse\",\"Image a photo of a horse\",\"Image a photo of a truck\",\"Image a photo of a horse\",\"Image a photo of a car\",\"Image a photo of a plane\",\"Image a photo of a ship\",\"Image a photo of a horse\",\"Image a photo of a truck\",\"Image a photo of a ship\",\"Image a photo of abird\",\"Image a photo of a horse\",\"Image a photo of a plane\",\"Image a photo of a ship\",\"Image a photo of a truck\",\"Image a photo of a horse\",\"Image a photo of a plane\",\"Image a photo of a plane\",\"Image a photo of a truck\",\"Image a photo of a plane\",\"Image a photo of a ship\",\"Image a photo of a truck\",\"Image a photo of a ship\",\"Image a photo of a plane\",\"Image a photo of a horse\",\"Image a photo of a horse\",\"Image a photo of a horse\",\"Image a photo of a car\",\"Image a photo of a car\",\"Image a photo of a truck\",\"Image a photo of abird\",\"Image a photo of a truck\",\"Image a photo of abird\",\"Image a photo of a horse\",\"Image a photo of a truck\",\"Image a photo of a truck\",\"Image a photo of a horse\",\"Image a photo of a truck\",\"Image a photo of a truck\",\"Image a photo of a truck\",\"Image a photo of a truck\",\"Image a photo of a truck\",\"Image a photo of abird\",\"Image a photo of a truck\",\"Image a photo of a plane\",\"Image a photo of a plane\",\"Image a photo of a horse\",\"Image a photo of a horse\",\"Image a photo of abird\",\"Image a photo of a truck\",\"Image a photo of a plane\",\"Image a photo of a car\",\"Image a photo of a truck\",\"Image a photo of a car\",\"Image a photo of a truck\",\"Image a photo of a truck\",\"Image a photo of a plane\",\"Image a photo of a car\",\"Image a photo of a truck\",\"Image a photo of a car\",\"Image a photo of a plane\",\"Image a photo of a ship\",\"Image a photo of a car\",\"Image a photo of a truck\",\"Image a photo of a plane\",\"Image a photo of a truck\",\"Image a photo of a plane\",\"Image a photo of a truck\",\"Image a photo of a truck\",\"Image a photo of a horse\",\"Image a photo of a plane\",\"Image a photo of a truck\",\"Image a photo of a car\",\"Image a photo of a ship\",\"Image a photo of a car\",\"Image a photo of abird\",\"Image a photo of a truck\",\"Image a photo of a horse\",\"Image a photo of a car\",\"Image a photo of a horse\",\"Image a photo of a truck\",\"Image a photo of a plane\",\"Image a photo of a plane\",\"Image a photo of a horse\",\"Image a photo of abird\",\"Image a photo of a car\",\"Image a photo of a horse\",\"Image a photo of a ship\",\"Image a photo of a plane\",\"Image a photo of abird\",\"Image a photo of a car\",\"Image a photo of a plane\",\"Image a photo of a horse\",\"Image a photo of a truck\",\"Image a photo of a horse\",\"Image a photo of a horse\",\"Image a photo of a car\",\"Image a photo of a ship\",\"Image a photo of a plane\",\"Image a photo of a truck\",\"Image a photo of a ship\",\"Image a photo of a horse\",\"Image a photo of a truck\",\"Image a photo of a ship\",\"Image a photo of a ship\",\"Image a photo of a horse\",\"Image a photo of a horse\",\"Image a photo of a truck\",\"Image a photo of a plane\",\"Image a photo of a truck\",\"Image a photo of a plane\",\"Image a photo of a plane\",\"Image a photo of a plane\",\"Image a photo of a ship\",\"Image a photo of a ship\",\"Image a photo of a truck\",\"Image a photo of a ship\",\"Image a photo of a horse\",\"Image a photo of a plane\",\"Image a photo of a truck\",\"Image a photo of a plane\",\"Image a photo of a horse\",\"Image a photo of a truck\",\"Image a photo of a plane\",\"Image a photo of a horse\",\"Image a photo of a ship\",\"Image a photo of a plane\",\"Image a photo of a horse\",\"Image a photo of abird\",\"Image a photo of a horse\",\"Image a photo of abird\",\"Image a photo of a ship\",\"Image a photo of a horse\",\"Image a photo of a ship\",\"Image a photo of a horse\",\"Image a photo of a horse\",\"Image a photo of a horse\",\"Image a photo of a horse\",\"Image a photo of a car\",\"Image a photo of a horse\",\"Image a photo of a horse\",\"Image a photo of a truck\",\"Image a photo of a horse\",\"Image a photo of a plane\",\"Image a photo of a plane\"],\"x\":[68.41693115234375,-430.4023742675781,-2.5105137825012207,-241.54739379882812,149.485107421875,-287.2923583984375,151.3447723388672,223.17918395996094,-379.1239929199219,-174.16676330566406,-401.26898193359375,-93.23577880859375,-80.22965240478516,-120.9330062866211,-157.88027954101562,-283.304443359375,180.8212890625,141.91639709472656,-155.24880981445312,-47.84482955932617,-152.99581909179688,-260.9070739746094,-168.27198791503906,174.6099853515625,-184.6976776123047,-399.7384948730469,-199.09230041503906,1.833360195159912,-457.0715637207031,-482.16064453125,-123.10076904296875,-391.00726318359375,-38.33513641357422,178.3589324951172,-261.4818115234375,-31.262292861938477,-239.4357452392578,10.231040954589844,-373.7346496582031,16.458948135375977,-104.15802001953125,-425.90655517578125,-414.65673828125,-90.87000274658203,-176.26158142089844,-239.01719665527344,-151.93356323242188,-250.939453125,-330.654296875,-25.458547592163086,79.58907318115234,-336.0975341796875,-508.0906982421875,-60.93178939819336,-399.2965087890625,-190.96263122558594,-132.85537719726562,-504.7771911621094,96.30775451660156,-282.5973205566406,-206.79086303710938,187.85598754882812,96.16722106933594,-212.2242889404297,137.91986083984375,-49.44087219238281,-115.85718536376953,-264.0231628417969,222.72323608398438,-140.0880889892578,-70.14883422851562,-367.23486328125,-446.9367980957031,-394.64776611328125,-216.75546264648438,30.87157440185547,-463.2131652832031,240.1932830810547,-341.2588806152344,-4.3036041259765625,-144.88270568847656,63.70060348510742,-256.25775146484375,-320.4991760253906,-161.58729553222656,-105.9071273803711,-423.58404541015625,-63.79930114746094,-161.23660278320312,-72.94646453857422,-137.10826110839844,18.406587600708008,-122.419189453125,-169.9748077392578,-107.7939453125,-427.9904479980469,-79.46066284179688,-244.8318328857422,-24.128089904785156,-300.30731201171875,-306.12860107421875,129.3246307373047,67.7417221069336,-213.8176727294922,-304.7446594238281,-415.71429443359375,-312.50018310546875,-444.5719909667969,-207.63497924804688,-520.46240234375,-366.0735168457031,-342.3719482421875,-185.7781524658203,-422.5572204589844,-212.53305053710938,-433.2281799316406,151.4368133544922,-242.6751708984375,-391.6024475097656,-144.1500244140625,-502.7333984375,-270.8986511230469,-133.232177734375,-169.2259979248047,14.50929069519043,-251.47389221191406,-78.07530212402344,-235.9672393798828,215.18031311035156,-288.01898193359375,-21.095197677612305,-348.26904296875,26.09841537475586,-6.103606700897217,70.2421646118164,-484.3485107421875,-259.0579833984375,-311.3575134277344,37.2894401550293,-24.394105911254883,-275.5054626464844,39.198890686035156,222.0655975341797,-388.0924377441406,-141.16510009765625,-313.6633605957031,-292.3336486816406,87.94416046142578,-119.17322540283203,109.48379516601562,-100.76791381835938,-115.20693969726562,163.51927185058594,-226.65599060058594,-480.953125,208.8423614501953,-86.79480743408203,-339.8575744628906,95.10244750976562,240.54696655273438,11.912520408630371,11.453755378723145,-97.3831787109375,-455.8216857910156,-115.43401336669922,-308.5738220214844,-340.5121765136719,-337.5993347167969,183.3959197998047,120.29398345947266,-325.2583923339844,30.30600357055664,-118.95065307617188,-278.5920715332031,-452.0662841796875,-283.674072265625,45.02508544921875,-180.21493530273438,-384.25994873046875,-46.75510787963867,108.10103607177734,-328.5746154785156,-182.81289672851562,-158.64353942871094,-25.14969253540039,-102.76197814941406,90.93431091308594,-70.68571472167969,205.78927612304688,112.49935913085938,1.1374369859695435,-127.4731674194336,20.26345443725586,-229.03805541992188,64.50081634521484,-55.224143981933594,-53.067562103271484,-14.732089042663574,-408.7151794433594,-254.4163360595703],\"xaxis\":\"x\",\"y\":[-186.75515747070312,127.95458984375,308.57281494140625,-77.76963806152344,-89.45012664794922,-188.83450317382812,-129.74057006835938,-17.507352828979492,179.09991455078125,-100.91292572021484,316.9590759277344,137.89035034179688,-59.23994064331055,50.915348052978516,472.71435546875,327.8369140625,-97.69487762451172,-51.365196228027344,-132.54046630859375,13.721780776977539,124.86618041992188,-170.74757385253906,438.1966857910156,-175.1703338623047,151.73960876464844,130.20838928222656,5.331396579742432,402.160400390625,113.19055938720703,324.4847106933594,-185.93218994140625,31.43790054321289,286.7785949707031,-58.996368408203125,35.469154357910156,317.3997802734375,-48.92951202392578,356.6207580566406,105.22112274169922,102.0754623413086,-89.8000259399414,244.9525909423828,-51.39262390136719,217.43658447265625,266.12884521484375,154.12388610839844,209.08096313476562,295.9670104980469,368.4294738769531,-11.52541446685791,-64.79396057128906,0.4602126479148865,155.97845458984375,213.50845336914062,284.7730407714844,-136.85623168945312,-95.93031311035156,107.82713317871094,-205.09361267089844,-102.92877960205078,129.9924774169922,-2.7109713554382324,-161.2559356689453,190.2509307861328,24.861373901367188,360.3467712402344,-55.923763275146484,378.2976989746094,-159.8098907470703,415.0171203613281,-156.65394592285156,302.2734680175781,357.5143127441406,-29.986059188842773,311.4631042480469,-8.006254196166992,166.2858123779297,-122.38521575927734,73.34739685058594,22.989673614501953,-157.857421875,-91.34354400634766,-117.78521728515625,-122.04325103759766,88.72711944580078,345.3963928222656,62.53858184814453,327.0765686035156,-200.33248901367188,174.0831756591797,149.99200439453125,-116.89567565917969,114.142333984375,178.99391174316406,178.2294464111328,156.31822204589844,248.5603485107422,91.74449920654297,179.54147338867188,401.4024963378906,236.86846923828125,-217.44381713867188,-220.43560791015625,366.4661865234375,102.74010467529297,356.0103759765625,-80.11739349365234,21.04161262512207,-34.84752655029297,59.071739196777344,-12.312264442443848,275.6629638671875,29.692834854125977,94.71613311767578,76.35867309570312,315.1983642578125,45.042198181152344,-202.16273498535156,79.95680236816406,252.71945190429688,185.21356201171875,257.30999755859375,188.30995178222656,390.738037109375,-142.12594604492188,329.4205627441406,91.5357666015625,-6.222102642059326,-202.68438720703125,-152.96629333496094,342.7544860839844,134.4664306640625,-47.58497619628906,84.76093292236328,-279.72515869140625,80.05703735351562,203.70619201660156,302.81951904296875,-213.6571807861328,382.5004577636719,-68.64710235595703,-178.31068420410156,-92.16053009033203,243.64109802246094,352.0897521972656,-172.8194580078125,145.58999633789062,-241.8070831298828,225.19715881347656,-117.20342254638672,-13.825438499450684,-223.8338165283203,-25.434524536132812,234.3167266845703,49.735111236572266,-124.87748718261719,-110.64513397216797,39.218624114990234,0.3777449429035187,-53.01596450805664,-227.0866241455078,-74.4886245727539,276.9199523925781,270.3185119628906,440.2832946777344,271.5942687988281,399.1811218261719,235.51217651367188,-139.63299560546875,-24.459163665771484,165.2852783203125,80.12336730957031,-134.3097381591797,297.5726013183594,74.31120300292969,357.4820251464844,-255.40228271484375,217.46575927734375,355.84490966796875,-80.92206573486328,-68.71129608154297,332.3280029296875,-168.01776123046875,325.64434814453125,-247.73663330078125,383.41082763671875,-34.122589111328125,0.8157328367233276,-46.75261306762695,-256.4720458984375,-191.77540588378906,-13.194259643554688,-275.0008239746094,39.845741271972656,-124.4491958618164,-49.3908805847168,136.31375122070312,-140.6315155029297,388.95770263671875,438.1527404785156],\"yaxis\":\"y\",\"type\":\"scatter\",\"textposition\":\"top center\"},{\"hovertemplate\":\"type=Text\\u003cbr\\u003ex=%{x}\\u003cbr\\u003ey=%{y}\\u003cbr\\u003elabel=%{text}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"Text\",\"marker\":{\"color\":\"#EF553B\",\"symbol\":\"circle\"},\"mode\":\"markers+text\",\"name\":\"Text\",\"orientation\":\"v\",\"showlegend\":true,\"text\":[\"a photo of a car\",\"a photo of a horse\",\"a photo of abird\",\"a photo of a plane\",\"a photo of a truck\",\"a photo of a plane\",\"a photo of a truck\",\"a photo of a truck\",\"a photo of a horse\",\"a photo of a ship\",\"a photo of a dog\",\"a photo of a frog\",\"a photo of a ship\",\"a photo of a cat\",\"a photo of a deer\",\"a photo of a cat\",\"a photo of a truck\",\"a photo of a truck\",\"a photo of a ship\",\"a photo of a ship\",\"a photo of a frog\",\"a photo of a plane\",\"a photo of a deer\",\"a photo of a truck\",\"a photo of a frog\",\"a photo of a horse\",\"a photo of a plane\",\"a photo of abird\",\"a photo of a horse\",\"a photo of a dog\",\"a photo of a ship\",\"a photo of a deer\",\"a photo of abird\",\"a photo of a truck\",\"a photo of a plane\",\"a photo of abird\",\"a photo of a plane\",\"a photo of abird\",\"a photo of a horse\",\"a photo of a plane\",\"a photo of a ship\",\"a photo of a dog\",\"a photo of a deer\",\"a photo of a frog\",\"a photo of a cat\",\"a photo of a cat\",\"a photo of a frog\",\"a photo of a cat\",\"a photo of a dog\",\"a photo of a ship\",\"a photo of a truck\",\"a photo of a deer\",\"a photo of a dog\",\"a photo of a frog\",\"a photo of a dog\",\"a photo of a ship\",\"a photo of a ship\",\"a photo of a horse\",\"a photo of a car\",\"a photo of a plane\",\"a photo of a dog\",\"a photo of a truck\",\"a photo of a car\",\"a photo of a frog\",\"a photo of a truck\",\"a photo of abird\",\"a photo of a ship\",\"a photo of a dog\",\"a photo of a truck\",\"a photo of a deer\",\"a photo of a ship\",\"a photo of a dog\",\"a photo of a dog\",\"a photo of a deer\",\"a photo of a cat\",\"a photo of a truck\",\"a photo of a horse\",\"a photo of a truck\",\"a photo of a dog\",\"a photo of a ship\",\"a photo of a ship\",\"a photo of a car\",\"a photo of a plane\",\"a photo of a plane\",\"a photo of a frog\",\"a photo of abird\",\"a photo of a horse\",\"a photo of abird\",\"a photo of a ship\",\"a photo of a frog\",\"a photo of a frog\",\"a photo of a car\",\"a photo of a deer\",\"a photo of a frog\",\"a photo of a frog\",\"a photo of a horse\",\"a photo of a frog\",\"a photo of abird\",\"a photo of a frog\",\"a photo of a cat\",\"a photo of a cat\",\"a photo of a car\",\"a photo of a car\",\"a photo of abird\",\"a photo of a horse\",\"a photo of a dog\",\"a photo of a plane\",\"a photo of a horse\",\"a photo of a plane\",\"a photo of a horse\",\"a photo of a deer\",\"a photo of a dog\",\"a photo of a plane\",\"a photo of a horse\",\"a photo of a plane\",\"a photo of a dog\",\"a photo of a truck\",\"a photo of a plane\",\"a photo of a horse\",\"a photo of a cat\",\"a photo of a deer\",\"a photo of a cat\",\"a photo of a frog\",\"a photo of a deer\",\"a photo of a car\",\"a photo of a cat\",\"a photo of a frog\",\"a photo of a plane\",\"a photo of a truck\",\"a photo of a plane\",\"a photo of abird\",\"a photo of a horse\",\"a photo of a car\",\"a photo of a plane\",\"a photo of a car\",\"a photo of a horse\",\"a photo of a cat\",\"a photo of a cat\",\"a photo of a car\",\"a photo of abird\",\"a photo of a plane\",\"a photo of a car\",\"a photo of a truck\",\"a photo of a dog\",\"a photo of abird\",\"a photo of a plane\",\"a photo of a dog\",\"a photo of a car\",\"a photo of a frog\",\"a photo of a car\",\"a photo of a ship\",\"a photo of a plane\",\"a photo of a truck\",\"a photo of a cat\",\"a photo of a horse\",\"a photo of a truck\",\"a photo of a ship\",\"a photo of a deer\",\"a photo of a truck\",\"a photo of a truck\",\"a photo of a car\",\"a photo of a car\",\"a photo of a frog\",\"a photo of a dog\",\"a photo of a deer\",\"a photo of a cat\",\"a photo of a cat\",\"a photo of a cat\",\"a photo of a truck\",\"a photo of a truck\",\"a photo of a horse\",\"a photo of a truck\",\"a photo of a ship\",\"a photo of a cat\",\"a photo of a horse\",\"a photo of a cat\",\"a photo of a car\",\"a photo of a frog\",\"a photo of a dog\",\"a photo of a ship\",\"a photo of a truck\",\"a photo of a dog\",\"a photo of a ship\",\"a photo of abird\",\"a photo of a car\",\"a photo of abird\",\"a photo of a truck\",\"a photo of a ship\",\"a photo of a truck\",\"a photo of a car\",\"a photo of a car\",\"a photo of a ship\",\"a photo of a car\",\"a photo of a plane\",\"a photo of a car\",\"a photo of a ship\",\"a photo of a frog\",\"a photo of a car\",\"a photo of a dog\",\"a photo of a dog\"],\"x\":[331.05853271484375,159.07984924316406,341.489990234375,467.9070739746094,528.354736328125,467.9070739746094,466.4014587402344,528.354736328125,159.07984924316406,361.77783203125,129.5187530517578,-374.6339111328125,411.7506103515625,0.41623327136039734,-115.635986328125,0.41623327136039734,528.354736328125,466.4014587402344,361.77783203125,361.77783203125,-374.6339111328125,467.9070739746094,-147.7427215576172,528.354736328125,-374.6339111328125,159.07984924316406,467.9070739746094,341.489990234375,159.07984924316406,129.5187530517578,361.77783203125,-147.7427215576172,341.489990234375,466.4014587402344,467.9070739746094,341.489990234375,467.9070739746094,341.489990234375,159.07984924316406,467.9070739746094,361.77783203125,129.5187530517578,-147.7427215576172,-374.6339111328125,0.41623327136039734,0.41623327136039734,-374.6339111328125,0.41623327136039734,129.5187530517578,411.7506103515625,528.354736328125,-115.635986328125,129.5187530517578,-374.6339111328125,129.5187530517578,411.7506103515625,361.77783203125,159.07984924316406,331.05853271484375,467.9070739746094,129.5187530517578,528.354736328125,331.05853271484375,-374.6339111328125,466.4014587402344,341.489990234375,411.7506103515625,129.5187530517578,528.354736328125,-147.7427215576172,355.17144775390625,129.5187530517578,129.5187530517578,-147.7427215576172,0.41623327136039734,528.354736328125,138.21591186523438,528.354736328125,129.5187530517578,361.77783203125,361.77783203125,331.05853271484375,467.9070739746094,467.9070739746094,-374.6339111328125,341.489990234375,159.07984924316406,341.489990234375,411.7506103515625,-374.6339111328125,-374.6339111328125,300.98162841796875,-115.635986328125,-374.6339111328125,-374.6339111328125,159.07984924316406,-374.6339111328125,341.489990234375,-374.6339111328125,0.41623327136039734,0.41623327136039734,300.98162841796875,300.98162841796875,341.489990234375,159.07984924316406,129.5187530517578,467.9070739746094,159.07984924316406,467.9070739746094,159.07984924316406,-147.7427215576172,129.5187530517578,467.9070739746094,138.21591186523438,467.9070739746094,129.5187530517578,528.354736328125,467.9070739746094,138.21591186523438,0.41623327136039734,-147.7427215576172,0.41623327136039734,-374.6339111328125,-147.7427215576172,331.05853271484375,0.41623327136039734,-374.6339111328125,467.9070739746094,528.354736328125,467.9070739746094,341.489990234375,159.07984924316406,331.05853271484375,467.9070739746094,331.05853271484375,138.21591186523438,0.41623327136039734,0.41623327136039734,331.05853271484375,341.489990234375,467.9070739746094,300.98162841796875,528.354736328125,129.5187530517578,341.489990234375,467.9070739746094,129.5187530517578,331.05853271484375,-374.6339111328125,331.05853271484375,411.7506103515625,467.9070739746094,528.354736328125,0.41623327136039734,159.07984924316406,528.354736328125,361.77783203125,-147.7427215576172,528.354736328125,528.354736328125,331.05853271484375,331.05853271484375,-374.6339111328125,129.5187530517578,-147.7427215576172,0.41623327136039734,0.41623327136039734,0.41623327136039734,528.354736328125,528.354736328125,159.07984924316406,528.354736328125,411.7506103515625,0.41623327136039734,159.07984924316406,0.41623327136039734,331.05853271484375,-374.6339111328125,129.5187530517578,361.77783203125,528.354736328125,129.5187530517578,411.7506103515625,341.489990234375,300.98162841796875,341.489990234375,528.354736328125,411.7506103515625,528.354736328125,300.98162841796875,300.98162841796875,411.7506103515625,300.98162841796875,467.9070739746094,300.98162841796875,355.17144775390625,-409.6748352050781,364.90478515625,70.95915985107422,70.95915985107422],\"xaxis\":\"x\",\"y\":[347.14825439453125,191.11050415039062,-417.5453796386719,-192.46591186523438,200.6714324951172,-192.46591186523438,226.00979614257812,200.6714324951172,191.11050415039062,44.920509338378906,-421.8150329589844,-359.8808898925781,7.2082743644714355,-548.1087036132812,-363.7363586425781,-548.1087036132812,200.6714324951172,226.00979614257812,44.920509338378906,44.920509338378906,-359.8808898925781,-192.46591186523438,-403.9136962890625,200.6714324951172,-359.8808898925781,191.11050415039062,-192.46591186523438,-417.5453796386719,191.11050415039062,-421.8150329589844,44.920509338378906,-403.9136962890625,-417.5453796386719,226.00979614257812,-192.46591186523438,-417.5453796386719,-192.46591186523438,-417.5453796386719,191.11050415039062,-192.46591186523438,44.920509338378906,-421.8150329589844,-403.9136962890625,-359.8808898925781,-548.1087036132812,-548.1087036132812,-359.8808898925781,-548.1087036132812,-421.8150329589844,7.2082743644714355,200.6714324951172,-363.7363586425781,-421.8150329589844,-359.8808898925781,-421.8150329589844,7.2082743644714355,44.920509338378906,191.11050415039062,347.14825439453125,-192.46591186523438,-421.8150329589844,200.6714324951172,347.14825439453125,-359.8808898925781,226.00979614257812,-417.5453796386719,7.2082743644714355,-421.8150329589844,200.6714324951172,-403.9136962890625,-17.104572296142578,-421.8150329589844,-421.8150329589844,-403.9136962890625,-548.1087036132812,200.6714324951172,246.67800903320312,200.6714324951172,-421.8150329589844,44.920509338378906,44.920509338378906,347.14825439453125,-192.46591186523438,-192.46591186523438,-359.8808898925781,-417.5453796386719,191.11050415039062,-417.5453796386719,7.2082743644714355,-359.8808898925781,-359.8808898925781,403.5274658203125,-363.7363586425781,-359.8808898925781,-359.8808898925781,191.11050415039062,-359.8808898925781,-417.5453796386719,-359.8808898925781,-548.1087036132812,-548.1087036132812,403.5274658203125,403.5274658203125,-417.5453796386719,191.11050415039062,-421.8150329589844,-192.46591186523438,191.11050415039062,-192.46591186523438,191.11050415039062,-403.9136962890625,-421.8150329589844,-192.46591186523438,246.67800903320312,-192.46591186523438,-421.8150329589844,200.6714324951172,-192.46591186523438,246.67800903320312,-548.1087036132812,-403.9136962890625,-548.1087036132812,-359.8808898925781,-403.9136962890625,347.14825439453125,-548.1087036132812,-359.8808898925781,-192.46591186523438,200.6714324951172,-192.46591186523438,-417.5453796386719,191.11050415039062,347.14825439453125,-192.46591186523438,347.14825439453125,246.67800903320312,-548.1087036132812,-548.1087036132812,347.14825439453125,-417.5453796386719,-192.46591186523438,403.5274658203125,200.6714324951172,-421.8150329589844,-417.5453796386719,-192.46591186523438,-421.8150329589844,347.14825439453125,-359.8808898925781,347.14825439453125,7.2082743644714355,-192.46591186523438,200.6714324951172,-548.1087036132812,191.11050415039062,200.6714324951172,44.920509338378906,-403.9136962890625,200.6714324951172,200.6714324951172,347.14825439453125,347.14825439453125,-359.8808898925781,-421.8150329589844,-403.9136962890625,-548.1087036132812,-548.1087036132812,-548.1087036132812,200.6714324951172,200.6714324951172,191.11050415039062,200.6714324951172,7.2082743644714355,-548.1087036132812,191.11050415039062,-548.1087036132812,347.14825439453125,-359.8808898925781,-421.8150329589844,44.920509338378906,200.6714324951172,-421.8150329589844,7.2082743644714355,-417.5453796386719,403.5274658203125,-417.5453796386719,200.6714324951172,7.2082743644714355,200.6714324951172,403.5274658203125,403.5274658203125,7.2082743644714355,403.5274658203125,-192.46591186523438,403.5274658203125,-17.104572296142578,-310.37005615234375,401.29107666015625,-402.405029296875,-402.405029296875],\"yaxis\":\"y\",\"type\":\"scatter\",\"textposition\":\"top center\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"x\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"y\"}},\"legend\":{\"title\":{\"text\":\"type\"},\"tracegroupgap\":0},\"title\":{\"text\":\"Interactive 2D t-SNE Visualization\"}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('0b11d866-e97d-49d8-a541-bf3a57a19c50');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Zero-shot Classification\n",
        "\n",
        "Zero-shot classification is the ability to classify images into categories without having explicitly trained the model on those specific categories. CLIP enables this by understanding images and text in a shared embedding space. This means that once the model has learned general concepts through contrastive learning, it can generalize to entirely new categories just by providing textual labels.\n",
        "\n",
        "This is one of CLIP's most remarkable capabilitiesâ€”performing tasks without being explicitly trained for them.\n",
        "\n",
        "In a traditional model, you would need to fine-tune the model for specific categories. In contrast, CLIP does this out-of-the-box. All you need to do is provide some candidate class names as text and let CLIP predict which one is the most similar to a given image.\n",
        "\n",
        "- Image Representation: The input image is passed through the image encoder to get its embedding.\n",
        "\n",
        "- Text Representation: Each of the class labels is passed through the text encoder to get their embeddings.\n",
        "\n",
        "- Similarity Calculation: CLIP computes the cosine similarity between the image embedding and each text embedding.\n",
        "\n",
        "- Prediction: The class label with the highest similarity score is chosen as the predicted label."
      ],
      "metadata": {
        "id": "UJDr08nI5E34"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Zero-shot classification function with CLIP\n",
        "def zero_shot_classification(image_url, class_labels):\n",
        "    # Load and preprocess the image\n",
        "    response = requests.get(image_url)\n",
        "    img = Image.open(BytesIO(response.content))\n",
        "    image = preprocess(img).unsqueeze(0).to(device)\n",
        "\n",
        "    # Tokenize and encode the class labels (text descriptions)\n",
        "    text = clip.tokenize(class_labels).to(device)\n",
        "\n",
        "    # Compute image and text embeddings\n",
        "    with torch.no_grad():\n",
        "        image_features = model.encode_image(image)\n",
        "        text_features = model.encode_text(text)\n",
        "\n",
        "    # Compute cosine similarity between image and text embeddings\n",
        "    image_features /= image_features.norm(dim=-1, keepdim=True)\n",
        "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
        "    similarity_scores = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
        "\n",
        "    # Output the most likely class\n",
        "    class_probabilities = similarity_scores[0]\n",
        "    best_idx = class_probabilities.argmax().item()\n",
        "\n",
        "    return class_labels[best_idx], class_probabilities[best_idx].item()"
      ],
      "metadata": {
        "id": "OYQ0ErkC5s4V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Try it out with a sample image and custom categories\n",
        "# image_url = input(\"Enter the URL of an image: \")\n",
        "\n",
        "# # Define class labels (text descriptions)\n",
        "# class_labels = [\"a dog\", \"a cat\", \"a car\", \"a person\", \"a bird\"]\n",
        "\n",
        "# # Perform zero-shot classification\n",
        "# predicted_class, probability = zero_shot_classification(image_url, class_labels)\n",
        "\n",
        "# # Display results\n",
        "# print(f\"\\nPredicted Class: {predicted_class}\")\n",
        "# print(f\"Confidence: {probability:.4f}\")"
      ],
      "metadata": {
        "id": "hEPunA985vWd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: Classifying an image of a sports event\n",
        "# sports_categories = [\"soccer\", \"basketball\", \"tennis\", \"swimming\", \"cycling\"]\n",
        "\n",
        "# # Use an image of a sports event\n",
        "# image_url = \"https://static.vecteezy.com/system/resources/thumbnails/027/829/023/small_2x/close-up-of-many-soccer-players-kicking-a-football-on-a-field-competition-scene-created-with-generative-ai-technology-free-photo.jpg\"\n",
        "\n",
        "# # Perform zero-shot classification\n",
        "# predicted_class, probability = zero_shot_classification(image_url, sports_categories)\n",
        "\n",
        "# # Display results\n",
        "# print(f\"\\nPredicted Class: {predicted_class}\")\n",
        "# print(f\"Confidence: {probability:.4f}\")"
      ],
      "metadata": {
        "id": "JcKkaNQr57ae"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Some Analysis reported by the paper:"
      ],
      "metadata": {
        "id": "FD1Fe4cNjTks"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### problems addressed by CLIP:"
      ],
      "metadata": {
        "id": "Ct4w77vXiuj9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "CLIP addresses several key challenges in the traditional deep learning approach to computer vision:\n",
        "\n",
        "Costly datasets: Traditional vision models require large, manually labeled datasets like ImageNet, which is expensive to create. CLIP avoids this by learning from publicly available text-image pairs, reducing the need for costly, labeled data.\n",
        "\n",
        "Limited adaptability: Standard models like those trained on ImageNet are restricted to predefined tasks (e.g., 1000 categories). CLIP, however, can adapt to various visual tasks without additional training, simply by providing relevant text prompts for the task's concepts.\n",
        "\n",
        "Poor real-world performance: Vision models often perform well on benchmarks but struggle in real-world applications due to overfitting to benchmark data. CLIP performs more robustly in real-world settings, as it doesn't require training on specific benchmark datasets, making its performance more generalizable. Testing has shown CLIP's performance remains consistent across multiple datasets, unlike models that \"study\" for benchmarks."
      ],
      "metadata": {
        "id": "iv9hLFuTh0Le"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "By not directly optimizing for the benchmark, CLIP becomes much more representative: CLIPs system closes this â€œrobustness gapâ€ by up to 75% while matching the performance of the original ResNet-507 on ImageNet(opens in a new window) zero-shot without using any of the original 1.28M labeled examples.\n",
        "\n",
        "![comarison with resnet on imagenet](https://blog.lancedb.com/content/images/2024/07/Untitled.png)\n",
        "\n",
        "Although both models have the same accuracy on the ImageNet test set, CLIPâ€™s performance is much more representative of how it will fare on datasets that measure accuracy in different, non-ImageNet settings. For instance, ObjectNet checks a modelâ€™s ability to recognize objects in many different poses and with many different backgrounds inside homes while ImageNet Rendition and ImageNet Sketch check a modelâ€™s ability to recognize more abstract depictions of objects."
      ],
      "metadata": {
        "id": "8AstkyT7gfRn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Key Takeaways:\n",
        "\n",
        "1. **CLIP is highly efficient**:\n",
        "   - CLIP trains on highly varied and noisy data in a zero-shot manner, similar to GPT-2 and GPT-3, but required significant compute to achieve strong performance. To reduce compute costs, two key algorithmic choices were made:\n",
        "     - **Contrastive objective**: Connecting text and images through a contrastive learning approach proved 4x to 10x more efficient than image-to-text methods.\n",
        "     - **Vision Transformer (ViT)**: Adopting ViT resulted in a further 3x gain in efficiency over traditional ResNet models.\n",
        "   - With these optimizations, the best CLIP model was trained on 256 GPUs for 2 weeks, comparable to other large-scale models.\n",
        "\n",
        "2. **CLIP is flexible and general**:\n",
        "   - CLIP learns a wide range of visual concepts from natural language, making it more flexible than models trained on datasets like ImageNet.\n",
        "   - CLIP demonstrated strong **zero-shot performance** across 30+ datasets, covering tasks like fine-grained object classification, geo-localization, action recognition, and even OCR (optical character recognition), which traditional models struggle with.\n",
        "   - In a linear probe evaluation, the best CLIP model outperformed the top ImageNet model (Noisy Student EfficientNet-L2) on 20 out of 26 transfer datasets. This underscores CLIPâ€™s generalization capabilities across different tasks."
      ],
      "metadata": {
        "id": "Bs2D5e2_jR5s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Linear Probe\n",
        "A linear probe is a method used in representation learning to evaluate the quality of learned representations. In this context, the linear probe involves freezing the pretrained model (such as CLIP) and training a simple linear classifier on top of its output features to solve a specific task (e.g., classification).\n",
        "\n",
        "- Evaluation of Pretrained Representations: Linear probing helps assess how much information is retained in the representations learned by the CLIP model.\n",
        "- Efficient Transfer Learning: Instead of fine-tuning the whole CLIP model on a new dataset, a linear classifier is trained on top of the frozen CLIP features, saving compute time and resources.\n",
        "- Generalization: If a linear probe performs well, it indicates that the representations are rich enough to solve tasks beyond what the model was trained on (i.e., zero-shot generalization)."
      ],
      "metadata": {
        "id": "eQ5blQTFkqKx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets try this"
      ],
      "metadata": {
        "id": "_4E--Auxk7EX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers torchvision"
      ],
      "metadata": {
        "id": "DDxfyMcBk8_K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "188baec2-cafe-4029-dc5e-8b6a01c45564"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.19.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: torch==2.4.1 in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.4.1+cu121)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (10.4.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.1->torchvision) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.4.1->torchvision) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.4.1->torchvision) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.1->torchvision) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.4.1->torchvision) (2024.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.4.1->torchvision) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.4.1->torchvision) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as T\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "\n",
        "# Load the pretrained CLIP model and processor\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
        "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")"
      ],
      "metadata": {
        "id": "G1RY14WzlAbm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title prepare dataset\n",
        "\n",
        "# Load the CIFAR-10 dataset\n",
        "transform = T.Compose([\n",
        "    T.Resize((224, 224)),  # Resize to CLIP's input size\n",
        "    T.ToTensor()\n",
        "])\n",
        "\n",
        "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, transform=transform, download=True)\n",
        "subset_indices = torch.randperm(len(train_dataset))[:500]  # Take only 500 samples for faster processing\n",
        "subset_dataset = torch.utils.data.Subset(train_dataset, subset_indices)\n",
        "train_loader = torch.utils.data.DataLoader(subset_dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, transform=transform, download=True)\n",
        "subset_indices_test = torch.randperm(len(test_dataset))[:200]  # Take only 200 samples for faster processing\n",
        "subset_dataset_test = torch.utils.data.Subset(test_dataset, subset_indices_test)\n",
        "test_loader = torch.utils.data.DataLoader(subset_dataset_test, batch_size=64, shuffle=False)\n",
        "\n",
        "# train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "# test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False)"
      ],
      "metadata": {
        "id": "lgSiOcoSlEfJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54f406ab-2665-4e22-cd1f-d1c3f4ffabb4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we extract image features using CLIP's visual encoder. These features will serve as inputs for the linear classifier."
      ],
      "metadata": {
        "id": "vXz6VIYqllXL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_clip_features(loader, model):\n",
        "    all_features = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in loader:\n",
        "            images = images.to(device)\n",
        "            # Process images to get CLIP features\n",
        "            features = model.get_image_features(images)\n",
        "            all_features.append(features.cpu())\n",
        "            all_labels.append(labels)\n",
        "\n",
        "            print(f\"Extracted features for batch\")\n",
        "\n",
        "    all_features = torch.cat(all_features)\n",
        "    all_labels = torch.cat(all_labels)\n",
        "\n",
        "    return all_features, all_labels\n",
        "\n",
        "# Extract features for training and test sets\n",
        "train_features, train_labels = extract_clip_features(train_loader, clip_model)\n",
        "test_features, test_labels = extract_clip_features(test_loader, clip_model)"
      ],
      "metadata": {
        "id": "E4VF-ggilTKV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8a38c5e-9e26-45ce-8f74-e7dc7da6aaa4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted features for batch\n",
            "Extracted features for batch\n",
            "Extracted features for batch\n",
            "Extracted features for batch\n",
            "Extracted features for batch\n",
            "Extracted features for batch\n",
            "Extracted features for batch\n",
            "Extracted features for batch\n",
            "Extracted features for batch\n",
            "Extracted features for batch\n",
            "Extracted features for batch\n",
            "Extracted features for batch\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a simple linear classifier (logistic regression)\n",
        "class LinearClassifier(torch.nn.Module):\n",
        "    def __init__(self, input_dim, num_classes):\n",
        "        super(LinearClassifier, self).__init__()\n",
        "        self.fc = torch.nn.Linear(input_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc(x)\n",
        "\n",
        "# Initialize the classifier\n",
        "input_dim = train_features.shape[1]  # CLIP embedding dimension\n",
        "num_classes = 10  # CIFAR-10 has 10 classes\n",
        "classifier = LinearClassifier(input_dim, num_classes).to(device)\n",
        "\n",
        "# Define loss and optimizer\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(classifier.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "def train_classifier(train_features, train_labels, model, criterion, optimizer, epochs=10):\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(train_features.to(device))\n",
        "        loss = criterion(outputs, train_labels.to(device))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        print(f'Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}')\n",
        "\n",
        "# Train the classifier\n",
        "train_classifier(train_features, train_labels, classifier, criterion, optimizer)"
      ],
      "metadata": {
        "id": "cOVgw3LYlvag",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a83bf39-8d35-4239-fb64-6f43a6df7f1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Loss: 2.3296\n",
            "Epoch 2/10, Loss: 2.3090\n",
            "Epoch 3/10, Loss: 2.3000\n",
            "Epoch 4/10, Loss: 2.2977\n",
            "Epoch 5/10, Loss: 2.2972\n",
            "Epoch 6/10, Loss: 2.2970\n",
            "Epoch 7/10, Loss: 2.2970\n",
            "Epoch 8/10, Loss: 2.2971\n",
            "Epoch 9/10, Loss: 2.2969\n",
            "Epoch 10/10, Loss: 2.2963\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation\n",
        "def evaluate_classifier(test_features, test_labels, model):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        outputs = model(test_features.to(device))\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        accuracy = (predicted == test_labels.to(device)).float().mean().item()\n",
        "    return accuracy\n",
        "\n",
        "accuracy = evaluate_classifier(test_features, test_labels, classifier)\n",
        "print(f'Test Accuracy: {accuracy * 100:.2f}%')"
      ],
      "metadata": {
        "id": "AWoNQFiXly-0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2002e224-93a1-41c8-d268-fc4c0f9bec9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 9.50%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## visualising after linear probe"
      ],
      "metadata": {
        "id": "sj6esbJSrfgm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text-Image Similarity Search with CLIP\n",
        "\n",
        "Text-image similarity search allows us to search for images that are most closely aligned with a given textual description, or vice versa, by comparing the embeddings in a shared space.\n",
        "\n",
        "Let's create an example where, given a text description, CLIP retrieves the best matching image from a set of images. This will demonstrate CLIP's text-to-image retrieval capability.\n",
        "\n",
        "In this section, we will:\n",
        "\n",
        "1. Provide a list of image URLs.\n",
        "2. Provide a text description as a query.\n",
        "3. CLIP will find the image that best matches the given text description based on cosine similarity.\n"
      ],
      "metadata": {
        "id": "-FxBqL0V6ZLH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "import torch\n",
        "\n",
        "# Function to fetch and preprocess images\n",
        "def preprocess_images(image_urls):\n",
        "    images = []\n",
        "    for url in image_urls:\n",
        "        response = requests.get(url)\n",
        "        img = Image.open(BytesIO(response.content))\n",
        "        image = preprocess(img).unsqueeze(0).to(device)\n",
        "        images.append(image)\n",
        "    return torch.cat(images)\n",
        "\n",
        "# Function for text-to-image search\n",
        "def text_to_image_search(text_query, image_urls):\n",
        "    # Preprocess the images\n",
        "    images = preprocess_images(image_urls)\n",
        "\n",
        "    # Encode the images\n",
        "    with torch.no_grad():\n",
        "        image_features = model.encode_image(images)\n",
        "\n",
        "    # Tokenize and encode the text query\n",
        "    text = clip.tokenize([text_query]).to(device)\n",
        "    with torch.no_grad():\n",
        "        text_features = model.encode_text(text)\n",
        "\n",
        "    # Normalize the embeddings\n",
        "    image_features /= image_features.norm(dim=-1, keepdim=True)\n",
        "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "    # Compute similarity between text and each image\n",
        "    similarities = (100.0 * image_features @ text_features.T).squeeze(1)\n",
        "\n",
        "    # Find the best matching image\n",
        "    best_match_idx = similarities.argmax().item()\n",
        "    best_similarity_score = similarities[best_match_idx].item()\n",
        "\n",
        "    return image_urls[best_match_idx], best_similarity_score\n"
      ],
      "metadata": {
        "id": "2h6_3PkW7NRq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: Set of image URLs to search from\n",
        "image_urls = [\n",
        "    \"https://upload.wikimedia.org/wikipedia/commons/3/3a/Cat03.jpg\",    # Cat image\n",
        "    \"https://upload.wikimedia.org/wikipedia/commons/6/62/Dog_face.png\",  # Dog image\n",
        "    \"https://upload.wikimedia.org/wikipedia/commons/9/9a/Car-Toyota.jpg\",# Car image\n",
        "]\n",
        "\n",
        "# Example text query\n",
        "text_query = \"a picture of a dog\"\n",
        "\n",
        "# Perform text-to-image search\n",
        "best_image_url, similarity_score = text_to_image_search(text_query, image_urls)\n",
        "\n",
        "# Show the result\n",
        "print(f\"Best Matching Image URL: {best_image_url}\")\n",
        "print(f\"Similarity Score: {similarity_score:.4f}\")"
      ],
      "metadata": {
        "id": "sBBMTqEr7aep"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine Tuning\n",
        "\n",
        "While CLIP is very effective out-of-the-box, you may want to fine-tune it on a specific dataset for specialized tasks. Fine-tuning CLIP on a domain-specific dataset can help it learn the nuances of that domain, thereby improving performance on domain-specific tasks.\n",
        "\n",
        "Fine-tuning involves continuing to train the model on a smaller, more specialized dataset (related to your domain) after its initial pretraining on a large general dataset. Fine-tuning can help CLIP:\n",
        "\n",
        "Focus on specific visual-text relationships relevant to a given domain.\n",
        "Adjust its embedding space for tasks with specific characteristics (e.g., medical images, satellite images).\n",
        "\n",
        "Hereâ€™s a high-level outline of the steps you would take to fine-tune CLIP on a custom dataset:\n",
        "\n",
        "- Dataset Preparation: You need a dataset of image-text pairs related to the task at hand. The dataset should have labeled images and corresponding textual descriptions.\n",
        "\n",
        "- Modify CLIP for Fine-Tuning: During fine-tuning, it is often beneficial to freeze the early layers of the model that capture general features. Weâ€™ll only update the higher layers that are more specific to the task.\n",
        "\n",
        "- Training Objective: The objective remains similar to CLIPâ€™s original trainingâ€”contrastive loss. We'll minimize the loss between matched image-text pairs while maximizing the loss between mismatched pairs.\n",
        "\n",
        "- Training Loop: The training loop involves calculating contrastive loss between image and text embeddings (like in the original CLIP training) and updating the modelâ€™s weights using a smaller learning rate."
      ],
      "metadata": {
        "id": "wefumXLR9hoV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import clip\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from PIL import Image\n",
        "import requests\n",
        "from io import BytesIO\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "xhCPdB43_KN7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "prepare dataset and dataloaders"
      ],
      "metadata": {
        "id": "HpAjz2Gp_NqR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom dataset class\n",
        "class CustomImageTextDataset(Dataset):\n",
        "    def __init__(self, image_urls, text_descriptions, preprocess):\n",
        "        self.image_urls = image_urls\n",
        "        self.text_descriptions = text_descriptions\n",
        "        self.preprocess = preprocess\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_urls)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Fetch image\n",
        "        response = requests.get(self.image_urls[idx])\n",
        "        img = Image.open(BytesIO(response.content))\n",
        "        img = self.preprocess(img)\n",
        "\n",
        "        # Fetch text\n",
        "        text = clip.tokenize([self.text_descriptions[idx]])[0]\n",
        "\n",
        "        return img, text\n",
        "\n",
        "# Simulated image URLs and text descriptions\n",
        "image_urls = [\n",
        "    \"https://upload.wikimedia.org/wikipedia/commons/3/3a/Cat03.jpg\",\n",
        "    \"https://upload.wikimedia.org/wikipedia/commons/6/62/Dog_face.png\",\n",
        "    \"https://upload.wikimedia.org/wikipedia/commons/9/9a/Car-Toyota.jpg\"\n",
        "]\n",
        "text_descriptions = [\"a cat\", \"a dog\", \"a car\"]\n",
        "\n",
        "# Initialize the dataset and dataloader\n",
        "dataset = CustomImageTextDataset(image_urls, text_descriptions, preprocess)\n",
        "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n"
      ],
      "metadata": {
        "id": "oEVqjGRb_P2_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load CLIP model and optimizer\n",
        "model, preprocess = clip.load(\"ViT-B/32\", device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Freeze the earlier layers if desired (optional)\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Unfreeze the last few layers\n",
        "for param in model.visual.transformer.resblocks[-1].parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "# Define optimizer (fine-tuning specific parameters)\n",
        "optimizer = optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-5)\n",
        "\n",
        "# Fine-tuning loop\n",
        "num_epochs = 5\n",
        "\n",
        "def fine_tune_clip(dataloader, model, optimizer, num_epochs):\n",
        "    model.train()  # Set model to training mode\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        running_loss = 0.0\n",
        "        for images, texts in dataloader:\n",
        "            images = images.to(device)\n",
        "            texts = texts.to(device)\n",
        "\n",
        "            # Zero the gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass: get image and text features\n",
        "            image_features = model.encode_image(images)\n",
        "            text_features = model.encode_text(texts)\n",
        "\n",
        "            # Normalize features\n",
        "            image_features = F.normalize(image_features, p=2, dim=-1)\n",
        "            text_features = F.normalize(text_features, p=2, dim=-1)\n",
        "\n",
        "            # Compute contrastive loss\n",
        "            logits_per_image = image_features @ text_features.T\n",
        "            logits_per_text = text_features @ image_features.T\n",
        "\n",
        "            # Targets are diagonal (correct image-text pairs)\n",
        "            targets = torch.arange(len(images), device=images.device)\n",
        "\n",
        "            # Compute contrastive loss in both directions\n",
        "            loss_image = F.cross_entropy(logits_per_image, targets)\n",
        "            loss_text = F.cross_entropy(logits_per_text, targets)\n",
        "            loss = (loss_image + loss_text) / 2\n",
        "\n",
        "            # Backward pass and optimize\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(dataloader):.4f}\")"
      ],
      "metadata": {
        "id": "5lx1ZRKu_IbM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform fine-tuning\n",
        "fine_tune_clip(dataloader, model, optimizer, num_epochs)"
      ],
      "metadata": {
        "id": "hi27CybX_XYZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        },
        "outputId": "bf611fa8-8ea8-4b0b-c202-50a5818532cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "UnidentifiedImageError",
          "evalue": "cannot identify image file <_io.BytesIO object at 0x7bac0982dd00>",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnidentifiedImageError\u001b[0m                    Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-40-2076dfd8b6d6>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Perform fine-tuning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mfine_tune_clip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-39-87dc97c9809e>\u001b[0m in \u001b[0;36mfine_tune_clip\u001b[0;34m(dataloader, model, optimizer, num_epochs)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mrunning_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtexts\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m             \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mtexts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 630\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    631\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    671\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 673\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    674\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-38-44dd433a0cf3>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;31m# Fetch image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_urls\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3496\u001b[0m         \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3497\u001b[0m     \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"cannot identify image file %r\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3498\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mUnidentifiedImageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3500\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnidentifiedImageError\u001b[0m: cannot identify image file <_io.BytesIO object at 0x7bac0982dd00>"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# References\n",
        "\n",
        "- https://openai.com/index/clip/\n",
        "\n",
        "- https://arxiv.org/abs/2103.00020\n",
        "\n",
        "- ChatGPT helped as well âš¡"
      ],
      "metadata": {
        "id": "ezprv3X959sv"
      }
    }
  ]
}
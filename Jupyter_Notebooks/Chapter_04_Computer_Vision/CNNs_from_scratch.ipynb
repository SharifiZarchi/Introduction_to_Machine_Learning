{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<font>\n",
    "<!-- <img src=\"https://sina.sharif.edu/~m_salehi/images/logo_sharif.png\" alt=\"SUT logo\" width=300 height=300 align=left class=\"saturate\"> -->\n",
    "<div dir=ltr align=center>\n",
    "<img src=\"https://sina.sharif.edu/~m_salehi/images/logo_sharif.png\" width=160 height=180>\n",
    "<br>\n",
    "<font color=0F5298 size=6>\n",
    "Introduction to Machine Learning <br>\n",
    "<font color= 6C3BAA size=6>\n",
    "CNN from Scratch <br>\n",
    "<font color=696880 size=5>\n",
    "<!-- <br> -->\n",
    "Computer Engineering Department\n",
    "<br>\n",
    "Sharif University of Technology\n",
    "\n",
    "<font color=696880 size=5>\n",
    "<br>\n",
    "CE 40477 - Fall 2025\n",
    "\n",
    "<font color=GREEN size=5>\n",
    "<br>\n",
    "Zahra Rahmani & Farzan Rahmani\n",
    "<!-- <br> -->\n",
    "\n",
    "____\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ecaqr9YkWpbL"
   },
   "source": [
    "## Notebook Objectives\n",
    "\n",
    "In this notebook we are going to implement and train a convolutional neural network from scratch using only numpy!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "1. [Libraries and Imports](#Libraries-and-Imports)</br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;1-1. [Get Sample Image](#Get-Sample-Image) <br>\n",
    "2. [NNs from scratch](#NNs-from-scratch)</br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;2-1. [Helper Functions](#Helper-Functions)<br>\n",
    "3. [Layers](#Layers)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;3-1. [Convolutional Layer](#Convolutional-Layer)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;3-1-1. [Stride and Padding](#Stride-and-Padding)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;3-2. [Max Pooling Layer](#Max-Pooling-Layer)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;3-3. [Average Pooling Layer](#Average-Pooling-Layer)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;3-4. [Flatten](#Flatten)\n",
    "<br>\n",
    "4. [Visualizations](#Visualizations)\n",
    "5. [Training](#Training)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;5-1. [Loading the Fashion-MNIST Dataset](#Loading-the-Fashion-MNIST-Dataset)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;5-2. [Training the Network](#Training-the-Network)<br>\n",
    "6. [Evaluation](#Evaluation)\n",
    "7. [PyTorch](#PyTorch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "PqFOwWLZVPI0"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import image as mpimg\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tqdm import trange"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Sample Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i6Me3jKEnZLt",
    "outputId": "bf64697d-be3a-4dcf-bf07-e66db7555b7a"
   },
   "outputs": [],
   "source": [
    "! gdown https://avatars.githubusercontent.com/u/6183533?v=4 -O asharifiz.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "zN_fZUEhqxol"
   },
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self):\n",
    "        self.inp = None\n",
    "        self.out = None\n",
    "\n",
    "    def __call__(self, inp: np.ndarray) -> np.ndarray:\n",
    "        return self.forward(inp)\n",
    "\n",
    "    def forward(self, inp: np.ndarray) -> np.ndarray:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def backward(self, up_grad: np.ndarray) -> np.ndarray:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def step(self, lr: float) -> None:\n",
    "        pass\n",
    "\n",
    "\n",
    "class Linear(Layer):\n",
    "    def __init__(self, in_dim: int, out_dim: int):\n",
    "        super().__init__()\n",
    "        # He initialization: better scaling for deep networks\n",
    "        self.w = 0.1 * np.random.randn(in_dim, out_dim)\n",
    "        self.b = np.zeros((1, out_dim))\n",
    "        self.dw = np.zeros_like(self.w)\n",
    "        self.db = np.zeros_like(self.b)\n",
    "\n",
    "    def forward(self, inp: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Perform the linear transformation: output = inp * W + b\"\"\"\n",
    "        self.inp = inp\n",
    "        self.out = np.dot(inp, self.w) + self.b\n",
    "        return self.out\n",
    "\n",
    "    def backward(self, up_grad: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Backpropagate the gradients through this layer.\"\"\"\n",
    "        # Compute gradients for weights and biases\n",
    "        self.dw = np.dot(self.inp.T, up_grad)  # Gradient wrt weights\n",
    "        self.db = np.sum(up_grad, axis=0, keepdims=True)  # Gradient wrt biases\n",
    "        # Compute gradient to propagate back (downstream)\n",
    "        down_grad = np.dot(up_grad, self.w.T)\n",
    "        return down_grad\n",
    "\n",
    "    def step(self, lr: float) -> None:\n",
    "        \"\"\"Update the weights and biases using the gradients.\"\"\"\n",
    "        self.w -= lr * self.dw\n",
    "        self.b -= lr * self.db\n",
    "\n",
    "\n",
    "class ReLU(Layer):\n",
    "    def forward(self, inp: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"ReLU Activation: f(x) = max(0, x)\"\"\"\n",
    "        self.inp = inp\n",
    "        self.out = np.maximum(0, inp)\n",
    "        return self.out\n",
    "\n",
    "    def backward(self, up_grad: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Backward pass for ReLU: derivative is 1 where input > 0, else 0.\"\"\"\n",
    "        down_grad = up_grad * (self.inp > 0)  # Efficient boolean indexing\n",
    "        return down_grad\n",
    "\n",
    "\n",
    "class Softmax(Layer):\n",
    "    def forward(self, inp: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Softmax Activation: f(x) = exp(x) / sum(exp(x))\"\"\"\n",
    "        # Subtract max for numerical stability\n",
    "        exp_values = np.exp(inp - np.max(inp, axis=1, keepdims=True))\n",
    "        self.out = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "        return self.out\n",
    "\n",
    "    def backward(self, up_grad: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Backward pass for Softmax using the Jacobian matrix.\"\"\"\n",
    "        down_grad = np.empty_like(up_grad)\n",
    "        for i in range(up_grad.shape[0]):\n",
    "            single_output = self.out[i].reshape(-1, 1)\n",
    "            jacobian = np.diagflat(single_output) - np.dot(single_output, single_output.T)\n",
    "            down_grad[i] = np.dot(jacobian, up_grad[i])\n",
    "        return down_grad\n",
    "\n",
    "\n",
    "class Loss:\n",
    "    def __init__(self):\n",
    "        self.prediction = None\n",
    "        self.target = None\n",
    "        self.loss = None\n",
    "\n",
    "    def __call__(self, prediction: np.ndarray, target: np.ndarray) -> float:\n",
    "        return self.forward(prediction, target)\n",
    "\n",
    "    def forward(self, prediction: np.ndarray, target: np.ndarray) -> float:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def backward(self) -> np.ndarray:\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class CrossEntropy(Loss):\n",
    "    def forward(self, prediction: np.ndarray, target: np.ndarray) -> float:\n",
    "        \"\"\"Cross-Entropy Loss for classification.\"\"\"\n",
    "        self.prediction = prediction\n",
    "        self.target = target\n",
    "        # Clip predictions to avoid log(0)\n",
    "        clipped_pred = np.clip(prediction, 1e-12, 1.0)\n",
    "        # Compute and return the loss\n",
    "        self.loss = -np.mean(np.sum(target * np.log(clipped_pred), axis=1))\n",
    "        return self.loss\n",
    "\n",
    "    def backward(self) -> np.ndarray:\n",
    "        \"\"\"Gradient of Cross-Entropy Loss.\"\"\"\n",
    "        # Gradient wrt prediction (assuming softmax and one-hot targets)\n",
    "        grad = -self.target / self.prediction / self.target.shape[0]\n",
    "        return grad\n",
    "\n",
    "\n",
    "class CNN:\n",
    "    def __init__(self, layers: list[Layer], loss_fn: Loss, lr: float) -> None:\n",
    "        \"\"\"\n",
    "        Convolutional Neural Network (CNN) class.\n",
    "        Arguments:\n",
    "        - layers: List of layers (e.g., Linear, ReLU, etc.).\n",
    "        - loss_fn: Loss function object (e.g., CrossEntropy, MSE).\n",
    "        - lr: Learning rate.\n",
    "        \"\"\"\n",
    "        self.layers = layers\n",
    "        self.loss_fn = loss_fn\n",
    "        self.lr = lr\n",
    "\n",
    "    def __call__(self, inp: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Makes the model callable, equivalent to forward pass.\"\"\"\n",
    "        return self.forward(inp)\n",
    "\n",
    "    def forward(self, inp: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Pass input through each layer sequentially.\"\"\"\n",
    "        for layer in self.layers:\n",
    "            inp = layer.forward(inp)\n",
    "        return inp\n",
    "\n",
    "    def loss(self, prediction: np.ndarray, target: np.ndarray) -> float:\n",
    "        \"\"\"Calculate the loss.\"\"\"\n",
    "        return self.loss_fn(prediction, target)\n",
    "\n",
    "    def backward(self) -> None:\n",
    "        \"\"\"Perform backpropagation by propagating the gradient backwards through the layers.\"\"\"\n",
    "        up_grad = self.loss_fn.backward()\n",
    "        for layer in reversed(self.layers):\n",
    "            up_grad = layer.backward(up_grad)\n",
    "\n",
    "    def update(self) -> None:\n",
    "        \"\"\"Update the parameters of each layer using the gradients and the learning rate.\"\"\"\n",
    "        for layer in self.layers:\n",
    "            layer.step(self.lr)\n",
    "\n",
    "    def train(self, x_train: np.ndarray, y_train: np.ndarray, epochs: int, batch_size: int) -> np.ndarray:\n",
    "        \"\"\"Train the MLP over the given dataset for a number of epochs.\"\"\"\n",
    "        losses, accuracies = np.empty(epochs), np.empty(epochs)\n",
    "        for epoch in (pbar := trange(epochs)):\n",
    "            running_loss = 0.0\n",
    "            correct = 0\n",
    "            for i in range(0, len(x_train), batch_size):\n",
    "                x_batch = x_train[i:i + batch_size]\n",
    "                y_batch = y_train[i:i + batch_size]\n",
    "\n",
    "                # Forward pass\n",
    "                prediction = self.forward(x_batch)\n",
    "\n",
    "                # Compute loss\n",
    "                running_loss += self.loss(prediction, y_batch) * batch_size\n",
    "\n",
    "                # Update correct\n",
    "                correct += np.sum(np.argmax(prediction, axis=1) == np.argmax(y_batch, axis=1))\n",
    "\n",
    "                # Backward pass\n",
    "                self.backward()\n",
    "\n",
    "                # Update parameters\n",
    "                self.update()\n",
    "\n",
    "            # Normalize running loss by total number of samples\n",
    "            running_loss /= len(x_train)\n",
    "            accuracy = 100 * correct / len(x_train)\n",
    "            pbar.set_description(f\"Loss: {running_loss:.3f} | Accuracy: {accuracy:.2f}% \")\n",
    "            losses[epoch] = running_loss\n",
    "            accuracies[epoch] = accuracy\n",
    "        return losses, accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "9AKM0RgiVSOQ"
   },
   "outputs": [],
   "source": [
    "def plot_conv(convolutions=None, img_src='asharifiz.png', sequential=False):\n",
    "    # Load the image\n",
    "    img = mpimg.imread(img_src)\n",
    "    cols = len(convolutions) + 1 if convolutions is not None else 1\n",
    "    fig, axes = plt.subplots(1, cols, figsize=(15, 15 * cols))\n",
    "\n",
    "    # Display the image\n",
    "    axes[0].imshow(img)\n",
    "    axes[0].axis('off')  # Hide the axis\n",
    "    axes[0].set_title('Original')\n",
    "\n",
    "    for i, conv in enumerate(convolutions):\n",
    "        x = np.transpose(img, (2, 0, 1))\n",
    "        x = np.expand_dims(x, axis=(0))\n",
    "        out = conv(x).squeeze()\n",
    "        out = np.transpose(out, (1, 2, 0))\n",
    "        axes[i + 1].imshow(out)\n",
    "        axes[i + 1].axis('off')  # Hide the axis\n",
    "        axes[i + 1].set_title(f'Filter {i + 1}')\n",
    "        if sequential:\n",
    "            img = out\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def plot_results(losses, accuracies):\n",
    "    # Plot the loss\n",
    "    plt.figure(figsize=(18, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(range(1, len(losses) + 1), losses)\n",
    "    plt.title('Training Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Plot the accuracy\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(range(1, len(accuracies) + 1), accuracies)\n",
    "    plt.title('Training Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, class_names, kept_classes):\n",
    "    dim = len(kept_classes)\n",
    "    labels = [class_names[i] for i in kept_classes]\n",
    "    # Plot the confusion matrix\n",
    "    conf_mat = confusion_matrix(y_true, y_pred)\n",
    "    norm_conf_mat = conf_mat / np.sum(conf_mat, axis=1)\n",
    "    # plot the matrix\n",
    "    fig, ax = plt.subplots()\n",
    "    plt.imshow(norm_conf_mat)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.xlabel('Predictions')\n",
    "    plt.ylabel('Labels')\n",
    "    plt.xticks(range(dim), labels, rotation=45)\n",
    "    plt.yticks(range(dim), labels)\n",
    "    plt.colorbar()\n",
    "    # Put number of each cell in plot\n",
    "    for i in range(dim):\n",
    "        for j in range(dim):\n",
    "            c = conf_mat[j, i]\n",
    "            color = 'black' if c > 500 else 'white'\n",
    "            ax.text(i, j, str(int(c)), va='center', ha='center', color=color)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def get_data(filter_classes):\n",
    "    fashion_mnist = fetch_openml(\"Fashion-MNIST\", parser='auto')\n",
    "    x, y = fashion_mnist['data'], fashion_mnist['target'].astype(int)\n",
    "    # Remove classes\n",
    "    filtered_indices = np.isin(y, filter_classes)\n",
    "    x, y = x[filtered_indices].to_numpy(), y[filtered_indices]\n",
    "    # Normalize the pixels to be in [-1, +1] range\n",
    "    x = ((x / 255.) - .5) * 2\n",
    "    removed_class_count = 0\n",
    "    for i in range(10):  # Fix the labels\n",
    "        if i in filter_classes and removed_class_count != 0:\n",
    "            y[y == i] = i - removed_class_count\n",
    "        elif i not in filter_classes:\n",
    "            removed_class_count += 1\n",
    "    # Do the train-test split\n",
    "    return train_test_split(x, y, test_size=10_000)\n",
    "\n",
    "\n",
    "def onehot_encoder(y, num_labels):\n",
    "    one_hot = np.zeros(shape=(y.size, num_labels), dtype=int)\n",
    "    one_hot[np.arange(y.size), y] = 1\n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GvFUeXMNadnZ"
   },
   "source": [
    "# Layers\n",
    "\n",
    "In this section we want to implement `Conv2D`, `MaxPool2D`, `AvgPool2D` and `Flatten` layers and train a CNN.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y3P4ztQQvHeN"
   },
   "source": [
    "\n",
    "Recall from our notebook [Neural Networks from Scratch](https://colab.research.google.com/github/SharifiZarchi/Introduction_to_Machine_Learning/blob/main/Jupyter_Notebooks/Chapter_03_Neural_Networks/NNs_from_scratch.ipynb) that the abstract `Layer` class is defined as follows:\n",
    "```\n",
    "class Layer:\n",
    "    def __init__(self):\n",
    "        self.inp = None\n",
    "        self.out = None\n",
    "\n",
    "    def __call__(self, inp: np.ndarray) -> np.ndarray:\n",
    "        return self.forward(inp)\n",
    "\n",
    "    def forward(self, inp: np.ndarray) -> np.ndarray:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def backward(self, up_grad: np.ndarray) -> np.ndarray:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def step(self, lr: float) -> None:\n",
    "        pass\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2G_b2_sAr1In"
   },
   "source": [
    "## Convolutional Layer\n",
    "Applies convolution operations to input images by sliding a learnable kernel over them, computing dot products with patches of the input. This helps detect spatial features like edges and textures.\n",
    "\n",
    "\n",
    "At first, we will build a simple convolutional layer without strides or padding for simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u0k6kL0GnRYa"
   },
   "outputs": [],
   "source": [
    "class SimpleConv2D(Layer):\n",
    "    def __init__(self, in_channels: int, out_channels: int, kernel_size: int):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "\n",
    "        # Initialize weights and biases\n",
    "        self.w = 0.1 * np.random.randn(out_channels, in_channels, kernel_size, kernel_size)\n",
    "        self.b = np.zeros((out_channels, 1))\n",
    "\n",
    "    def forward(self, inp: np.ndarray) -> np.ndarray:\n",
    "        self.inp = inp\n",
    "        batch_size, in_channels, height, width = inp.shape\n",
    "        assert in_channels == self.in_channels, \"Input channels must match.\"\n",
    "\n",
    "        # Output dimensions\n",
    "        out_height = height - self.kernel_size + 1\n",
    "        out_width = width - self.kernel_size + 1\n",
    "        self.out = np.zeros((batch_size, self.out_channels, out_height, out_width))\n",
    "\n",
    "        # Convolution operation\n",
    "        for i in range(out_height):\n",
    "            for j in range(out_width):\n",
    "                region = inp[:, :, i:i+self.kernel_size, j:j+self.kernel_size]\n",
    "                self.out[:, :, i, j] = np.tensordot(region, self.w, axes=([1, 2, 3], [1, 2, 3])) + self.b.T\n",
    "\n",
    "        return self.out\n",
    "\n",
    "    def backward(self, up_grad: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Backward pass for Conv2D layer.\"\"\"\n",
    "        batch_size, in_channels, height, width = self.inp.shape\n",
    "        _, _, out_height, out_width = up_grad.shape\n",
    "\n",
    "        # Initialize gradients\n",
    "        self.dw = np.zeros_like(self.w)\n",
    "        self.db = np.sum(up_grad, axis=(0, 2, 3), keepdims=True).reshape(self.out_channels, 1)\n",
    "        down_grad = np.zeros_like(self.inp)\n",
    "\n",
    "        # Gradient computation for weights and input\n",
    "        for i in range(out_height):\n",
    "            for j in range(out_width):\n",
    "                region = self.inp[:, :, i:i+self.kernel_size, j:j+self.kernel_size]\n",
    "                self.dw += np.tensordot(up_grad[:, :, i, j], region, axes=([0], [0]))  # Compute weight gradient\n",
    "                for n in range(batch_size):\n",
    "                    down_grad[n, :, i:i+self.kernel_size, j:j+self.kernel_size] += np.tensordot(self.w, up_grad[n, :, i, j], axes=(0, 0))\n",
    "\n",
    "        return down_grad\n",
    "\n",
    "    def step(self, lr: float) -> None:\n",
    "        \"\"\"Update weights and biases.\"\"\"\n",
    "        self.w -= lr * self.dw\n",
    "        self.b -= lr * self.db\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WMx8r4mXtkaH"
   },
   "source": [
    "Now that you've seen how we can build a simple convolutional layer, let's incorporate stride and padding into our design."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stride and Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4a4HoUpxr29X"
   },
   "outputs": [],
   "source": [
    "class Conv2D(Layer):\n",
    "    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, stride: int = 1, padding: int = 0):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "\n",
    "        # Initialize weights and biases\n",
    "        self.w = 0.1 * np.random.randn(out_channels, in_channels, kernel_size, kernel_size)\n",
    "        self.b = np.zeros((out_channels, 1))\n",
    "\n",
    "    def forward(self, inp: np.ndarray) -> np.ndarray:\n",
    "        self.inp = inp\n",
    "        batch_size, in_channels, height, width = inp.shape\n",
    "\n",
    "        # Padding the input\n",
    "        self.padded_inp = np.pad(inp, ((0, 0), (0, 0), (self.padding, self.padding), (self.padding, self.padding)), mode='constant')\n",
    "\n",
    "        # Output dimensions\n",
    "        out_height = (height + 2 * self.padding - self.kernel_size) // self.stride + 1\n",
    "        out_width = (width + 2 * self.padding - self.kernel_size) // self.stride + 1\n",
    "        self.out = np.zeros((batch_size, self.out_channels, out_height, out_width))\n",
    "\n",
    "        # Convolution operation\n",
    "        for i in range(out_height):\n",
    "            for j in range(out_width):\n",
    "                region = self.padded_inp[:, :, i*self.stride:i*self.stride+self.kernel_size, j*self.stride:j*self.stride+self.kernel_size]\n",
    "                self.out[:, :, i, j] = np.tensordot(region, self.w, axes=([1, 2, 3], [1, 2, 3])) + self.b.T\n",
    "\n",
    "        return self.out\n",
    "\n",
    "    def backward(self, up_grad: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Backward pass for Conv2D layer.\"\"\"\n",
    "        batch_size, in_channels, height, width = self.inp.shape\n",
    "        _, _, out_height, out_width = up_grad.shape\n",
    "\n",
    "        # Initialize gradients\n",
    "        self.dw = np.zeros_like(self.w)\n",
    "        self.db = np.sum(up_grad, axis=(0, 2, 3), keepdims=True).reshape(self.out_channels, 1)\n",
    "        down_grad = np.zeros_like(self.padded_inp)\n",
    "\n",
    "        # Gradient computation for weights and input\n",
    "        for i in range(out_height):\n",
    "            for j in range(out_width):\n",
    "                region = self.padded_inp[:, :, i*self.stride:i*self.stride+self.kernel_size, j*self.stride:j*self.stride+self.kernel_size]\n",
    "                self.dw += np.tensordot(up_grad[:, :, i, j], region, axes=([0], [0]))  # Compute weight gradient\n",
    "                for n in range(batch_size):\n",
    "                    down_grad[n, :, i*self.stride:i*self.stride+self.kernel_size, j*self.stride:j*self.stride+self.kernel_size] += np.tensordot(self.w, up_grad[n, :, i, j], axes=(0, 0))\n",
    "\n",
    "        # Remove padding if applied\n",
    "        if self.padding > 0:\n",
    "            down_grad = down_grad[:, :, self.padding:-self.padding, self.padding:-self.padding]\n",
    "\n",
    "        return down_grad\n",
    "\n",
    "    def step(self, lr: float) -> None:\n",
    "        \"\"\"Update weights and biases.\"\"\"\n",
    "        self.w -= lr * self.dw\n",
    "        self.b -= lr * self.db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-coiX1IbNCaM"
   },
   "source": [
    "Now lets test our `Conv2D` layer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mNjAl-5-NGUO",
    "outputId": "2c955357-35a9-475a-f6fb-e10f128866c5"
   },
   "outputs": [],
   "source": [
    "x = np.arange(9).reshape(1, 1, 3, 3)\n",
    "print(x)\n",
    "\n",
    "convolution = Conv2D(in_channels=1 , out_channels=1, kernel_size=2)\n",
    "kernel = np.ones(shape=(1, 1, 2, 2))\n",
    "convolution.w = kernel\n",
    "\n",
    "print(convolution(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4KjqIR_Is_Wx"
   },
   "source": [
    "## Max Pooling Layer\n",
    "Reduces the spatial dimensions of the input by selecting the maximum value from non-overlapping regions (windows) of the input. This helps in downsampling and capturing important features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x4kAUQ17s3c2"
   },
   "outputs": [],
   "source": [
    "class MaxPool2D(Layer):\n",
    "    def __init__(self, pool_size: int = 2, stride: int = 2):\n",
    "        \"\"\"Max Pooling Layer.\"\"\"\n",
    "        super().__init__()\n",
    "        self.pool_size = pool_size\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, inp: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Forward pass of max pooling.\"\"\"\n",
    "        self.inp = inp\n",
    "        batch_size, channels, height, width = inp.shape\n",
    "\n",
    "        out_height = (height - self.pool_size) // self.stride + 1\n",
    "        out_width = (width - self.pool_size) // self.stride + 1\n",
    "\n",
    "        out = np.zeros((batch_size, channels, out_height, out_width))\n",
    "\n",
    "        for i in range(out_height):\n",
    "            for j in range(out_width):\n",
    "                region = inp[:, :, i*self.stride:i*self.stride+self.pool_size, j*self.stride:j*self.stride+self.pool_size]\n",
    "                out[:, :, i, j] = np.max(region, axis=(2, 3))\n",
    "\n",
    "        self.out = out\n",
    "        return self.out\n",
    "\n",
    "    def backward(self, up_grad: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Backward pass for max pooling.\"\"\"\n",
    "        batch_size, channels, height, width = self.inp.shape\n",
    "        down_grad = np.zeros_like(self.inp)\n",
    "\n",
    "        out_height, out_width = up_grad.shape[2], up_grad.shape[3]\n",
    "\n",
    "        for i in range(out_height):\n",
    "            for j in range(out_width):\n",
    "                region = self.inp[:, :, i*self.stride:i*self.stride+self.pool_size, j*self.stride:j*self.stride+self.pool_size]\n",
    "                max_mask = (region == np.max(region, axis=(2, 3), keepdims=True))\n",
    "                down_grad[:, :, i*self.stride:i*self.stride+self.pool_size, j*self.stride:j*self.stride+self.pool_size] += max_mask * up_grad[:, :, i, j][:, :, None, None]\n",
    "\n",
    "        return down_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XL-IKsqSN8IL"
   },
   "source": [
    "Now lets test our `MaxPool2D` layer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0rBtnVZMN8V-",
    "outputId": "d2503c71-60ab-4347-f04d-a67743f45afe"
   },
   "outputs": [],
   "source": [
    "x = np.arange(16).reshape(1, 1, 4, 4)\n",
    "print(x)\n",
    "\n",
    "maxpool = MaxPool2D()\n",
    "print(maxpool(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "toBCxhwTs8Br"
   },
   "source": [
    "## Average Pooling Layer\n",
    "Similar to MaxPooling, but instead of taking the maximum value, it averages the values in each window. This smoothens the features while reducing spatial dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O3C5PnIAs4d5"
   },
   "outputs": [],
   "source": [
    "class AvgPool2D(Layer):\n",
    "    def __init__(self, pool_size: int = 2, stride: int = 2):\n",
    "        \"\"\"Average Pooling Layer.\"\"\"\n",
    "        super().__init__()\n",
    "        self.pool_size = pool_size\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, inp: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Forward pass of average pooling.\"\"\"\n",
    "        self.inp = inp\n",
    "        batch_size, channels, height, width = inp.shape\n",
    "\n",
    "        out_height = (height - self.pool_size) // self.stride + 1\n",
    "        out_width = (width - self.pool_size) // self.stride + 1\n",
    "\n",
    "        out = np.zeros((batch_size, channels, out_height, out_width))\n",
    "\n",
    "        for i in range(out_height):\n",
    "            for j in range(out_width):\n",
    "                region = inp[:, :, i*self.stride:i*self.stride+self.pool_size, j*self.stride:j*self.stride+self.pool_size]\n",
    "                out[:, :, i, j] = np.mean(region, axis=(2, 3))\n",
    "\n",
    "        self.out = out\n",
    "        return self.out\n",
    "\n",
    "    def backward(self, up_grad: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Backward pass for average pooling.\"\"\"\n",
    "        batch_size, channels, height, width = self.inp.shape\n",
    "        down_grad = np.zeros_like(self.inp)\n",
    "\n",
    "        out_height, out_width = up_grad.shape[2], up_grad.shape[3]\n",
    "\n",
    "        for i in range(out_height):\n",
    "            for j in range(out_width):\n",
    "                region_grad = up_grad[:, :, i, j][:, :, None, None] / (self.pool_size * self.pool_size)\n",
    "                down_grad[:, :, i*self.stride:i*self.stride+self.pool_size, j*self.stride:j*self.stride+self.pool_size] += region_grad\n",
    "\n",
    "        return down_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tJKvrW_6OXqk"
   },
   "source": [
    "Now lets test our `AvgPool2D` layer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1JEOeA-jOXVB",
    "outputId": "7897bf9a-dee2-42e1-bcbe-ad61dafcc9a7"
   },
   "outputs": [],
   "source": [
    "x = np.arange(16).reshape(1, 1, 4, 4)\n",
    "print(x)\n",
    "\n",
    "avgpool = AvgPool2D()\n",
    "print(avgpool(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FF2tcD4Is41v"
   },
   "source": [
    "## Flatten\n",
    "Reshapes a multi-dimensional input (like an image) into a single vector. This is typically used to transition from convolutional layers to fully connected layers in a neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aomYpXp8s4US"
   },
   "outputs": [],
   "source": [
    "class Flatten(Layer):\n",
    "    def forward(self, inp: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Flatten the input into a 2D array.\"\"\"\n",
    "        self.inp_shape = inp.shape\n",
    "        return inp.reshape(self.inp_shape[0], -1)\n",
    "\n",
    "    def backward(self, up_grad: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Reshape the gradient back to the original input shape.\"\"\"\n",
    "        return up_grad.reshape(self.inp_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G_PWFuNVPBlw"
   },
   "source": [
    "Now lets test our `Flatten` layer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L0i2jDemO3TR",
    "outputId": "a93827a2-5be0-44ee-a422-8ff5f680ee03"
   },
   "outputs": [],
   "source": [
    "x = np.arange(9).reshape(1, 1, 3, 3)\n",
    "print(x)\n",
    "\n",
    "flatten = Flatten()\n",
    "print(flatten(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uW-NKXjvtOxX"
   },
   "source": [
    "# Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "mUxvSjdlxmaO"
   },
   "outputs": [],
   "source": [
    "# @title Custome Kernels\n",
    "\n",
    "# Black and White Kernel\n",
    "black_and_white = Conv2D(in_channels=3 , out_channels=3, kernel_size=3)\n",
    "black_and_white.w = np.ones(shape=(3, 3, 3, 3)) / 27\n",
    "\n",
    "\n",
    "# Red Channel\n",
    "red = Conv2D(in_channels=3, out_channels=3, kernel_size=1)\n",
    "red.w = np.zeros_like(red.w)\n",
    "red.w[0, 0, :, :] = np.array([1])\n",
    "\n",
    "\n",
    "# Green Channel\n",
    "green = Conv2D(in_channels=3, out_channels=3, kernel_size=1)\n",
    "green.w = np.zeros_like(green.w)\n",
    "green.w[1, 1, :] = np.array([1])\n",
    "\n",
    "\n",
    "# Blue Channel\n",
    "blue = Conv2D(in_channels=3, out_channels=3, kernel_size=1)\n",
    "blue.w = np.zeros_like(blue.w)\n",
    "blue.w[2, 2, :] = np.array([1])\n",
    "\n",
    "\n",
    "# Gaussian Blur Kernel\n",
    "gaussian3 = Conv2D(in_channels=3, out_channels=3, kernel_size=3)\n",
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        if i == j:\n",
    "            gaussian3.w[i, j, :, :] = np.array([[1, 2, 1],\n",
    "                                                [2, 4, 2],\n",
    "                                                [1, 2, 1]]) / 16\n",
    "        else:\n",
    "            gaussian3.w[i, j, :, :] = 0\n",
    "\n",
    "\n",
    "# Gaussian Blur Kernel\n",
    "gaussian5 = Conv2D(in_channels=3, out_channels=3, kernel_size=5)\n",
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        if i == j:\n",
    "            gaussian5.w[i, j, :, :] = np.array([[1, 4, 6, 4, 1],\n",
    "                                                [4, 16, 24, 16, 4],\n",
    "                                                [6, 24, 36, 24, 6],\n",
    "                                                [4, 16, 24, 16, 4],\n",
    "                                                [1, 4, 6, 4, 1]]) / 256\n",
    "        else:\n",
    "            gaussian5.w[i, j, :, :] = 0\n",
    "\n",
    "\n",
    "# Sobel (Vertical)\n",
    "sobel_v = Conv2D(in_channels=3, out_channels=3, kernel_size=3)\n",
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        if i == j:\n",
    "            sobel_v.w[i, j, :, :] = np.array([[1, 0, -1],\n",
    "                                              [2, 0, -2],\n",
    "                                              [1, 0, -1]])\n",
    "        else:\n",
    "            sobel_v.w[i, j, :, :] = 0\n",
    "\n",
    "# Sobel (Horizontal)\n",
    "sobel_h = Conv2D(in_channels=3, out_channels=3, kernel_size=3)\n",
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        if i == j:\n",
    "            sobel_h.w[i, j, :, :] = np.array([[1, 2, 1],\n",
    "                                              [0, 0, 0],\n",
    "                                              [-1, -2, -1]])\n",
    "        else:\n",
    "            sobel_h.w[i, j, :, :] = 0\n",
    "\n",
    "\n",
    "# Max Pool\n",
    "max_pool = MaxPool2D()\n",
    "\n",
    "# Average Pool\n",
    "avg_pool = AvgPool2D()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 225
    },
    "id": "DJD_uOHX7kkd",
    "outputId": "a00f62a6-43e8-435f-f8aa-4f533dbcf5a3"
   },
   "outputs": [],
   "source": [
    "plot_conv(convolutions=[red, green, blue, black_and_white, avg_pool])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 225
    },
    "id": "K807leMxtODC",
    "outputId": "9139c0ec-c20f-47eb-b666-8d97cab8b992"
   },
   "outputs": [],
   "source": [
    "plot_conv(convolutions=[gaussian3, gaussian5, max_pool,\n",
    "                        avg_pool, black_and_white],\n",
    "          sequential=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AzONLtl5kc8W"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r93pdZdrZ77B"
   },
   "source": [
    "## Loading the Fashion-MNIST Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bOCW3Kh9azY6"
   },
   "source": [
    "For simplicity you can use `get_data` to load the Fashion-MNIST dataset. Since we aren't using GPUs, in order to save time and get better results, we are only going to include 3 classes in our training. However you can easily modify this cell to include different classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xmRyKM-oZ-hh"
   },
   "outputs": [],
   "source": [
    "class_names = {0: 'T-shirt/top', 1: 'Trouser', 2: 'Pullover',\n",
    "               3: 'Dress', 4: 'Coat', 5:  'Sandal', 6: 'Shirt',\n",
    "               7: 'Sneaker', 8: 'Bag', 9: 'Ankle boot'}\n",
    "\n",
    "# Include all the classes you want to see in training\n",
    "kept_classes = [0, 1, 7]  # T-shirt/top, Trouser, Sneaker\n",
    "\n",
    "# Download the dataset and split it into training and testing sets\n",
    "x_train, x_test, y_train, y_test = get_data(kept_classes)\n",
    "\n",
    "# Reshape the images for the convolutional neural network\n",
    "x_train = x_train.reshape(-1, 1, 28, 28)\n",
    "x_test = x_test.reshape(-1, 1, 28, 28)\n",
    "\n",
    "# One-hot encode the target labels of the training set\n",
    "y_train = onehot_encoder(y_train, num_labels=len(kept_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sZeNgQXybi4P"
   },
   "source": [
    "## Training the Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2qfmP-hYkpfz"
   },
   "source": [
    "Now we can define the network and train it on the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 519
    },
    "id": "eaIADoc8W7CS",
    "outputId": "e8711a06-159f-4b5f-97ac-e0f1ea778fbf"
   },
   "outputs": [],
   "source": [
    "# Define the layers of the neural network\n",
    "layers = [Conv2D(1, 4, 7, stride=3),\n",
    "          ReLU(),\n",
    "          MaxPool2D(),\n",
    "          Conv2D(4, 4, 3, padding=1),\n",
    "          ReLU(),\n",
    "          MaxPool2D(),\n",
    "          Flatten(),\n",
    "          ReLU(),\n",
    "          Linear(16, len(kept_classes)),\n",
    "          Softmax()]\n",
    "\n",
    "# Create the model\n",
    "model = CNN(layers, CrossEntropy(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "results = model.train(x_train, y_train, epochs=30, batch_size=16)\n",
    "\n",
    "# Plot the loss curve and accuracies\n",
    "plot_results(*results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H4Uh-ddyk2mR"
   },
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sgO7eZ9Uk5X0"
   },
   "source": [
    "We can measure the models accuracy on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I3M4fWNaZoHW",
    "outputId": "fd447752-5d69-4c3b-afea-89a8bab40105"
   },
   "outputs": [],
   "source": [
    "# Test the model\n",
    "y_prediction = np.argmax(model(x_test), axis=1)\n",
    "acc = 100 * np.mean(y_prediction == y_test)\n",
    "print(f'Test accuracy with {len(y_train)} training examples on {len(y_test)} test samples is {acc:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I2DrYQPilBlF"
   },
   "source": [
    "The confusion matrix can also be observed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 518
    },
    "id": "49LwgNLMj72R",
    "outputId": "8f21bc85-da70-4612-cb36-33d0aa918f9c"
   },
   "outputs": [],
   "source": [
    "# Plot the confusion matrix\n",
    "plot_confusion_matrix(y_test, y_prediction, class_names, kept_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q_OHTSEKRqXJ"
   },
   "source": [
    "# PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xmq1epcfZse9"
   },
   "source": [
    "Here we implement the same CNN but we use PyTorch instead of coding it from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "xzj78-EhSZDQ"
   },
   "outputs": [],
   "source": [
    "# @title imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "lSo0v72lShvn"
   },
   "outputs": [],
   "source": [
    "# @title dataset\n",
    "\n",
    "def get_data(filter_classes):\n",
    "    # Download and prepare the dataset\n",
    "    trainset = torchvision.datasets.FashionMNIST(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
    "    testset = torchvision.datasets.FashionMNIST(root='./data', train=False, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "    # Function to filter dataset and remap labels\n",
    "    def filter_and_remap(dataset, filter_classes):\n",
    "        data = dataset.data.numpy()\n",
    "        targets = dataset.targets.numpy()\n",
    "\n",
    "        # Remove classes\n",
    "        filtered_indices = np.isin(targets, filter_classes)\n",
    "        data, targets = data[filtered_indices], targets[filtered_indices]\n",
    "\n",
    "        # Normalize the pixels to be in [-1, +1] range\n",
    "        data = ((data / 255.0) - 0.5) * 2\n",
    "\n",
    "        # Remap labels\n",
    "        removed_class_count = 0\n",
    "        for i in range(10):\n",
    "            if i in filter_classes and removed_class_count != 0:\n",
    "                targets[targets == i] = i - removed_class_count\n",
    "            elif i not in filter_classes:\n",
    "                removed_class_count += 1\n",
    "\n",
    "        # Update the dataset\n",
    "        dataset.data = torch.tensor(data, dtype=torch.float32)\n",
    "        dataset.targets = torch.tensor(targets, dtype=torch.long)\n",
    "        return dataset\n",
    "\n",
    "    # Filter and remap the train and test datasets\n",
    "    trainset = filter_and_remap(trainset, filter_classes)\n",
    "    testset = filter_and_remap(testset, filter_classes)\n",
    "\n",
    "    return trainset, testset\n",
    "\n",
    "\n",
    "kept_classes = [0, 1, 7]  # T-shirt/top, Trouser, Sneaker\n",
    "trainset, testset = get_data(kept_classes)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=16, shuffle=True, num_workers=2)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=16, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "0ljyv5tAYXni"
   },
   "outputs": [],
   "source": [
    "# @title helper functions\n",
    "\n",
    "def train_epoch(model, dataloader, loss_fn, optimizer):\n",
    "    model.train()\n",
    "    train_loss = 0.\n",
    "    train_acc = 0.\n",
    "    for images, labels in dataloader:\n",
    "        images, labels = images, labels\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(images)\n",
    "        loss = loss_fn(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        train_acc += (logits.argmax(dim=1) == labels).sum().item()\n",
    "    train_loss /= len(dataloader)\n",
    "    train_acc /= len(dataloader.dataset)\n",
    "    return train_loss, 100 * train_acc\n",
    "\n",
    "def train_model(model, loader, optimizer, n_epochs):\n",
    "    losses, accuracies = np.empty(n_epochs), np.empty(n_epochs)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    for i in (pbar := trange(n_epochs)):\n",
    "        loss, acc = train_epoch(model, loader, loss_fn, optimizer)\n",
    "        losses[i], accuracies[i] = loss, acc\n",
    "        pbar.set_description(f'Loss: {loss:.3f} | Accuracy: {acc:.2f}% ')\n",
    "    return losses, accuracies\n",
    "\n",
    "def test_model(model, loader):\n",
    "    model.eval()\n",
    "    acc = 0.\n",
    "    y_test, y_prediction =[], []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            logits = model(images)\n",
    "            pred = logits.argmax(dim=1)\n",
    "            acc += (pred == labels).sum().item()\n",
    "            y_test.extend(labels.numpy())\n",
    "            y_prediction.extend(pred.numpy())\n",
    "    size = len(loader.dataset)\n",
    "    acc = 100 * acc / size\n",
    "    print(f'Test accuracy on {size} test samples is {acc:.2f}%')\n",
    "    return y_test, y_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 519
    },
    "id": "aM1nPR2JSlG3",
    "outputId": "87e31567-c5c7-43f4-8254-494c8463ff88"
   },
   "outputs": [],
   "source": [
    "# Create the model\n",
    "model = nn.Sequential(nn.Conv2d(1, 4, kernel_size=7, stride=3),\n",
    "                      nn.ReLU(),\n",
    "                      nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "                      nn.Conv2d(4, 4, kernel_size=3, padding=1),\n",
    "                      nn.ReLU(),\n",
    "                      nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "                      nn.Flatten(),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(16, len(kept_classes)))\n",
    "\n",
    "# Setup the optimizer\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "results = train_model(model, trainloader, optimizer, n_epochs=30)\n",
    "\n",
    "# Plot the loss curve and accuracies\n",
    "plot_results(*results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 535
    },
    "id": "2jbF-BsOa97D",
    "outputId": "b55e7ffb-e185-4fa7-dbdd-0c7a8980a8ee"
   },
   "outputs": [],
   "source": [
    "# Test the model\n",
    "y_test, y_pred = test_model(model, testloader)\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plot_confusion_matrix(y_test, y_pred, class_names, kept_classes)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "Ecaqr9YkWpbL"
   ],
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

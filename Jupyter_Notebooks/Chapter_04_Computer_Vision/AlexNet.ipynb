{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<font>\n",
    "<!-- <img src=\"https://sina.sharif.edu/~m_salehi/images/logo_sharif.png\" alt=\"SUT logo\" width=300 height=300 align=left class=\"saturate\"> -->\n",
    "<div dir=ltr align=center>\n",
    "<img src=\"https://sina.sharif.edu/~m_salehi/images/logo_sharif.png\" width=160 height=180>\n",
    "<br>\n",
    "<font color=0F5298 size=6>\n",
    "Introduction to Machine Learning <br>\n",
    "<font color= 6C3BAA size=6>\n",
    "AlexNet <br>\n",
    "<font color=696880 size=5>\n",
    "<!-- <br> -->\n",
    "Computer Engineering Department\n",
    "<br>\n",
    "Sharif University of Technology\n",
    "\n",
    "<font color=696880 size=5>\n",
    "<br>\n",
    "CE 40477 - Fall 2025\n",
    "\n",
    "<font color=GREEN size=5>\n",
    "<br>\n",
    "Zahra Rahmani & Farzan Rahmani\n",
    "<!-- <br> -->\n",
    "\n",
    "____\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "1. [AlexNet Architecture](#AlexNet-Architecture)\n",
    "2. [Why Does AlexNet Achieve Better Results?](#Why-Does-AlexNet-Achieve-Better-Results)\n",
    "3. [AlexNet Implementation](#AlexNet-Implementation)\n",
    "4. [Library Imports & Constants](#Library-Imports--Constant-Definitions)\n",
    "5. [Architecture Overview](#Architecture-Overview)\n",
    "6. [Helper Functions](#Helper-Functions)\n",
    "7. [Train & Test](#Train--Test-Functions)\n",
    "8. [Observations](#Observation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AlexNet Architecture\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "AlexNet is a groundbreaking convolutional neural network (CNN) architecture designed by Geoffrey Hinton and his student Alex Krizhevsky. It gained fame as the winner of the 2012 ImageNet Large Scale Visual Recognition Challenge (ILSVRC), where it achieved remarkable results that significantly outperformed traditional machine learning approaches.\n",
    "\n",
    "<br>\n",
    "\n",
    "After AlexNet's success, the field of deep learning saw a surge in the development of deeper and more complex neural network architectures, including influential models like VGGNet and GoogLeNet.\n",
    "\n",
    "<br>\n",
    "\n",
    "In the ILSVRC 2012 competition, AlexNet achieved an impressive top-5 error rate of 15.3%, translating to an accuracy of 57.1%. This performance marked a significant milestone in image classification, demonstrating the effectiveness of deep learning techniques in visual recognition tasks. By comparison, traditional machine learning classification algorithms struggled to achieve similar levels of accuracy.\n",
    "\n",
    "\n",
    "\n",
    "![title](https://raw.githubusercontent.com/blurred-machine/Data-Science/master/Deep%20Learning%20SOTA/img/alexnet2.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The following table explains the network structure of AlexNet:\n",
    "\n",
    "<table>\n",
    "<thead>\n",
    "\t<tr>\n",
    "\t\t<th>Size / Operation</th>\n",
    "\t\t<th>Filter</th>\n",
    "\t\t<th>Depth</th>\n",
    "\t\t<th>Stride</th>\n",
    "\t\t<th>Padding</th>\n",
    "\t\t<th>Number of Parameters</th>\n",
    "\t\t<th>Forward Computation</th>\n",
    "\t</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "\t<tr>\n",
    "\t\t<td>Input Image<br>(3 * 227 * 227)</td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t</tr>\n",
    "\t<tr>\n",
    "\t\t<td>Conv1 + Relu<br>(96 * 55 * 55)</td>\n",
    "\t\t<td>11 * 11</td>\n",
    "\t\t<td>96</td>\n",
    "\t\t<td>4</td>\n",
    "\t\t<td></td>\n",
    "\t\t<td>(11 * 11 * 3 + 1) * 96 = 34,944</td>\n",
    "\t\t<td>(11 * 11 * 3 + 1) * 96 * 55 * 55 = 105,705,600</td>\n",
    "\t</tr>\n",
    "\t<tr>\n",
    "\t\t<td>Max Pooling<br>(96 * 27 * 27)</td>\n",
    "\t\t<td>3 * 3</td>\n",
    "\t\t<td></td>\n",
    "\t\t<td>2</td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t</tr>\n",
    "\t<tr>\n",
    "\t\t<td>Norm</td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t</tr>\n",
    "\t<tr>\n",
    "\t\t<td>Conv2 + Relu<br>(256 * 27 * 27)</td>\n",
    "\t\t<td>5 * 5</td>\n",
    "\t\t<td>256</td>\n",
    "\t\t<td>1</td>\n",
    "\t\t<td>2</td>\n",
    "\t\t<td>(5 * 5 * 96 + 1) * 256 = 614,656</td>\n",
    "\t\t<td>(5 * 5 * 96 + 1) * 256 * 27 * 27 = 448,084,224</td>\n",
    "\t</tr>\n",
    "\t<tr>\n",
    "\t\t<td>Max Pooling<br>(256 * 13 * 13)</td>\n",
    "\t\t<td>3 * 3</td>\n",
    "\t\t<td></td>\n",
    "\t\t<td>2</td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t</tr>\n",
    "\t<tr>\n",
    "\t\t<td>Norm</td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t</tr>\n",
    "\t<tr>\n",
    "\t\t<td>Conv3 + Relu<br>(384 * 13 * 13)</td>\n",
    "\t\t<td>3 * 3</td>\n",
    "\t\t<td>384</td>\n",
    "\t\t<td>1</td>\n",
    "\t\t<td>1</td>\n",
    "\t\t<td>(3 * 3 * 256 + 1) * 384 = 885,120</td>\n",
    "\t\t<td>(3 * 3 * 256 + 1) * 384 * 13 * 13 = 149,585,280</td>\n",
    "\t</tr>\n",
    "\t<tr>\n",
    "\t\t<td>Conv4 + Relu<br>(384 * 13 * 13)</td>\n",
    "\t\t<td>3 * 3</td>\n",
    "\t\t<td>384</td>\n",
    "\t\t<td>1</td>\n",
    "\t\t<td>1</td>\n",
    "\t\t<td>(3 * 3 * 384 + 1) * 384 = 1,327,488</td>\n",
    "\t\t<td>(3 * 3 * 384 + 1) * 384 * 13 * 13 = 224,345,472</td>\n",
    "\t</tr>\n",
    "\t<tr>\n",
    "\t\t<td>Conv5 + Relu<br>(256 * 13 * 13)</td>\n",
    "\t\t<td>3 * 3</td>\n",
    "\t\t<td>256</td>\n",
    "\t\t<td>1</td>\n",
    "\t\t<td>1</td>\n",
    "\t\t<td>(3 * 3 * 384 + 1) * 256 = 884,992</td>\n",
    "\t\t<td>(3 * 3 * 384 + 1) * 256 * 13 * 13 = 149,563,648</td>\n",
    "\t</tr>\n",
    "\t<tr>\n",
    "\t\t<td>Max Pooling<br>(256 * 6 * 6)</td>\n",
    "\t\t<td>3 * 3</td>\n",
    "\t\t<td></td>\n",
    "\t\t<td>2</td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t</tr>\n",
    "\t<tr>\n",
    "\t\t<td>Dropout (rate 0.5)</td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t</tr>\n",
    "\t<tr>\n",
    "\t\t<td>FC6 + Relu<br>(4096)</td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td>256 * 6 * 6 * 4096 = 37,748,736</td>\n",
    "\t\t<td>256 * 6 * 6 * 4096 = 37,748,736</td>\n",
    "\t</tr>\n",
    "\t<tr>\n",
    "\t\t<td>Dropout (rate 0.5)</td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t</tr>\n",
    "\t<tr>\n",
    "\t\t<td>FC7 + Relu<br>(4096)</td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td>4096 * 4096 = 16,777,216</td>\n",
    "\t\t<td>4096 * 4096 = 16,777,216</td>\n",
    "\t</tr>\n",
    "\t<tr>\n",
    "\t\t<td>FC8 + Relu<br>(1000 classes)</td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td>4096 * 1000 = 4,096,000</td>\n",
    "\t\t<td>4096 * 1000 = 4,096,000</td>\n",
    "\t</tr>\n",
    "\t<tr>\n",
    "\t\t<td>Overall</td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td>62,369,152 = 62.3 million</td>\n",
    "\t\t<td>1,135,906,176 = 1.1 billion</td>\n",
    "\t</tr>\n",
    "\t<tr>\n",
    "\t\t<td>Conv VS FC</td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td>Conv: 3.7 million (6%), FC: 58.6 million (94%)</td>\n",
    "\t\t<td>Conv: 1.08 billion (95%), FC: 58.6 million (5%)</td>\n",
    "\t</tr>\n",
    "</tbody>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Does AlexNet Achieve Better Results?\n",
    "\n",
    "\n",
    "\n",
    "In this section, we will briefly highlight four key techniques that contribute to the improved performance of AlexNet:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **ReLU Activation Function**\n",
    "\n",
    "- **ReLU Function**: The Rectified Linear Unit (ReLU) activation function is defined as \\( f(x) = \\max(0, x) \\). This simple yet effective function allows for faster training of deep convolutional networks compared to traditional activation functions like sigmoid and hyperbolic tangent (tanh).\n",
    "\n",
    "- **Training Speed**: ReLU-based networks can be trained several times faster than those using tanh or sigmoid. This increased speed is due to the reduced likelihood of the vanishing gradient problem, where gradients become too small for effective learning. The figure below illustrates the number of iterations required for a four-layer convolutional network on the CIFAR-10 dataset to reach 25% training error using both tanh and ReLU.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "![alex1](https://raw.githubusercontent.com/blurred-machine/Data-Science/master/Deep%20Learning%20SOTA/img/alex512.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Local Response Normalization (LRN)**\n",
    "\n",
    "- **Normalization After ReLU**: After applying the ReLU function, the output values are unbounded, unlike those from tanh or sigmoid functions. To address this, normalization techniques like Local Response Normalization (LRN) can be applied. This method helps to maintain a stable learning environment by enhancing the generalization capabilities of the network.\n",
    "\n",
    "\n",
    "\n",
    "- **Biological Inspiration**: LRN is inspired by a phenomenon in neuroscience known as \"lateral inhibition,\" where active neurons suppress the activity of neighboring neurons. This concept helps to emphasize the most salient features detected by the network.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alex2](https://iq.opengenus.org/content/images/2022/05/1-QspNGlKrJ5dAW9VCrP4T4Q.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### **Dropout**\n",
    "\n",
    "- **Preventing Overfitting**: Dropout is a regularization technique that effectively mitigates overfitting in neural networks. Unlike traditional linear models that use explicit regularization methods, dropout modifies the neural network architecture itself.\n",
    "\n",
    "- **Implementation**: During training, dropout randomly \"drops out\" a subset of neurons (with a defined probability) from a layer, while keeping the input and output layer neurons unchanged. This prevents the network from becoming overly reliant on specific neurons and encourages redundancy in feature representation. In subsequent iterations, different neurons are randomly selected for dropout, ensuring varied learning pathways until training concludes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Data Augmentation**\n",
    "\n",
    "- **Enhancing Data Size**: In deep learning, especially when the training dataset is small, data augmentation is crucial. It artificially expands the training set by generating \"new\" data from existing samples through several techniques:\n",
    "\n",
    "\n",
    "\n",
    "  - **Translation**: This technique involves shifting the image along the x or y axis. By moving the image left, right, up, or down, the model learns to recognize objects in different positions within the frame. This helps improve its robustness to variations in object location.\n",
    "\n",
    "\n",
    "\n",
    "  - **Rotation**: Rotation involves turning the image by a certain angle (e.g., 90°, 180°, or any degree). This technique helps the model become invariant to the orientation of the objects, ensuring that it can recognize them regardless of how they are presented in the image.\n",
    "\n",
    "\n",
    "\n",
    "  - **Flipping**: Flipping refers to mirroring the image horizontally or vertically. This technique is particularly useful for tasks like facial recognition, where the orientation may vary, and helps ensure that the model learns to recognize features regardless of their left or right orientation.\n",
    "\n",
    "\n",
    "\n",
    "  - **Adding Noise**: Adding noise involves introducing random variations to the images, such as Gaussian noise, which can simulate real-world conditions like lighting changes or sensor imperfections. This technique helps the model become more robust to noisy inputs and improves its generalization to new, unseen data.\n",
    "\n",
    "\n",
    "\n",
    "- **Other Techniques to Combat Overfitting**:\n",
    "\n",
    "  - **Regularization**: In cases of limited data, regularization techniques can be applied to prevent overfitting. This involves adding a regularization term to the loss function, which helps to balance the training and test errors. However, this approach requires manual tuning of hyperparameters.\n",
    "\n",
    "  \n",
    "\n",
    "  - **Unsupervised Pre-training**: Another method is unsupervised pre-training, where autoencoders or restricted Boltzmann machines (RBMs) are used to initialize the network layers. This layer-by-layer approach prepares the network for supervised fine-tuning with a classification layer added at the end.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Augmentation visualisation**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T12:28:30.945100Z",
     "start_time": "2024-10-02T12:28:14.392620Z"
    },
    "execution": {
     "iopub.execute_input": "2024-10-20T12:31:14.074328Z",
     "iopub.status.busy": "2024-10-20T12:31:14.073422Z",
     "iopub.status.idle": "2024-10-20T12:31:15.394307Z",
     "shell.execute_reply": "2024-10-20T12:31:15.393346Z",
     "shell.execute_reply.started": "2024-10-20T12:31:14.074286Z"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "from io import BytesIO\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "IMAGE_DIM = 277\n",
    "\n",
    "\n",
    "\n",
    "augmentations = [\n",
    "\n",
    "    (\"Original\", transforms.Compose([\n",
    "\n",
    "        transforms.CenterCrop(IMAGE_DIM),\n",
    "\n",
    "        transforms.ToTensor()\n",
    "\n",
    "    ])),\n",
    "\n",
    "    (\"RandomResizedCrop\", transforms.Compose([\n",
    "\n",
    "        transforms.RandomResizedCrop(IMAGE_DIM, scale=(0.9, 1.0)),\n",
    "\n",
    "        transforms.ToTensor()\n",
    "\n",
    "    ])),\n",
    "\n",
    "    (\"HorizontalFlip\", transforms.Compose([\n",
    "\n",
    "        transforms.CenterCrop(IMAGE_DIM),\n",
    "\n",
    "        transforms.RandomHorizontalFlip(1),\n",
    "\n",
    "        transforms.ToTensor()\n",
    "\n",
    "    ])),\n",
    "\n",
    "    (\"ColorJitter\", transforms.Compose([\n",
    "\n",
    "        transforms.CenterCrop(IMAGE_DIM),\n",
    "\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "\n",
    "        transforms.ToTensor()\n",
    "\n",
    "    ])),\n",
    "\n",
    "    (\"RandomRotation\", transforms.Compose([\n",
    "\n",
    "        transforms.CenterCrop(IMAGE_DIM),\n",
    "\n",
    "        transforms.RandomRotation(degrees=15),\n",
    "\n",
    "        transforms.ToTensor()\n",
    "\n",
    "    ])),\n",
    "\n",
    "    (\"All Combined\", transforms.Compose([\n",
    "\n",
    "        transforms.RandomResizedCrop(IMAGE_DIM, scale=(0.9, 1.0)),\n",
    "\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "\n",
    "        transforms.RandomRotation(degrees=15),\n",
    "\n",
    "        transforms.ToTensor()\n",
    "\n",
    "    ]))\n",
    "\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "image_url = 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQFuAFw9qqY0WdxJ8OWNkrVKpbCZi0rXaslxg&s'\n",
    "\n",
    "response = requests.get(image_url)\n",
    "\n",
    "image = Image.open(BytesIO(response.content))\n",
    "\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "axs = axs.ravel()\n",
    "\n",
    "\n",
    "\n",
    "for i, (title, aug) in enumerate(augmentations):\n",
    "\n",
    "    augmented_img = aug(image)  # Apply the augmentation\n",
    "\n",
    "    augmented_img_np = augmented_img.permute(1, 2, 0).numpy()\n",
    "\n",
    "    axs[i].imshow(np.clip(augmented_img_np, 0, 1))\n",
    "\n",
    "    axs[i].axis('off')\n",
    "\n",
    "    axs[i].set_title(title)\n",
    "\n",
    "\n",
    "\n",
    "for j in range(i+1, len(axs)):\n",
    "\n",
    "    axs[j].axis('off')\n",
    "\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AlexNet Implementation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library Imports & Constant Definitions\n",
    "\n",
    "In this section, we will import the necessary libraries and define the constants required for implementing the AlexNet model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-20T12:31:15.396765Z",
     "iopub.status.busy": "2024-10-20T12:31:15.396106Z",
     "iopub.status.idle": "2024-10-20T12:31:15.403922Z",
     "shell.execute_reply": "2024-10-20T12:31:15.402787Z",
     "shell.execute_reply.started": "2024-10-20T12:31:15.396720Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision.datasets as datasets\n",
    "\n",
    "from torch.utils.data import DataLoader, Subset, ConcatDataset\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, f1_score, recall_score\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-20T12:31:15.405456Z",
     "iopub.status.busy": "2024-10-20T12:31:15.405119Z",
     "iopub.status.idle": "2024-10-20T12:31:15.419266Z",
     "shell.execute_reply": "2024-10-20T12:31:15.418392Z",
     "shell.execute_reply.started": "2024-10-20T12:31:15.405388Z"
    }
   },
   "outputs": [],
   "source": [
    "# define model parameters\n",
    "\n",
    "IMAGE_DIM = 28  # pixels of Fashion-MNIST\n",
    "\n",
    "NUM_CLASSES = 10  # 10 classes for Fashion-MNIST\n",
    "\n",
    "DEVICE_IDS = [0]  # GPUs to use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture Overview\n",
    "\n",
    "In this section, we will define the architecture of the AlexNet model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-20T12:31:15.421785Z",
     "iopub.status.busy": "2024-10-20T12:31:15.421436Z",
     "iopub.status.idle": "2024-10-20T12:31:15.436297Z",
     "shell.execute_reply": "2024-10-20T12:31:15.435515Z",
     "shell.execute_reply.started": "2024-10-20T12:31:15.421734Z"
    }
   },
   "outputs": [],
   "source": [
    "class AlexNet(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "\n",
    "            nn.Conv2d(in_channels=3, out_channels=96, kernel_size=11, stride=4),  # (b x 96 x 55 x 55)\n",
    "\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.LocalResponseNorm(size=5, alpha=0.0001, beta=0.75, k=2), # Local response normalization\n",
    "\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),  # (b x 96 x 27 x 27)\n",
    "\n",
    "            nn.Conv2d(96, 256, 5, padding=2),  # (b x 256 x 27 x 27)\n",
    "\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.LocalResponseNorm(size=5, alpha=0.0001, beta=0.75, k=2), # Local response normalization\n",
    "\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),  # (b x 256 x 13 x 13)\n",
    "\n",
    "            nn.Conv2d(256, 384, 3, padding=1),  # (b x 384 x 13 x 13)\n",
    "\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Conv2d(384, 384, 3, padding=1),  # (b x 384 x 13 x 13)\n",
    "\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Conv2d(384, 256, 3, padding=1),  # (b x 256 x 13 x 13)\n",
    "\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),  # (b x 256 x 6 x 6)\n",
    "\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "\n",
    "            nn.Dropout(p=0.5, inplace=False), # Dropout layer for regularization\n",
    "\n",
    "            nn.Linear(in_features=(256 * 6 * 6), out_features=4096),\n",
    "\n",
    "            nn.ReLU(inplace=False),\n",
    "\n",
    "            nn.Dropout(p=0.5, inplace=False), # Dropout layer for regularization\n",
    "\n",
    "            nn.Linear(in_features=4096, out_features=4096),\n",
    "\n",
    "            nn.ReLU(inplace=False),\n",
    "\n",
    "            nn.Linear(in_features=4096, out_features=num_classes), # Output layer for classification\n",
    "\n",
    "        )\n",
    "\n",
    "        self.init_bias() # Initializes the biases of the model layers\n",
    "\n",
    "\n",
    "\n",
    "    def init_bias(self):\n",
    "\n",
    "        for layer in self.net:\n",
    "\n",
    "            if isinstance(layer, nn.Conv2d):\n",
    "\n",
    "                nn.init.normal_(layer.weight, mean=0, std=0.01)\n",
    "\n",
    "                nn.init.constant_(layer.bias, 0)\n",
    "\n",
    "        nn.init.constant_(self.net[4].bias, 1)\n",
    "\n",
    "        nn.init.constant_(self.net[10].bias, 1)\n",
    "\n",
    "        nn.init.constant_(self.net[12].bias, 1)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.net(x)\n",
    "\n",
    "        x = x.view(-1, 256 * 6 * 6)\n",
    "\n",
    "        return self.classifier(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The architecture shown above is the original one described in the paper. However, to adapt this model for use with 28x28 images, several modifications are required. Below is the modified architecture reflecting these changes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-20T12:31:15.437616Z",
     "iopub.status.busy": "2024-10-20T12:31:15.437305Z",
     "iopub.status.idle": "2024-10-20T12:31:15.451347Z",
     "shell.execute_reply": "2024-10-20T12:31:15.450654Z",
     "shell.execute_reply.started": "2024-10-20T12:31:15.437584Z"
    }
   },
   "outputs": [],
   "source": [
    "class AlexNetFashionMNIST(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes=10):  \n",
    "\n",
    "        super(AlexNetFashionMNIST, self).__init__()\n",
    "\n",
    "        self.features = nn.Sequential(\n",
    "\n",
    "            nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1),  # 28x28 -> 28x28\n",
    "\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # 28x28 -> 14x14\n",
    "\n",
    "            nn.Conv2d(64, 192, kernel_size=3, padding=1),  # 14x14 -> 14x14\n",
    "\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # 14x14 -> 7x7\n",
    "\n",
    "            nn.Conv2d(192, 384, kernel_size=3, padding=1),  # 7x7 -> 7x7\n",
    "\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv2d(384, 256, kernel_size=3, padding=1),  # 7x7 -> 7x7\n",
    "\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),  # 7x7 -> 7x7\n",
    "\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            # No further pooling to maintain feature map size at 7x7\n",
    "\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "\n",
    "            nn.Dropout(p=0.5),  # Dropout for regularization\n",
    "\n",
    "            nn.Linear(256 * 7 * 7, 4096),  # Adjusted for 7x7 input\n",
    "\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Dropout(p=0.5),  # Dropout for regularization\n",
    "\n",
    "            nn.Linear(4096, 1024),  # Reduced for computational efficiency\n",
    "\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Linear(1024, num_classes),  # Output for 10 classes in Fashion-MNIST\n",
    "\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.features(x)\n",
    "\n",
    "        x = torch.flatten(x, 1)\n",
    "\n",
    "        return self.classifier(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions\n",
    "\n",
    "In this section, we will define several helper functions to streamline the training process of the AlexNet model. These functions will handle tasks such as loading the dataset, sampling, and applying data augmentation when necessary, while also integrating the required classes for model training.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-20T12:31:15.452927Z",
     "iopub.status.busy": "2024-10-20T12:31:15.452559Z",
     "iopub.status.idle": "2024-10-20T12:31:15.464863Z",
     "shell.execute_reply": "2024-10-20T12:31:15.463825Z",
     "shell.execute_reply.started": "2024-10-20T12:31:15.452883Z"
    }
   },
   "outputs": [],
   "source": [
    "# create model\n",
    "\n",
    "def create_model():\n",
    "\n",
    "    return AlexNetFashionMNIST(num_classes=NUM_CLASSES).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-20T12:31:15.466294Z",
     "iopub.status.busy": "2024-10-20T12:31:15.466021Z",
     "iopub.status.idle": "2024-10-20T12:31:15.474038Z",
     "shell.execute_reply": "2024-10-20T12:31:15.473276Z",
     "shell.execute_reply.started": "2024-10-20T12:31:15.466264Z"
    }
   },
   "outputs": [],
   "source": [
    "# create optimizer and criterion\n",
    "\n",
    "def create_optimizer(model):\n",
    "\n",
    "    return optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "\n",
    "def create_criterion():\n",
    "\n",
    "    return nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-20T12:31:15.475256Z",
     "iopub.status.busy": "2024-10-20T12:31:15.474979Z",
     "iopub.status.idle": "2024-10-20T12:31:15.486153Z",
     "shell.execute_reply": "2024-10-20T12:31:15.485284Z",
     "shell.execute_reply.started": "2024-10-20T12:31:15.475226Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_smaller_dataset(dataset, fraction=0.1):\n",
    "\n",
    "    dataset_size = len(dataset)\n",
    "\n",
    "    indices = np.random.choice(dataset_size, int(dataset_size * fraction), replace=False)\n",
    "\n",
    "    return Subset(dataset, indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-20T12:31:15.487904Z",
     "iopub.status.busy": "2024-10-20T12:31:15.487301Z",
     "iopub.status.idle": "2024-10-20T12:31:15.498672Z",
     "shell.execute_reply": "2024-10-20T12:31:15.497818Z",
     "shell.execute_reply.started": "2024-10-20T12:31:15.487862Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function to augment the dataset by adding horizontally and vertically flipped images\n",
    "\n",
    "def augment_dataset(dataset):\n",
    "\n",
    "    \"\"\"Expand the dataset by adding horizontally and vertically flipped versions of the images.\"\"\"\n",
    "\n",
    "    horizontal_flip_transform = transforms.Compose([transforms.RandomHorizontalFlip(p=1.0), transforms.ToTensor()])\n",
    "\n",
    "    vertical_flip_transform = transforms.Compose([transforms.RandomVerticalFlip(p=1.0), transforms.ToTensor()])\n",
    "\n",
    "\n",
    "\n",
    "    # Augment the dataset by flipping\n",
    "\n",
    "    horizontal_flipped_dataset = datasets.FashionMNIST(root='./data', train=True, download=True, transform=horizontal_flip_transform)\n",
    "\n",
    "    vertical_flipped_dataset = datasets.FashionMNIST(root='./data', train=True, download=True, transform=vertical_flip_transform)\n",
    "\n",
    "\n",
    "\n",
    "    # Combine the original, horizontally flipped, and vertically flipped datasets\n",
    "\n",
    "    augmented_dataset = ConcatDataset([dataset, horizontal_flipped_dataset, vertical_flipped_dataset])\n",
    "\n",
    "\n",
    "\n",
    "    return augmented_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-20T12:31:15.501394Z",
     "iopub.status.busy": "2024-10-20T12:31:15.501112Z",
     "iopub.status.idle": "2024-10-20T12:31:15.509309Z",
     "shell.execute_reply": "2024-10-20T12:31:15.508540Z",
     "shell.execute_reply.started": "2024-10-20T12:31:15.501364Z"
    }
   },
   "outputs": [],
   "source": [
    "# Download and augment Fashion-MNIST dataset\n",
    "\n",
    "def load_data(augment=False, batch_size=64, fraction=0.1):\n",
    "\n",
    "    transform_train = transforms.Compose([\n",
    "\n",
    "        transforms.ToTensor(),\n",
    "\n",
    "        transforms.Normalize((0.1307,), (0.3081,)),\n",
    "\n",
    "    ])\n",
    "\n",
    "\n",
    "\n",
    "    transform_test = transforms.Compose([\n",
    "\n",
    "        transforms.ToTensor(),\n",
    "\n",
    "        transforms.Normalize((0.1307,), (0.3081,)),\n",
    "\n",
    "    ])\n",
    "\n",
    "\n",
    "\n",
    "    # Load full Fashion-MNIST dataset and subsample them based on the given fraction\n",
    "\n",
    "    trainset = datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform_train)\n",
    "\n",
    "    testset = datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform_test)\n",
    "\n",
    "    small_trainset = get_smaller_dataset(trainset, fraction)\n",
    "\n",
    "\n",
    "\n",
    "    # If augment=True, augment the dataset with horizontal and vertical flips\n",
    "\n",
    "    if augment:\n",
    "\n",
    "        small_trainset = augment_dataset(small_trainset)\n",
    "\n",
    "\n",
    "\n",
    "    trainloader = DataLoader(small_trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "    testloader = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "\n",
    "\n",
    "    return trainloader, testloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train & Test Functions\n",
    "\n",
    "In this section, we will define the training & testing functions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-20T12:31:15.510698Z",
     "iopub.status.busy": "2024-10-20T12:31:15.510322Z",
     "iopub.status.idle": "2024-10-20T12:31:15.523142Z",
     "shell.execute_reply": "2024-10-20T12:31:15.522401Z",
     "shell.execute_reply.started": "2024-10-20T12:31:15.510659Z"
    }
   },
   "outputs": [],
   "source": [
    "# train function \n",
    "\n",
    "def train_model(model, trainloader, criterion, optimizer, device, epochs=10):\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        running_loss = 0.0\n",
    "\n",
    "        progress_bar = tqdm(enumerate(trainloader), total=len(trainloader), desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "\n",
    "        for i, (inputs, labels) in progress_bar:\n",
    "\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            \n",
    "\n",
    "            # calculate the loss\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "\n",
    "\n",
    "            # update the parameters\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            progress_bar.set_postfix(loss=running_loss / (i + 1))\n",
    "\n",
    "        print(f\"Finished Epoch {epoch+1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-20T12:31:15.524477Z",
     "iopub.status.busy": "2024-10-20T12:31:15.524188Z",
     "iopub.status.idle": "2024-10-20T12:31:15.535788Z",
     "shell.execute_reply": "2024-10-20T12:31:15.535043Z",
     "shell.execute_reply.started": "2024-10-20T12:31:15.524432Z"
    }
   },
   "outputs": [],
   "source": [
    "# Test function\n",
    "\n",
    "def evaluate_model(model, testloader, criterion, device):\n",
    "\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "\n",
    "    \n",
    "\n",
    "    correct = 0\n",
    "\n",
    "    total = 0\n",
    "\n",
    "    test_loss = 0.0\n",
    "\n",
    "    all_labels = []\n",
    "\n",
    "    all_preds = []\n",
    "\n",
    "    \n",
    "\n",
    "    progress_bar = tqdm(enumerate(testloader), total=len(testloader), desc=\"Evaluating\")\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient computation for testing\n",
    "\n",
    "        for i, (inputs, labels) in progress_bar:\n",
    "\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "\n",
    "\n",
    "            # Forward pass\n",
    "\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "\n",
    "\n",
    "\n",
    "            # Predictions\n",
    "\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "            total += labels.size(0)\n",
    "\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "            \n",
    "\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "\n",
    "            progress_bar.set_postfix(loss=test_loss / (i + 1), accuracy=100 * correct / total)\n",
    "\n",
    "            \n",
    "\n",
    "    # Calculate average test loss, accuracy, F1 score, and recall \n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "    recall = recall_score(all_labels, all_preds, average='weighted')\n",
    "    \n",
    "    print(f\"Test Loss: {test_loss / len(testloader):.4f}, Accuracy: {accuracy:.2f}%, F1 Score: {f1:.4f}, Recall Score: {recall:.4f}\")\n",
    "\n",
    "    # Compute confusion matrix\n",
    "\n",
    "    conf_matrix = confusion_matrix(all_labels, all_preds)\n",
    "    print(f\"Confusion Matrix:\\n{conf_matrix}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation\n",
    "\n",
    "In this section, we will create datasets both with and without data augmentation. We will train and test our models on these datasets to evaluate the impact of augmentation on the performance of the AlexNet model. The model will be trained for 5 epochs using only 10% of the dataset, allowing us to observe whether this reduced data is sufficient to achieve efficient performance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-20T12:31:15.537065Z",
     "iopub.status.busy": "2024-10-20T12:31:15.536780Z",
     "iopub.status.idle": "2024-10-20T12:34:22.698186Z",
     "shell.execute_reply": "2024-10-20T12:34:22.697112Z",
     "shell.execute_reply.started": "2024-10-20T12:31:15.537024Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def run_experiment(augment=False, epochs=10, fraction=0.1):\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "\n",
    "    # Initialize model, criterion, and optimizer\n",
    "\n",
    "    model = create_model()\n",
    "\n",
    "    criterion = create_criterion()\n",
    "\n",
    "    optimizer = create_optimizer(model)\n",
    "\n",
    "\n",
    "\n",
    "    # Load data with the specified fraction\n",
    "\n",
    "    trainloader, testloader = load_data(augment=augment, fraction=fraction)\n",
    "\n",
    "\n",
    "\n",
    "    # Train model\n",
    "\n",
    "    print(f\"Training with {'augmentation' if augment else 'no augmentation'} on a {fraction * 100}% subset of Fashion-MNIST\")\n",
    "\n",
    "    train_model(model, trainloader, criterion, optimizer, device, epochs=epochs)\n",
    "\n",
    "\n",
    "\n",
    "    # Evaluate model\n",
    "\n",
    "    print(\"Evaluating model...\")\n",
    "\n",
    "    evaluate_model(model, testloader, criterion, device)\n",
    "\n",
    "\n",
    "\n",
    "# without augmentation\n",
    "\n",
    "print(\"Running training on Fashion-MNIST without augmentation:\")\n",
    "\n",
    "run_experiment(augment=False, epochs=4, fraction=0.1)  # Using 10% of the dataset\n",
    "\n",
    "\n",
    "\n",
    "# with augmentation\n",
    "\n",
    "print(\"\\n\\nRunning training on Fashion-MNIST with augmentation:\")\n",
    "\n",
    "run_experiment(augment=True, epochs=4, fraction=0.1)  # Using 10% of the dataset\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

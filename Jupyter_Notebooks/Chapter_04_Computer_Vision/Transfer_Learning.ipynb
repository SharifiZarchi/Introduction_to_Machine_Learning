{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<font>\n",
    "<!-- <img src=\"https://sina.sharif.edu/~m_salehi/images/logo_sharif.png\" alt=\"SUT logo\" width=300 height=300 align=left class=\"saturate\"> -->\n",
    "<div dir=ltr align=center>\n",
    "<img src=\"https://sina.sharif.edu/~m_salehi/images/logo_sharif.png\" width=160 height=180>\n",
    "<br>\n",
    "<font color=0F5298 size=6>\n",
    "Introduction to Machine Learning <br>\n",
    "<font color= 6C3BAA size=6>\n",
    "Transfer Learning <br>\n",
    "<font color=696880 size=5>\n",
    "<!-- <br> -->\n",
    "Computer Engineering Department\n",
    "<br>\n",
    "Sharif University of Technology\n",
    "\n",
    "<font color=696880 size=5>\n",
    "<br>\n",
    "CE 40477 - Fall 2025\n",
    "\n",
    "<font color=GREEN size=5>\n",
    "<br>\n",
    "Zahra Rahmani & Farzan Rahmani\n",
    "<!-- <br> -->\n",
    "\n",
    "____\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "axViOHHuSBe5"
   },
   "source": [
    "## Notebook Objectives\n",
    "\n",
    "In this notebook we are going to investigate Transfer Learning using models available from torchvision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "1. [Libraries and Imports](#Libraries-and-Imports)\n",
    "2. [Helper Functions](#Helper-Functions)\n",
    "3. [CIFAR10 dataset](#CIFAR10-dataset)\n",
    "4. [ResNet18](#ResNet18)\n",
    "<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;4-1. [Without Pre-Training](#Without-Pre-Training)\n",
    "<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;4-2. [With Pre-Training](#With-Pre-Training)\n",
    "<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;4-3. [Results](#Results)<br>\n",
    "\n",
    "5. [ResNet152](#ResNet152)\n",
    "6. [Freeze](#When-to-Freeze-Layers)\n",
    "7. [Refrences](#Refrences)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {
     "iopub.execute_input": "2024-09-16T22:31:42.337312Z",
     "iopub.status.busy": "2024-09-16T22:31:42.336584Z",
     "iopub.status.idle": "2024-09-16T22:31:44.846283Z",
     "shell.execute_reply": "2024-09-16T22:31:44.845479Z",
     "shell.execute_reply.started": "2024-09-16T22:31:42.337265Z"
    },
    "id": "CYQYyHliSpeZ"
   },
   "outputs": [],
   "source": [
    "# @title setup and imports\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import trange\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchvision import transforms, models\n",
    "from torchvision.datasets.cifar import CIFAR10\n",
    "\n",
    "from torchsummary import summary\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {
     "iopub.execute_input": "2024-09-16T22:31:44.848921Z",
     "iopub.status.busy": "2024-09-16T22:31:44.848376Z",
     "iopub.status.idle": "2024-09-16T22:31:44.864790Z",
     "shell.execute_reply": "2024-09-16T22:31:44.863780Z",
     "shell.execute_reply.started": "2024-09-16T22:31:44.848876Z"
    },
    "id": "PmVvvoucSpbo"
   },
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, loss_fn, optimizer, device):\n",
    "    model.train()\n",
    "    train_loss = 0.\n",
    "    train_acc = 0.\n",
    "    for images, labels in dataloader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        logits = model(images)\n",
    "        loss = loss_fn(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        train_loss += loss.item()\n",
    "        train_acc += (logits.argmax(dim=1) == labels).sum().item()\n",
    "    train_loss /= len(dataloader)\n",
    "    train_acc /= len(dataloader.dataset)\n",
    "    return train_loss, train_acc\n",
    "\n",
    "\n",
    "def validate_epoch(model, dataloader, loss_fn, device):\n",
    "    model.eval()\n",
    "    val_loss = 0.\n",
    "    val_acc = 0.\n",
    "    with torch.inference_mode():\n",
    "        for images, labels in dataloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            logits = model(images)\n",
    "            loss = loss_fn(logits, labels)\n",
    "            val_loss += loss.item()\n",
    "            val_acc += (logits.argmax(dim=1) == labels).sum().item()\n",
    "        val_loss /= len(dataloader)\n",
    "        val_acc /= len(dataloader.dataset)\n",
    "    return val_loss, val_acc\n",
    "\n",
    "\n",
    "def train_model(model, train_dataloader, val_dataloader, optimizer, n_epochs, device=device):\n",
    "    history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
    "    loss_fn = CrossEntropyLoss()\n",
    "    for _ in (pbar := trange(n_epochs)):\n",
    "        train_loss, train_acc = train_epoch(model, train_dataloader, loss_fn, optimizer, device)\n",
    "        history['train_loss'].append(train_loss), history['train_acc'].append(train_acc)\n",
    "        val_loss, val_acc = validate_epoch(model, val_dataloader, loss_fn, device)\n",
    "        history['val_loss'].append(val_loss), history['val_acc'].append(val_acc)\n",
    "        pbar.set_description(f'Training Accuracy {100 * train_acc:.2f}% | Validation Accuracy {100 * val_acc:.2f}% ')\n",
    "    return history\n",
    "\n",
    "\n",
    "def plot_history(history):\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    ax1.plot(history['train_loss'], label='train')\n",
    "    ax1.plot(history['val_loss'], label='val')\n",
    "    ax1.set_xlabel('Epochs')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.legend()\n",
    "\n",
    "    ax2.plot(history['train_acc'], label='train')\n",
    "    ax2.plot(history['val_acc'], label='val')\n",
    "    ax2.set_xlabel('Epochs')\n",
    "    ax2.set_ylabel('Accuracy')\n",
    "    ax2.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def compare_results(normal_results, pre_trained_results):\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "    ax1.plot(results['train_acc'], label='Normal')\n",
    "    ax1.plot(pre_trained_results['train_acc'], label='Pre-Trained')\n",
    "    ax1.set_xlabel('Epochs')\n",
    "    ax1.set_ylabel('Accuracy')\n",
    "    ax1.set_title('Training Accuracy')\n",
    "    ax1.legend()\n",
    "\n",
    "    ax2.plot(results['val_acc'], label='Normal')\n",
    "    ax2.plot(pre_trained_results['val_acc'], label='Pre-Trained')\n",
    "    ax2.set_xlabel('Epochs')\n",
    "    ax2.set_ylabel('Accuracy')\n",
    "    ax2.set_title('Test Accuracy')\n",
    "    ax2.legend()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIFAR10 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-09-16T22:31:44.866907Z",
     "iopub.status.busy": "2024-09-16T22:31:44.866368Z",
     "iopub.status.idle": "2024-09-16T22:31:46.503536Z",
     "shell.execute_reply": "2024-09-16T22:31:46.502680Z",
     "shell.execute_reply.started": "2024-09-16T22:31:44.866855Z"
    },
    "id": "4-ORDaFPSpXz",
    "outputId": "20410e77-dd4e-483c-9840-17872daeb48b"
   },
   "outputs": [],
   "source": [
    "norm_mean = (0.4914, 0.4822, 0.4465)\n",
    "norm_std = (0.2023, 0.1994, 0.2010)\n",
    "batch_size = 128\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(norm_mean, norm_std),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(norm_mean, norm_std),\n",
    "])\n",
    "\n",
    "trainset = CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
    "trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "testset = CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
    "testloader = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer',\n",
    "           'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ATVheApKgEbJ"
   },
   "source": [
    "# ResNet18"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DyWC67CaIjHg"
   },
   "source": [
    "\n",
    "\n",
    "To observe the improvement caused by transfer learning, we train the `ResNet18` model twice, once with pre-trained weights (from the ImageNet dataset) and once without pre-training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8vwoikzNKCe_"
   },
   "source": [
    "## Without Pre-Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c8DayF6bDz8m"
   },
   "source": [
    "We will start by training a ResNet18 without pre-trained weights, so we set `weights=None`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 484
    },
    "execution": {
     "iopub.execute_input": "2024-09-16T23:09:54.192141Z",
     "iopub.status.busy": "2024-09-16T23:09:54.191754Z",
     "iopub.status.idle": "2024-09-16T23:12:37.988038Z",
     "shell.execute_reply": "2024-09-16T23:12:37.986994Z",
     "shell.execute_reply.started": "2024-09-16T23:09:54.192096Z"
    },
    "id": "eyLwP0maywKC",
    "outputId": "6730abf7-a329-48e1-e12a-c23aa93a8b96"
   },
   "outputs": [],
   "source": [
    "resnet18 = models.resnet18(weights=None)\n",
    "num_ftrs = resnet18.fc.in_features\n",
    "resnet18.fc = nn.Linear(num_ftrs, len(classes))\n",
    "\n",
    "resnet18 = resnet18.to(device)\n",
    "if device == 'cuda':\n",
    "    resnet18 = torch.compile(resnet18)\n",
    "optim = Adam(resnet18.parameters())\n",
    "\n",
    "results = train_model(resnet18, trainloader, testloader, optim, n_epochs=20)\n",
    "plot_history(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3ElgJ76UJ5jq"
   },
   "source": [
    "## With Pre-Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nMqn6kNaECqA"
   },
   "source": [
    "By setting `weights='DEFAULT'` we can load the pre-trained weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dQEwx40-Di6n"
   },
   "outputs": [],
   "source": [
    "resnet18 = models.resnet18(weights='DEFAULT')\n",
    "num_ftrs = resnet18.fc.in_features\n",
    "resnet18.fc = nn.Linear(num_ftrs, len(classes))\n",
    "\n",
    "resnet18 = resnet18.to(device)\n",
    "if device == 'cuda':\n",
    "    resnet18 = torch.compile(resnet18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fqB8axFpFNQ1"
   },
   "source": [
    "We can fine-tune the pre-trained model. Here we are not going to freeze any of its layers and therefore the only real difference from normal trainig is that instead of initializing the network, we use pre-trained weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 482
    },
    "execution": {
     "iopub.execute_input": "2024-09-16T23:07:07.530923Z",
     "iopub.status.busy": "2024-09-16T23:07:07.530519Z",
     "iopub.status.idle": "2024-09-16T23:09:54.190299Z",
     "shell.execute_reply": "2024-09-16T23:09:54.189206Z",
     "shell.execute_reply.started": "2024-09-16T23:07:07.530878Z"
    },
    "id": "norUrdhkyzyO",
    "outputId": "c154dbfd-ac65-4967-f262-878d456060a8"
   },
   "outputs": [],
   "source": [
    "optim = Adam(resnet18.parameters())\n",
    "pt_results = train_model(resnet18, trainloader, testloader, optim, n_epochs=20)\n",
    "plot_history(pt_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c7KG-XRDKGaJ"
   },
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qUMJfOsKKHzT"
   },
   "source": [
    "As you can see the pre-trained weights improve the accuracy significantly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "id": "ws6_X4tu547G",
    "outputId": "e478e164-e1b6-44ca-987c-b3ea5f2cc649"
   },
   "outputs": [],
   "source": [
    "compare_results(results, pt_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b-_0T-tI6BpO"
   },
   "source": [
    "# ResNet152"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1OenCUkC6vN0"
   },
   "source": [
    "Alternatively we could have frozen all the layers except for the final fully connected layer which is what we are going to do here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dsd2mdVu6BHT"
   },
   "outputs": [],
   "source": [
    "resnet152 = models.resnet152(weights='DEFAULT')\n",
    "\n",
    "# Freeze All Layers Except the Final Fully Connected Layer\n",
    "for param in resnet152.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Replace the Final Fully Connected Layer\n",
    "num_ftrs = resnet152.fc.in_features\n",
    "resnet152.fc = nn.Linear(num_ftrs, len(classes))\n",
    "\n",
    "resnet152 = resnet152.to(device)\n",
    "if device == 'cuda':\n",
    "    resnet152 = torch.compile(resnet152)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EdnQc1y-Jh8c"
   },
   "source": [
    "All we have to do now is to pass the parameters of the fully connected layer to the optimizer and train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 482
    },
    "id": "bXYb5CTxDgSC",
    "outputId": "670b5d67-1a1b-4f52-ee2b-a6274498ed18"
   },
   "outputs": [],
   "source": [
    "optim = Adam(resnet152.fc.parameters())\n",
    "results = train_model(resnet152, trainloader, testloader, optim, n_epochs=15)\n",
    "plot_history(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KLbJR1PL_c-P"
   },
   "source": [
    "# To Freeze or not to Freeze ü•∂"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DvD94V8cJRKe"
   },
   "source": [
    "\n",
    "> ‚ö†Ô∏è It's often best **NOT TO FREEZE** the layers. ‚ö†Ô∏è"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ww8s30kl_f6_"
   },
   "source": [
    "## When to Freeze Layers\n",
    "- **Small Dataset**: If your dataset is small, freezing most of the layers helps prevent overfitting. The pretrained layers already contain useful features learned from a large dataset.\n",
    "- **Similar Task**: If your new task is similar to the original task the model was trained on, freezing the layers can help retain the useful features.\n",
    "- **Limited Computational Resources**: Freezing layers reduces the number of parameters to train, which can save computational resources and time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LhSvRqZ4JLTN"
   },
   "source": [
    "## When Not to Freeze Layers\n",
    "- **Large Dataset**: If you have a large dataset, you can afford to unfreeze more layers to fine-tune the model more extensively.\n",
    "- **Different Task**: If your new task is significantly different from the original task, you might need to unfreeze more layers to adapt the model to the new task.\n",
    "- **Performance Needs**: If the initial performance with frozen layers is not satisfactory, you can gradually unfreeze layers and fine-tune them to improve performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1YQcszUnTpFf"
   },
   "source": [
    "# Refrences\n",
    "\n",
    "*   [Transfer Learning for Computer Vision Tutorial](https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html)\n",
    "*   [Models and Pre-Trained Weights from PyTorch](https://pytorch.org/vision/stable/models.html)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 30762,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

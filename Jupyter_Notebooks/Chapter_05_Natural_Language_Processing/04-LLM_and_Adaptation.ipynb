{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "931b0be6",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <h1>Introduction to Machine Learning</h1>\n",
    "    <h2>Chapter 5: Natural Language Processing</h2>\n",
    "    <h3>Large Language Models & Adaptation</h3>\n",
    "    <h4>Author: Sina Daneshgar<h4>\n",
    "</div>\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/SharifiZarchi/Introduction_to_Machine_Learning/blob/main/Jupyter_Notebooks/Chapter_05_Natural_Language_Processing/04-LLM_and_Adaptation.ipynb)\n",
    "[![Open In kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://raw.githubusercontent.com/SharifiZarchi/Introduction_to_Machine_Learning/main/Jupyter_Notebooks/Chapter_05_Natural_Language_Processing/04-LLM_and_Adaptation.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1.  [Language Modeling Basics](#1.-Language-Modeling-Basics)\n",
    "2.  [Text Generation Strategies](#2.-Text-Generation-Strategies)\n",
    "3.  [In-Context Learning (Zero-shot & Few-shot)](#3.-In-Context-Learning-(Zero-shot-&-Few-shot))\n",
    "4.  [Model Adaptation: Parameter-Efficient Fine-Tuning (PEFT)](#4.-Model-Adaptation:-Parameter-Efficient-Fine-Tuning-(PEFT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1604b765",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary libraries if not already installed\n",
    "# !pip install transformers torch numpy matplotlib\n",
    "# !pip install peft  -----> For the adaptation section\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Set device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d0493f",
   "metadata": {},
   "source": [
    "## 1. Language Modeling Basics\n",
    "\n",
    "At its core, a causal language model (like GPT) is trained to predict the probability of the next token given a sequence of previous tokens:\n",
    "$$ P(w_t | w_{1}, w_{2}, ..., w_{t-1}) $$\n",
    "\n",
    "Let's load a pre-trained GPT-2 model and see this in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7447ca3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained model and tokenizer\n",
    "model_name = \"gpt2\"  # You can use \"gpt2-medium\" or \"gpt2-large\" for better results\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name).to(device)\n",
    "model.eval()\n",
    "\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb66f73",
   "metadata": {},
   "source": [
    "### Predicting the Next Token\n",
    "Let's give the model a sentence and see what it thinks the next word should be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5f076c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_next_token(text, top_k=5):\n",
    "    # Encode input text\n",
    "    input_ids = tokenizer.encode(text, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    # Get model output\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids)\n",
    "        predictions = outputs.logits[0, -1, :]  # Get logits for the last token\n",
    "    \n",
    "    # Apply softmax to get probabilities\n",
    "    probs = F.softmax(predictions, dim=-1)\n",
    "    \n",
    "    # Get top k predictions\n",
    "    top_probs, top_indices = torch.topk(probs, top_k)\n",
    "    \n",
    "    print(f\"Input text: '{text}'\")\n",
    "    print(\"Top next token predictions:\")\n",
    "    for prob, idx in zip(top_probs, top_indices):\n",
    "        token = tokenizer.decode([idx])\n",
    "        print(f\"  '{token}': {prob.item():.4f}\")\n",
    "\n",
    "# Test with a few examples\n",
    "predict_next_token(\"The capital of France is\")\n",
    "print(\"-\" * 30)\n",
    "predict_next_token(\"Machine learning is a subfield of\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07002caa",
   "metadata": {},
   "source": [
    "## 2. Text Generation Strategies\n",
    "\n",
    "Generating text involves repeatedly predicting the next token and appending it to the input. However, *how* we select that next token matters significantly.\n",
    "\n",
    "### A. Greedy Search\n",
    "Simply select the token with the highest probability at each step.\n",
    "- **Pros**: Fast, deterministic.\n",
    "- **Cons**: Can get stuck in loops, misses high-probability paths that start with lower-probability tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55080cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_greedy(text, max_new_tokens=20):\n",
    "    input_ids = tokenizer.encode(text, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    output = model.generate(\n",
    "        input_ids, \n",
    "        max_new_tokens=max_new_tokens, \n",
    "        do_sample=False,  # Disable sampling for greedy search\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    \n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Greedy Generation:\")\n",
    "print(generate_greedy(\"Once upon a time,\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efbf2755",
   "metadata": {},
   "source": [
    "### B. Beam Search\n",
    "Maintains multiple possible sequences (beams) and keeps the most likely ones.\n",
    "- **Pros**: Finds better overall sequences than greedy.\n",
    "- **Cons**: Slower, can still be repetitive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90be9dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_beam(text, num_beams=5, max_new_tokens=20):\n",
    "    input_ids = tokenizer.encode(text, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    output = model.generate(\n",
    "        input_ids, \n",
    "        max_new_tokens=max_new_tokens, \n",
    "        num_beams=num_beams, \n",
    "        early_stopping=True,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    \n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Beam Search Generation:\")\n",
    "print(generate_beam(\"Once upon a time,\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f22231",
   "metadata": {},
   "source": [
    "### C. Sampling (Temperature, Top-k, Top-p)\n",
    "Instead of picking the max, we sample from the probability distribution.\n",
    "- **Temperature**: Controls randomness. Low temp (<1) makes it more confident/conservative. High temp (>1) makes it more random/creative.\n",
    "- **Top-k**: Only sample from the top $k$ most likely tokens.\n",
    "- **Top-p (Nucleus Sampling)**: Sample from the smallest set of tokens whose cumulative probability exceeds $p$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7700cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sampling(text, temperature=0.7, top_k=50, top_p=0.9, max_new_tokens=40):\n",
    "    input_ids = tokenizer.encode(text, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    output = model.generate(\n",
    "        input_ids, \n",
    "        max_new_tokens=max_new_tokens, \n",
    "        do_sample=True, \n",
    "        temperature=temperature,\n",
    "        top_k=top_k,\n",
    "        top_p=top_p,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    \n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Sampling Generation (Creative):\")\n",
    "print(generate_sampling(\"The future of artificial intelligence is\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b2d1a7",
   "metadata": {},
   "source": [
    "## 3. In-Context Learning (Zero-shot & Few-shot)\n",
    "\n",
    "One of the most powerful features of LLMs is their ability to learn from the context provided in the prompt, without updating weights.\n",
    "\n",
    "### Zero-shot Learning\n",
    "Asking the model to perform a task without any examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6bde073",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zero-shot Sentiment Analysis\n",
    "prompt_zero_shot = \"\"\"\n",
    "Classify the sentiment of the following sentence as Positive or Negative.\n",
    "Sentence: The movie was shockingly bad!\n",
    "Sentiment:\"\"\"\n",
    "\n",
    "print(\"--- Zero-shot ---\")\n",
    "print(generate_greedy(prompt_zero_shot, max_new_tokens=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7538f29",
   "metadata": {},
   "source": [
    "### Few-shot Learning\n",
    "Providing a few examples (shots) in the prompt to guide the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95652d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Few-shot Sentiment Analysis\n",
    "prompt_few_shot = \"\"\"\n",
    "Classify the sentiment of the following sentences as Positive or Negative.\n",
    "\n",
    "Sentence: I hated this movie.\n",
    "Sentiment: Negative\n",
    "\n",
    "Sentence: This is the best day of my life.\n",
    "Sentiment: Positive\n",
    "\n",
    "Sentence: The movie was shockingly bad!\n",
    "Sentiment:\"\"\"\n",
    "\n",
    "print(\"--- Few-shot ---\")\n",
    "print(generate_greedy(prompt_few_shot, max_new_tokens=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc1f56f",
   "metadata": {},
   "source": [
    "## 4. Model Adaptation: Parameter-Efficient Fine-Tuning (PEFT)\n",
    "\n",
    "Fine-tuning a full LLM (billions of parameters) is expensive. **PEFT** methods allow us to adapt models by training only a small number of extra parameters.\n",
    "\n",
    "### LoRA (Low-Rank Adaptation)\n",
    "LoRA freezes the pre-trained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture.\n",
    "\n",
    "$$ W_{new} = W_{pretrained} + B \\cdot A $$\n",
    "\n",
    "Where $W$ is a weight matrix (e.g., in the attention layer), and $B, A$ are small trainable matrices.\n",
    "\n",
    "*Note: The code below requires the `peft` library.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660152f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a demonstration of how to set up LoRA. \n",
    "# It requires the 'peft' library installed.\n",
    "\n",
    "try:\n",
    "    from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "    # Define LoRA Configuration\n",
    "    peft_config = LoraConfig(\n",
    "        task_type=TaskType.CAUSAL_LM, \n",
    "        inference_mode=False, \n",
    "        r=8,            # Rank of the low-rank matrices\n",
    "        lora_alpha=32,  # Scaling factor\n",
    "        lora_dropout=0.1\n",
    "    )\n",
    "\n",
    "    # Wrap the base model with LoRA\n",
    "    # We use the same 'model' we loaded earlier\n",
    "    peft_model = get_peft_model(model, peft_config)\n",
    "\n",
    "    print(\"LoRA Model Created!\")\n",
    "    peft_model.print_trainable_parameters()\n",
    "    \n",
    "    # Now 'peft_model' can be trained just like a standard PyTorch model, \n",
    "    # but only a tiny fraction of parameters will be updated!\n",
    "\n",
    "except ImportError:\n",
    "    print(\"The 'peft' library is not installed. Please install it with `pip install peft` to run this section.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc18dd33",
   "metadata": {},
   "source": [
    "### Fine-tuning with LoRA (Toy Example)\n",
    "\n",
    "Now that we have our `peft_model`, we can train it using standard PyTorch loops. Notice that we are only updating the LoRA parameters (approx. 0.3% of the total), which makes the backward pass much faster and less memory-intensive.\n",
    "\n",
    "Let's try to fine-tune the model on a single sentence to see the loss decrease.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb61fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define optimizer (only optimizing LoRA parameters)\n",
    "optimizer = torch.optim.AdamW(peft_model.parameters(), lr=1e-3)\n",
    "\n",
    "# Dummy training data\n",
    "text = \"Fine-tuning LLMs is efficient with LoRA.\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# Labels are the same as inputs for Causal LM\n",
    "labels = inputs.input_ids.clone()\n",
    "\n",
    "print(\"Starting training...\")\n",
    "peft_model.train()\n",
    "\n",
    "for epoch in range(10):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward pass\n",
    "    outputs = peft_model(input_ids=inputs.input_ids, labels=labels)\n",
    "    loss = outputs.loss\n",
    "    \n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}: Loss = {loss.item():.4f}\")\n",
    "\n",
    "print(\"Training finished! The model has adapted to this specific sentence.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca033ce8",
   "metadata": {},
   "source": [
    "## Summary\n",
    "In this notebook, we covered:\n",
    "1.  **Language Modeling**: Predicting the next token is the fundamental task of GPT models.\n",
    "2.  **Generation**: Sampling strategies (Temperature, Top-p) allow for diverse and creative outputs compared to greedy search.\n",
    "3.  **In-Context Learning**: LLMs can perform tasks given just a prompt (Zero-shot) or a few examples (Few-shot).\n",
    "4.  **Adaptation**: Techniques like LoRA allow us to fine-tune massive models efficiently."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

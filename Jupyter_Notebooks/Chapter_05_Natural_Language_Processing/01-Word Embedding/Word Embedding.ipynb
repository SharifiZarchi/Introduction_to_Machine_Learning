{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <h1>Introduction to Machine Learning</h1>\n",
    "    <h2>Chapter 5: Natural Language Processing</h2>\n",
    "    <h3>Word Embeddings (Word2Vec)</h3>\n",
    "    <h4>Authors: Omid-d, Sina Daneshgar<h4>\n",
    "</div>\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/SharifiZarchi/Introduction_to_Machine_Learning/blob/main/Jupyter_Notebooks/Chapter_05_Natural_Language_Processing/01-Word%20Embedding/Word%20Embedding.ipynb)\n",
    "[![Open In kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://raw.githubusercontent.com/SharifiZarchi/Introduction_to_Machine_Learning/main/Jupyter_Notebooks/Chapter_05_Natural_Language_Processing/01-Word%20Embedding/Word%20Embedding.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wvTr7cCwlZ5h"
   },
   "source": [
    "# **Word Embedding**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TjzLpputEude"
   },
   "source": [
    "**Word embedding is a dense representation of words in the form of numeric vectors. It can be learned using a variety of language models. The word embedding representation is able to reveal many hidden relationships between words. For example, vector(â€œcatâ€) - vector(â€œkittenâ€) is similar to vector(â€œdogâ€) - vector(â€œpuppyâ€). This post introduces several models for learning word embedding and how their loss functions are designed for the purpose.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Word Embeddings? (One-Hot vs. Dense Vectors)\n",
    "\n",
    "Before implementing Skip-Gram, let's understand why we need embeddings.\n",
    "Traditional **One-Hot Encoding** represents words as sparse vectors where only one index is 1 and the rest are 0. This has two major issues:\n",
    "1.  **Sparsity**: Vectors are huge and mostly empty.\n",
    "2.  **No Semantic Meaning**: The dot product between any two different one-hot vectors is 0, meaning \"cat\" and \"dog\" are as different as \"cat\" and \"car\".\n",
    "\n",
    "**Word Embeddings** (like Word2Vec) solve this by learning dense vectors where similar words are close in vector space.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2427,
     "status": "ok",
     "timestamp": 1730294427529,
     "user": {
      "displayName": "Omid Daliran",
      "userId": "14293939971066168269"
     },
     "user_tz": -210
    },
    "id": "UfKVbKrZcCBR"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_onehot_vs_embedding():\n",
    "    # Vocabulary\n",
    "    vocab = [\"cat\", \"dog\", \"car\"]\n",
    "    word2idx = {w: i for i, w in enumerate(vocab)}\n",
    "    \n",
    "    # One-Hot Encoding\n",
    "    one_hot_cat = np.zeros(len(vocab))\n",
    "    one_hot_cat[word2idx[\"cat\"]] = 1\n",
    "    \n",
    "    one_hot_dog = np.zeros(len(vocab))\n",
    "    one_hot_dog[word2idx[\"dog\"]] = 1\n",
    "    \n",
    "    print(\"One-Hot 'cat':\", one_hot_cat)\n",
    "    print(\"One-Hot 'dog':\", one_hot_dog)\n",
    "    print(\"Dot Product (Similarity):\", np.dot(one_hot_cat, one_hot_dog)) # Always 0\n",
    "    \n",
    "    # Hypothetical Embeddings (Dense Vectors)\n",
    "    # Notice how cat and dog share similar values in some dimensions\n",
    "    embed_cat = np.array([0.9, 0.1, 0.5])\n",
    "    embed_dog = np.array([0.8, 0.2, 0.4])\n",
    "    \n",
    "    print(\"\\nEmbedding 'cat':\", embed_cat)\n",
    "    print(\"Embedding 'dog':\", embed_dog)\n",
    "    print(\"Dot Product (Similarity):\", np.dot(embed_cat, embed_dog)) # Non-zero, indicates similarity\n",
    "\n",
    "compare_onehot_vs_embedding()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skip Gram Implementation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dE8l7FFfldQY"
   },
   "source": [
    "Defines a Vocabulary class to map words to unique indices and vice versa. The vocabulary is built from a corpus of sentences, counting word occurrences, and filtering based on a minimum frequency threshold. This helps create a lookup for words and their corresponding indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 500,
     "status": "ok",
     "timestamp": 1730293574861,
     "user": {
      "displayName": "Omid Daliran",
      "userId": "14293939971066168269"
     },
     "user_tz": -210
    },
    "id": "JPP3YaQhcHgx"
   },
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "    def __init__(self):\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "        self.word_count = defaultdict(int)\n",
    "        self.total_words = 0\n",
    "        self.vocab_size = 0\n",
    "\n",
    "    def build_vocab(self, sentences, min_count=2):\n",
    "        # Count words\n",
    "        for word in sentences:\n",
    "            self.word_count[word] += 1\n",
    "\n",
    "        # Create word2idx and idx2word mapping\n",
    "        idx = 0\n",
    "        for word, count in self.word_count.items():\n",
    "            if count >= min_count:\n",
    "              self.word2idx.update({word: idx})\n",
    "              idx += 1\n",
    "\n",
    "        # self.word2idx = {word: idx for idx, (word, count) in enumerate(self.word_count.items()) if count >= min_count}\n",
    "        self.idx2word = {idx: word for word, idx in self.word2idx.items()}\n",
    "        self.vocab_size = len(self.word2idx)\n",
    "        self.total_words = sum([count for word, count in self.word_count.items() if count >= min_count])\n",
    "\n",
    "    def word_to_index(self, word):\n",
    "        return self.word2idx.get(word, -1)\n",
    "\n",
    "    def index_to_word(self, index):\n",
    "        return self.idx2word.get(index, None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6DgQ97AQlyzT"
   },
   "source": [
    "A function to generate training data for a model using the skip-gram approach. It builds pairs of words (center word and context words) from a list of sentences based on a specified window size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1730293576569,
     "user": {
      "displayName": "Omid Daliran",
      "userId": "14293939971066168269"
     },
     "user_tz": -210
    },
    "id": "gPzlftDvcKlo"
   },
   "outputs": [],
   "source": [
    "def generate_training_data(vocab, sentences, window_size=2):\n",
    "    training_data = []\n",
    "    sentence_indices = [vocab.word_to_index(word) for word in sentences if vocab.word_to_index(word) != -1]\n",
    "\n",
    "    for center_idx, center_word in enumerate(sentence_indices):\n",
    "        context_start = max(0, center_idx - window_size)\n",
    "        context_end = min(len(sentence_indices), center_idx + window_size + 1)\n",
    "\n",
    "        for context_idx in range(context_start, context_end):\n",
    "            if context_idx != center_idx:\n",
    "                context_word = sentence_indices[context_idx]\n",
    "                training_data.append((center_word, context_word))\n",
    "\n",
    "    return np.array(training_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "okCsz5u2ClfC"
   },
   "source": [
    "A skip-gram model predicts the context (or neighbors) of a word, given the word itself. The model is trained on skip-grams, which are n-grams that allow tokens to be skipped (see the diagram below for an example). The context of a word can be represented through a set of skip-gram pairs of `(target_word, context_word)` where `context_word` appears in the neighboring context of `target_word`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mR_m8SL9FaMz"
   },
   "source": [
    "Consider the following sentence of eight words:\n",
    "\n",
    "> The wide road shimmered in the hot sun.\n",
    "\n",
    "The context words for each of the 8 words of this sentence are defined by a window size. The window size determines the span of words on either side of a `target_word` that can be considered a `context word`. Below is a table of skip-grams for target words based on different window sizes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AbTxnTboFgYr"
   },
   "source": [
    "![word2vec_skipgrams](https://tensorflow.org/text/tutorials/images/word2vec_skipgram.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zc_7AZC9GEBq"
   },
   "source": [
    "The training objective of the skip-gram model is to maximize the probability of predicting context words given the target word. For a sequence of words *w<sub>1</sub>, w<sub>2</sub>, ... w<sub>T</sub>*, the objective can be written as the average log probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e9wZppT3GF4e"
   },
   "source": [
    "![word2vec_skipgram_objective](https://tensorflow.org/text/tutorials/images/word2vec_skipgram_objective.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LR6mzqPVGI1B"
   },
   "source": [
    "where `c` is the size of the training context. The basic skip-gram formulation defines this probability using the softmax function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vB5VY-aTGK3f"
   },
   "source": [
    "![word2vec_full_softmax](https://tensorflow.org/text/tutorials/images/word2vec_full_softmax.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "48QmRZYrGOzx"
   },
   "source": [
    "where *v* and *v<sup>'<sup>* are target and context vector representations of words and *W* is vocabulary size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec Implementation Details\n",
    "\n",
    "The `Word2Vec` class below implements the Skip-Gram model using a simple neural network with one hidden layer (the embedding layer).\n",
    "\n",
    "1.  **Initialization (`__init__`)**:\n",
    "    *   `self.W`: The input-to-hidden weight matrix. This *is* the embedding matrix we want to learn. Shape: `(vocab_size, embed_size)`.\n",
    "    *   `self.W_prime`: The hidden-to-output weight matrix. Shape: `(embed_size, vocab_size)`.\n",
    "\n",
    "2.  **Training (`train`)**:\n",
    "    *   **Forward Pass**:\n",
    "        *   Get the embedding vector `h` for the center word from `self.W`.\n",
    "        *   Compute the score `u` by multiplying `h` with `self.W_prime`.\n",
    "        *   Apply `softmax` to get probabilities `y_pred`.\n",
    "    *   **Backward Pass**:\n",
    "        *   Compute the error `error = y_pred - y_true`.\n",
    "        *   Update `self.W_prime` and `self.W` using Gradient Descent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 452,
     "status": "ok",
     "timestamp": 1730293580366,
     "user": {
      "displayName": "Omid Daliran",
      "userId": "14293939971066168269"
     },
     "user_tz": -210
    },
    "id": "3r7DO0LhcNNo"
   },
   "outputs": [],
   "source": [
    "class Word2Vec:\n",
    "    def __init__(self, vocab_size, embed_size=100, learning_rate=0.001):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_size = embed_size\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        # Initialize weight matrices\n",
    "        self.W = np.random.uniform(-0.5, 0.5, (vocab_size, embed_size))  # Input to hidden\n",
    "        self.W_prime = np.random.uniform(-0.5, 0.5, (embed_size, vocab_size))  # Hidden to output\n",
    "\n",
    "    def softmax(self, x):\n",
    "        exp_x = np.exp(x - np.max(x))\n",
    "        return exp_x / np.sum(exp_x)\n",
    "\n",
    "    def train(self, training_data, epochs=1000):\n",
    "        for epoch in range(epochs):\n",
    "            loss = 0\n",
    "            for center_word, context_word in training_data:\n",
    "                h = self.W[center_word]  # Get hidden layer (input->hidden)\n",
    "                u = np.dot(h, self.W_prime)  # Predict output\n",
    "                y_pred = self.softmax(u)\n",
    "\n",
    "                # One-hot encoding the true context word\n",
    "                y_true = np.zeros(self.vocab_size)\n",
    "                y_true[context_word] = 1\n",
    "\n",
    "                # Calculate the error\n",
    "                error = y_pred - y_true\n",
    "\n",
    "                # Update weights with gradient descent\n",
    "                self.W_prime -= self.learning_rate * np.outer(h, error)\n",
    "                self.W[center_word] -= self.learning_rate * np.dot(self.W_prime, error)\n",
    "\n",
    "                # Calculate loss (cross-entropy)\n",
    "                loss -= np.log(y_pred[context_word])\n",
    "\n",
    "            if epoch % 100 == 0:\n",
    "                print(f'Epoch {epoch}, Loss: {loss}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iGnfx6dml3xy"
   },
   "source": [
    "Read a Persian stopwords file, tokenizes a Persian text sample, removes stopwords, and reconstructs the filtered sentence. This preprocessing is essential for cleaning text data before further processing, such as embedding or modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 938,
     "status": "ok",
     "timestamp": 1727106128605,
     "user": {
      "displayName": "Omid Daliran",
      "userId": "14293939971066168269"
     },
     "user_tz": -210
    },
    "id": "9LlbDm1y-QTy",
    "outputId": "006b1263-0b27-4b0c-aa3a-17efd44c7f44"
   },
   "outputs": [],
   "source": [
    "# Reading the Persian stopwords from the file\n",
    "with open('persian.txt', 'r', encoding='utf-8') as f:\n",
    "    stopwords = set(f.read().splitlines())  # Using a set for faster lookup\n",
    "\n",
    "# Sample Persian text\n",
    "text = \"Ù…Ù„Ú©Ù‡ Ùˆ Ø²Ù† Ù‡Ø§ Ø¯Ø± Ú©Ù†Ø§Ø± Ù‡Ù…Ø³Ø±Ø§Ù† Ùˆ Ø®Ø§Ù†ÙˆØ§Ø¯Ù‡ Ø®ÙˆØ¯ ÛŒØ¹Ù†ÛŒ Ø´Ø§Ù‡ Ùˆ Ù…Ø±Ø¯ Ù‡Ø§ Ø¯Ø± ÛŒÚ© Ø³Ø±Ø²Ù…ÛŒÙ† Ù¾Ù‡Ù†Ø§ÙˆØ± Ø²Ù†Ø¯Ú¯ÛŒ Ù…ÛŒâ€ŒÚ©Ø±Ø¯Ù†Ø¯ Ø´Ø§Ù‡ Ù‡Ù…ÛŒØ´Ù‡ Ø¨Ù‡ Ù…Ø±Ø¯ Ù‡Ø§ ØªØ°Ú©Ø± Ù…ÛŒØ¯Ø§Ø¯ Ú©Ù‡ Ù‚Ø¯Ø±Øª Ø¯Ø± Ø§ØªØ­Ø§Ø¯ Ù…Ø±Ø¯ Ù‡Ø§ Ùˆ Ø´Ø§Ù‡ Ù†Ù‡ÙØªÙ‡ Ø§Ø³Øª Ùˆ Ø¯Ø± Ø§ÛŒÙ† Ù‚Ù„Ù…Ø±Ùˆ Ù…Ù„Ú©Ù‡ Ø¨Ù‡ Ø²Ù† Ù‡Ø§ ÛŒØ§Ø¯Ø¢ÙˆØ±ÛŒ Ù…ÛŒâ€ŒÚ©Ø±Ø¯ Ú©Ù‡ Ù‡Ù…Ø¨Ø³ØªÚ¯ÛŒ Ø²Ù† Ù‡Ø§ Ùˆ Ù…Ù„Ú©Ù‡ Ù…Ù‡Ù… Ø§Ø³Øª Ùˆ Ø¯Ø± Ø§ÛŒÙ† Ø¯Ø§Ø³ØªØ§Ù† Ù‡Ø± Ù…Ø±Ø¯ Ú©Ù‡ Ù†Ø²Ø¯ Ø´Ø§Ù‡ ÛŒØ§ Ø²Ù† Ú©Ù‡ Ù†Ø²Ø¯ Ù…Ù„Ú©Ù‡ Ù…ÛŒâ€ŒØ¢Ù…Ø¯ Ø§Ø² Ø¢Ù†Ù‡Ø§ Ø­Ú©Ù… Ù…ÛŒâ€ŒÚ¯Ø±ÙØª ØªØ§ Ø¨Ù‡ Ø¯ÛŒÚ¯Ø±Ø§Ù† Ú©Ù…Ú© Ú©Ù†Ù†Ø¯ Ø´Ø§Ù‡ Ø¹Ø§Ø¯Ù„ Ùˆ Ù‚Ø§Ø¯Ø± Ø¨ÙˆØ¯ Ùˆ Ù…Ù„Ú©Ù‡ Ø®Ø±Ø¯Ù…Ù†Ø¯ Ùˆ Ø²ÛŒØ¨Ø§ Ùˆ Ù‡Ø± Ù…Ø±Ø¯ Ú©Ù‡ Ø§Ø² Ø­Ú©Ù…Øª Ø´Ø§Ù‡ ÛŒØ§ Ø²Ù† Ú©Ù‡ Ø§Ø² Ø¹Ø¯Ø§Ù„Øª Ù…Ù„Ú©Ù‡ Ø±Ø§Ø¶ÛŒ Ù†Ø¨ÙˆØ¯ Ù†Ø²Ø¯ Ø¢Ù†Ù‡Ø§ Ù…ÛŒâ€ŒØ±ÙØª ØªØ§ Ø´Ú©Ø§ÛŒØª Ø®ÙˆØ¯ Ø±Ø§ Ù…Ø·Ø±Ø­ Ú©Ù†Ø¯ Ø´Ø§Ù‡ Ùˆ Ù…Ø±Ø¯ Ùˆ Ù…Ù„Ú©Ù‡ Ùˆ Ø²Ù† Ø¯Ø± Ú©Ù†Ø§Ø± Ù‡Ù… Ø¨ÙˆØ¯Ù†Ø¯ Ùˆ Ù‡ÛŒÚ† Ú©Ø³ Ø§Ø² Ø´Ø§Ù‡ ÛŒØ§ Ù…Ù„Ú©Ù‡ Ù†Ù…ÛŒâ€ŒØªØ±Ø³ÛŒØ¯ Ø´Ø§Ù‡ Ù‡Ù…ÛŒØ´Ù‡ Ø¨Ù‡ Ø²Ø¨Ø§Ù† Ù…ÛŒØ§ÙˆØ±Ø¯ Ú©Ù‡ Ù…Ø±Ø¯ Ù‡Ø§ Ø¨Ø§ÛŒØ¯ Ø¨Ù‡ ÛŒÚ©Ø¯ÛŒÚ¯Ø± Ú©Ù…Ú© Ú©Ù†Ù†Ø¯ Ùˆ Ù…Ù„Ú©Ù‡ ØªØ£Ú©ÛŒØ¯ Ø¯Ø§Ø´ØªÙ†Ø¯ Ø²Ù† Ù‡Ø§ Ù‡Ù… Ø¨Ø§ÛŒØ¯ Ù…ØªØ­Ø¯ Ø¨Ø§Ø´Ù†Ø¯\"\n",
    "\n",
    "# Tokenizing the text (you can modify the tokenizer if needed)\n",
    "words = text.split()\n",
    "\n",
    "# Removing stopwords\n",
    "filtered_text = [word for word in words if word not in stopwords]\n",
    "\n",
    "# Joining the words back into a sentence\n",
    "cleaned_text = ' '.join(filtered_text)\n",
    "\n",
    "print(cleaned_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5749,
     "status": "ok",
     "timestamp": 1727106139433,
     "user": {
      "displayName": "Omid Daliran",
      "userId": "14293939971066168269"
     },
     "user_tz": -210
    },
    "id": "NkspL7i-cQ43",
    "outputId": "54375f5c-55d3-47ce-aa4f-c52332252db9"
   },
   "outputs": [],
   "source": [
    "# wiki_dump_path = 'enwiki-latest-pages-articles.xml.bz2'  # Path to your Wikipedia dump file\n",
    "\n",
    "# # Load and preprocess the dataset\n",
    "# sentences = list(load_wiki_data(wiki_dump_path))\n",
    "\n",
    "# Build the vocabulary\n",
    "vocab = Vocabulary()\n",
    "vocab.build_vocab(cleaned_text.split(' '))\n",
    "\n",
    "# Generate training data\n",
    "training_data = generate_training_data(vocab, cleaned_text.split(' '))\n",
    "\n",
    "# Initialize and train Word2Vec model\n",
    "word2vec_model = Word2Vec(vocab.vocab_size)\n",
    "word2vec_model.train(training_data, epochs=1000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "THLeFGKsl-2n"
   },
   "source": [
    "Defines a function to retrieve the word embedding for a given word from a Word2Vec model. If the word exists in the vocabulary, its corresponding vector is returned; otherwise, None is returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 754,
     "status": "ok",
     "timestamp": 1727104712881,
     "user": {
      "displayName": "Omid Daliran",
      "userId": "14293939971066168269"
     },
     "user_tz": -210
    },
    "id": "y05t33ogufIm",
    "outputId": "4e8911f4-4ab8-4981-b552-a5e3eb766478"
   },
   "outputs": [],
   "source": [
    "def get_word_embedding(word, vocab, model):\n",
    "    word_idx = vocab.word_to_index(word)\n",
    "    if word_idx != -1:\n",
    "        return model.W[word_idx]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "embedding = get_word_embedding(\"Ù…Ø±Ø¯\", vocab, word2vec_model)\n",
    "print(embedding)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ORS4NzOgBV4o"
   },
   "source": [
    "Cosine similarity is used to find words similar to a target word. The cosine similarity between two vectors\n",
    "ð‘¢\n",
    "âƒ—\n",
    "u\n",
    "  and\n",
    "ð‘£\n",
    "âƒ—\n",
    "v\n",
    "  is:\n",
    "\n",
    "cosine_similarity\n",
    "(\n",
    "ð‘¢\n",
    ",\n",
    "ð‘£\n",
    ")\n",
    "=\n",
    "ð‘¢\n",
    "â‹…\n",
    "ð‘£\n",
    "âˆ¥\n",
    "ð‘¢\n",
    "âˆ¥\n",
    "âˆ¥\n",
    "ð‘£\n",
    "âˆ¥\n",
    "cosine_similarity(u,v)=\n",
    "âˆ¥uâˆ¥âˆ¥vâˆ¥\n",
    "uâ‹…v\n",
    "â€‹\n",
    "\n",
    "It ranges from -1 (opposite) to 1 (identical)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 725,
     "status": "ok",
     "timestamp": 1727106717668,
     "user": {
      "displayName": "Omid Daliran",
      "userId": "14293939971066168269"
     },
     "user_tz": -210
    },
    "id": "p6T9Y5RF-raw",
    "outputId": "ed2e039e-e4b2-4f02-a5f4-bdf1191d443a"
   },
   "outputs": [],
   "source": [
    "def cosine_similarity(vec1, vec2):\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    norm_vec1 = np.linalg.norm(vec1)\n",
    "    norm_vec2 = np.linalg.norm(vec2)\n",
    "    return dot_product / (norm_vec1 * norm_vec2)\n",
    "\n",
    "def find_similar_words(word, vocab, model, k=5):\n",
    "    word_idx = vocab.word_to_index(word)\n",
    "\n",
    "    if word_idx == -1:\n",
    "        return f\"Word '{word}' not in vocabulary.\"\n",
    "\n",
    "    word_vector = model.W[word_idx]\n",
    "    similarities = []\n",
    "\n",
    "    for idx in range(vocab.vocab_size):\n",
    "        if idx != word_idx:  # Skip the word itself\n",
    "            other_word_vector = model.W[idx]\n",
    "            sim_score = cosine_similarity(word_vector, other_word_vector)\n",
    "            similarities.append((vocab.index_to_word(idx), sim_score))\n",
    "\n",
    "    # Sort by similarity scores in descending order\n",
    "    similarities = sorted(similarities, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    return similarities[:k]\n",
    "\n",
    "# Example usage:\n",
    "similar_words = find_similar_words(\"Ù…Ù„Ú©Ù‡\", vocab, word2vec_model, k=5)\n",
    "print(similar_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zGuqnU8emDLc"
   },
   "source": [
    "Perform a word analogy task (e.g., \"man is to king as woman is to X\") by computing vector differences and finding the word most similar to the resulting vector. The function identifies the word closest to the vector result of the analogy operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 813,
     "status": "ok",
     "timestamp": 1727106494536,
     "user": {
      "displayName": "Omid Daliran",
      "userId": "14293939971066168269"
     },
     "user_tz": -210
    },
    "id": "fAROfewT-4SP",
    "outputId": "8db5a60f-420b-4f35-8ce2-ac73dab33981"
   },
   "outputs": [],
   "source": [
    "def find_analogy(word_a, word_b, word_c, vocab, model):\n",
    "    # Convert words to their vector representations\n",
    "    vec_a = model.W[vocab.word_to_index(word_a)] if vocab.word_to_index(word_a) != -1 else None\n",
    "    vec_b = model.W[vocab.word_to_index(word_b)] if vocab.word_to_index(word_b) != -1 else None\n",
    "    vec_c = model.W[vocab.word_to_index(word_c)] if vocab.word_to_index(word_c) != -1 else None\n",
    "\n",
    "    if vec_a is None or vec_b is None or vec_c is None:\n",
    "        return f\"One of the words '{word_a}', '{word_b}', or '{word_c}' is not in the vocabulary.\"\n",
    "\n",
    "    # Word analogy: vector equation (vec_d = vec_a - vec_b + vec_c)\n",
    "    target_vec = vec_a - vec_b + vec_c\n",
    "    similarities = []\n",
    "\n",
    "    # Find the word most similar to the resulting vector\n",
    "    for idx in range(vocab.vocab_size):\n",
    "        other_word_vector = model.W[idx]\n",
    "        sim_score = cosine_similarity(target_vec, other_word_vector)\n",
    "        similarities.append((vocab.index_to_word(idx), sim_score))\n",
    "\n",
    "    # Sort by similarity and return the most similar word (excluding word_a, word_b, word_c)\n",
    "    similarities = sorted(similarities, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Filter out the original words from the analogy (optional)\n",
    "    filtered_similarities = [word for word in similarities if word[0] not in {word_a, word_b, word_c}]\n",
    "\n",
    "    return filtered_similarities[0] if filtered_similarities else None\n",
    "\n",
    "# Example usage:\n",
    "analogy_result = find_analogy(\"Ù…Ø±Ø¯\", \"Ø´Ø§Ù‡\", \"Ø²Ù†\", vocab, word2vec_model)\n",
    "print(analogy_result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_m-COB9rcQAe"
   },
   "source": [
    "# Using a Pre-Trained Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HUFCNb_5Aspb"
   },
   "source": [
    "**The quality of word embeddings improves as the model is trained on larger and more diverse datasets. Word2Vec models, for example, learn better representations of words when exposed to more data because they capture a richer context of how words are used in different situations. To ensure high-quality embeddings for our tasks, we will use a pre-trained Word2Vec model that has been trained on a vast corpus of news articles. This large dataset allows the model to learn nuanced relationships between words, resulting in more accurate and meaningful embeddings.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5430,
     "status": "ok",
     "timestamp": 1730293595815,
     "user": {
      "displayName": "Omid Daliran",
      "userId": "14293939971066168269"
     },
     "user_tz": -210
    },
    "id": "D08BzU11lhG-",
    "outputId": "f3988eaa-b209-494b-c941-911599270894"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 516,
     "status": "ok",
     "timestamp": 1730293601861,
     "user": {
      "displayName": "Omid Daliran",
      "userId": "14293939971066168269"
     },
     "user_tz": -210
    },
    "id": "N8fEghxlAz2s"
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def preprocess_text(text, minimum_length=1, stopword_removal=True, stopwords_domain=[], lower_case=True,\n",
    "                       punctuation_removal=True):\n",
    "    \"\"\"\n",
    "    preprocess text by removing stopwords, punctuations, and converting to lowercase, and also filter based on a min length\n",
    "    for stopwords use nltk.corpus.stopwords.words('english')\n",
    "    for punctuations use string.punctuation\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    text: str\n",
    "        text to be preprocessed\n",
    "    minimum_length: int\n",
    "        minimum length of the token\n",
    "    stopword_removal: bool\n",
    "        whether to remove stopwords\n",
    "    stopwords_domain: list\n",
    "        list of stopwords to be removed base on domain\n",
    "    lower_case: bool\n",
    "        whether to convert to lowercase\n",
    "    punctuation_removal: bool\n",
    "        whether to remove punctuations\n",
    "    \"\"\"\n",
    "    if lower_case:\n",
    "        text = text.lower()\n",
    "\n",
    "    if punctuation_removal:\n",
    "        text = re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    if stopword_removal:\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        if stopwords_domain:\n",
    "            stop_words.update(stopwords_domain)\n",
    "        tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "    tokens = [word for word in tokens if len(word) >= minimum_length]\n",
    "\n",
    "    processed_text = ' '.join(tokens)\n",
    "\n",
    "    return processed_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0djn1vylmdR_"
   },
   "source": [
    "Implementation of a class that uses a pre-trained Word2Vec model to compute word embeddings and perform word analogy tasks. It preprocesses the input text, generates embeddings for each word in a query, and averages them to create a sentence embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 428,
     "status": "ok",
     "timestamp": 1730294350194,
     "user": {
      "displayName": "Omid Daliran",
      "userId": "14293939971066168269"
     },
     "user_tz": -210
    },
    "id": "jzGtiCqBleog"
   },
   "outputs": [],
   "source": [
    "class PreTrainedWord2Vec:\n",
    "    \"\"\"\n",
    "    A class used fro a pre-trained Word2Vec model and generate embeddings for text data.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    model: Word2Vec model\n",
    "        The trained Word2Vec model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, preprocessor=None, model=None):\n",
    "        \"\"\"\n",
    "        Initializes the FastText with a preprocessor and a model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        model : Word2Vec model\n",
    "        \"\"\"\n",
    "        self.preprocessor = preprocessor\n",
    "        self.model = model\n",
    "\n",
    "\n",
    "\n",
    "    def get_query_embedding(self, query):\n",
    "        \"\"\"\n",
    "        Generates an embedding for the given query.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        query : str\n",
    "            The query to generate an embedding for.\n",
    "\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        np.ndarray\n",
    "            The embedding for the query.\n",
    "        \"\"\"\n",
    "        if self.preprocessor:\n",
    "            query = self.preprocessor(query)\n",
    "\n",
    "        word_vectors = []\n",
    "\n",
    "        for word in query.split(' '):\n",
    "            if word in model:\n",
    "                word_vectors.append(model[word])\n",
    "\n",
    "\n",
    "        sentence_vector = np.mean(word_vectors, axis=0)\n",
    "        return sentence_vector\n",
    "\n",
    "\n",
    "    def analogy(self, word1, word2, word3):\n",
    "        \"\"\"\n",
    "        Perform an analogy task: word1 is to word2 as word3 is to __.\n",
    "\n",
    "        Args:\n",
    "            word1 (str): The first word in the analogy.\n",
    "            word2 (str): The second word in the analogy.\n",
    "            word3 (str): The third word in the analogy.\n",
    "\n",
    "        Returns:\n",
    "            str: The word that completes the analogy.\n",
    "        \"\"\"\n",
    "        new_word1 = self.preprocessor(word1)\n",
    "        new_word2 = self.preprocessor(word2)\n",
    "        new_word3 = self.preprocessor(word3)\n",
    "        word1_emb = self.model[new_word1]\n",
    "        word2_emb = self.model[new_word2]\n",
    "        word3_emb = self.model[new_word3]\n",
    "\n",
    "        result = word3_emb + (word2_emb - word1_emb)\n",
    "\n",
    "        vocab_vectors = {word: self.model[word] for word in list(self.model.index_to_key)}\n",
    "\n",
    "        input_words = {word1, word2, word3}\n",
    "        possible_results = {word: vec for word, vec in vocab_vectors.items() if word not in input_words}\n",
    "\n",
    "        nearest_word = max(possible_results, key=lambda word: np.dot(possible_results[word], result) / (\n",
    "                    np.linalg.norm(possible_results[word]) * np.linalg.norm(result)))\n",
    "\n",
    "        return nearest_word\n",
    "\n",
    "    def plot_words(self, words):\n",
    "        \"\"\"\n",
    "        Plots the given words in a 2D space using PCA for dimensionality reduction.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        words : list of str\n",
    "            A list of words to be plotted.\n",
    "        \"\"\"\n",
    "        # Collect the word vectors for the specified words\n",
    "        word_vectors = [self.model[word] for word in words if word in self.model]\n",
    "\n",
    "        # Apply PCA to reduce the dimensions to 2D\n",
    "        pca = PCA(n_components=2)\n",
    "        word_vecs_2d = pca.fit_transform(word_vectors)\n",
    "\n",
    "        # Plotting the words\n",
    "        plt.figure(figsize=(10, 7))\n",
    "        for i, word in enumerate(words):\n",
    "            if word in self.model:\n",
    "                plt.scatter(word_vecs_2d[i, 0], word_vecs_2d[i, 1])\n",
    "                plt.text(word_vecs_2d[i, 0] + 0.01, word_vecs_2d[i, 1] + 0.01, word, fontsize=12)\n",
    "\n",
    "        plt.xlabel(\"PCA Component 1\")\n",
    "        plt.ylabel(\"PCA Component 2\")\n",
    "        plt.title(\"Word Embeddings in 2D space\")\n",
    "        plt.grid(True)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4491,
     "status": "ok",
     "timestamp": 1730293612405,
     "user": {
      "displayName": "Omid Daliran",
      "userId": "14293939971066168269"
     },
     "user_tz": -210
    },
    "id": "QlTQjXcbTDFu",
    "outputId": "faeca6f3-3246-43e4-e7cc-3dc9ff02a647"
   },
   "outputs": [],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QW1PpUgOmwm9"
   },
   "source": [
    "Loading a pre-trained Word2Vec model (Google News corpus) using Gensim's downloader. This model is used in subsequent cells for word embedding tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 507516,
     "status": "ok",
     "timestamp": 1730294121524,
     "user": {
      "displayName": "Omid Daliran",
      "userId": "14293939971066168269"
     },
     "user_tz": -210
    },
    "id": "6ebu1L1nTkAQ",
    "outputId": "27eadbe2-6697-4cc9-d2cc-b13d7e97675b"
   },
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "# Load a pre-trained skip-gram Word2Vec model (Google News model)\n",
    "model = api.load('word2vec-google-news-300')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 444,
     "status": "ok",
     "timestamp": 1730294352277,
     "user": {
      "displayName": "Omid Daliran",
      "userId": "14293939971066168269"
     },
     "user_tz": -210
    },
    "id": "fI1K6jEwX3vB"
   },
   "outputs": [],
   "source": [
    "w2v = PreTrainedWord2Vec(preprocess_text, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 43740,
     "status": "ok",
     "timestamp": 1730294401210,
     "user": {
      "displayName": "Omid Daliran",
      "userId": "14293939971066168269"
     },
     "user_tz": -210
    },
    "id": "pcTYk3jzX17j",
    "outputId": "e7beb87f-7730-4c7f-816d-040f60283f7f"
   },
   "outputs": [],
   "source": [
    "print(10 * \"*\" + \"Similarity\" + 10 * \"*\")\n",
    "word = 'queen'\n",
    "neighbors = w2v.model.most_similar(word)\n",
    "\n",
    "for neighbor in neighbors:\n",
    "    print(f\"Word: {neighbor[1]}, Similarity: {neighbor[0]}\")\n",
    "\n",
    "print(10 * \"*\" + \"Analogy\" + 10 * \"*\")\n",
    "word1 = \"man\"\n",
    "word2 = \"woman\"\n",
    "word3 = \"boy\"\n",
    "print(f\"Similarity between {word1} and {word2} is like similarity between {word3} and {w2v.analogy(word1, word2, word3)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 641
    },
    "executionInfo": {
     "elapsed": 487,
     "status": "ok",
     "timestamp": 1730294483624,
     "user": {
      "displayName": "Omid Daliran",
      "userId": "14293939971066168269"
     },
     "user_tz": -210
    },
    "id": "EoDDjeE_lNpi",
    "outputId": "8c5c9ad4-e713-4fd1-d1d4-3cc032188bbe"
   },
   "outputs": [],
   "source": [
    "w2v.plot_words(['boy', 'girl', 'man', 'woman'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HRnjwKnrrhbz"
   },
   "source": [
    "# Sentiment Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3rCr8asJCDFn"
   },
   "source": [
    "**We will use the embedding model as input for a simple neural network to predict whether the tone of a sentence is positive or negative. By feeding the sentence embeddings into the network, the model can learn patterns that distinguish between positive and negative sentiments, enabling it to classify sentences based on their tone.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E_D-wvlbm38N"
   },
   "source": [
    "Defining the full pipeline for sentiment analysis:\n",
    "\n",
    "1. Prepares positive and negative sentiment examples.\n",
    "2. Splits the data into training and validation sets.\n",
    "3. Defines a custom dataset class to retrieve embeddings for each sentence.\n",
    "4. Creates a neural network model for sentiment classification.\n",
    "5. Trains the model using a binary classification approach, optimizing over several epochs and computing validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1277,
     "status": "ok",
     "timestamp": 1727355976808,
     "user": {
      "displayName": "Omid Daliran",
      "userId": "14293939971066168269"
     },
     "user_tz": -210
    },
    "id": "VgtvLyq7xEZl",
    "outputId": "e0d8f4b6-86e8-4420-b8c0-68b5c1212baf"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import fasttext\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "\n",
    "# 1. DATASET: Positive and Negative Examples\n",
    "positive_examples = [\n",
    "    \"I absolutely loved the movie!\",\n",
    "    \"The food was delicious and the service was excellent.\",\n",
    "    \"This product exceeded my expectations.\",\n",
    "    \"I had a fantastic time at the concert.\",\n",
    "    \"The weather was beautiful during our vacation.\",\n",
    "    \"The customer support team was incredibly helpful.\",\n",
    "    \"I would definitely recommend this restaurant.\",\n",
    "    \"The book was a joy to read from start to finish.\",\n",
    "    \"I'm very happy with my purchase.\",\n",
    "    \"The experience was truly unforgettable.\"\n",
    "]\n",
    "\n",
    "negative_examples = [\n",
    "    \"The movie was terrible and a complete waste of time.\",\n",
    "    \"The food was cold and the service was slow.\",\n",
    "    \"I was very disappointed with the quality of this product.\",\n",
    "    \"The concert was boring and not worth the money.\",\n",
    "    \"The weather ruined our entire trip.\",\n",
    "    \"The customer support team was unhelpful and rude.\",\n",
    "    \"I will never visit this restaurant again.\",\n",
    "    \"The book was poorly written and hard to follow.\",\n",
    "    \"I'm extremely dissatisfied with my purchase.\",\n",
    "    \"The experience was awful and I regret it.\"\n",
    "]\n",
    "\n",
    "positive_labels = [1] * len(positive_examples)\n",
    "negative_labels = [0] * len(negative_examples)\n",
    "\n",
    "sentences = positive_examples + negative_examples\n",
    "labels = positive_labels + negative_labels\n",
    "\n",
    "# 2. Shuffle and Split into Training and Validation Sets\n",
    "dataset = list(zip(sentences, labels))\n",
    "random.shuffle(dataset)\n",
    "train_data, val_data = train_test_split(dataset, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# 4. Custom Dataset Class\n",
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, data, word2vec_model):\n",
    "        self.data = data\n",
    "        self.word2vec_model = word2vec_model\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sentence, label = self.data[idx]\n",
    "        embedding = self.get_embedding(sentence)\n",
    "        return embedding, torch.tensor(label, dtype=torch.float)\n",
    "\n",
    "    def get_embedding(self, sentence):\n",
    "        \"\"\"Get word2vec embedding for a given sentence\"\"\"\n",
    "        embedding = self.word2vec_model.get_query_embedding(sentence)\n",
    "        return torch.tensor(embedding).float()\n",
    "\n",
    "# 5. Model Definition\n",
    "class SentimentClassifier(nn.Module):\n",
    "    def __init__(self, embedding_dim):\n",
    "        super(SentimentClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(embedding_dim, 64)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(64, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "# 6. Hyperparameters and Dataset Preparation\n",
    "embedding_dim = 300  # Word2Vec embedding size\n",
    "train_dataset = SentimentDataset(train_data, w2v)\n",
    "val_dataset = SentimentDataset(val_data, w2v)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=4)\n",
    "\n",
    "# 7. Training Setup\n",
    "nn_model = SentimentClassifier(embedding_dim)\n",
    "criterion = nn.BCELoss()  # Binary cross entropy loss for binary classification\n",
    "optimizer = optim.Adam(nn_model.parameters(), lr=0.001)\n",
    "\n",
    "# 8. Training Loop\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    nn_model.train()\n",
    "    running_loss = 0.0\n",
    "    for embeddings, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = nn_model(embeddings)\n",
    "        loss = criterion(outputs.squeeze(), labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # Validation\n",
    "    nn_model.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for embeddings, labels in val_loader:\n",
    "            outputs = nn_model(embeddings)\n",
    "            predicted = (outputs.squeeze() >= 0.5).float()\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss:.4f}, Validation Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "print(\"Training complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oL54FLj3nLgl"
   },
   "source": [
    "Implementation of a function to predict the sentiment of a given sentence using the trained sentiment analysis model. It embeds the sentence using the Word2Vec model, passes it through the neural network, and outputs a prediction of either \"positive\" or \"negative.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ozndtXeSKBHk"
   },
   "outputs": [],
   "source": [
    "def predict_sentiment(nn_model, sentence, w2v):\n",
    "    \"\"\"\n",
    "    Predicts the sentiment of a given sentence using the trained model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    nn_model : SentimentClassifier\n",
    "        The trained sentiment classification model.\n",
    "    sentence : str\n",
    "        The input sentence to classify.\n",
    "    w2v : Word2Vec model\n",
    "        The model used to generate embeddings.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        The predicted sentiment ('positive' or 'negative').\n",
    "    \"\"\"\n",
    "    # Step 1: Get FastText embedding for the sentence\n",
    "    embedding = w2v.get_query_embedding(sentence)\n",
    "    embedding = torch.tensor(embedding).float().unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "    # Step 2: Pass the embedding through the model to get prediction\n",
    "    nn_model.eval()  # Set the model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        output = nn_model(embedding)\n",
    "\n",
    "    # Step 3: Interpret the result\n",
    "    prediction = output.item()  # Get the output as a scalar value\n",
    "    sentiment = \"positive\" if prediction >= 0.5 else \"negative\"\n",
    "\n",
    "    return sentiment\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 627,
     "status": "ok",
     "timestamp": 1727355981642,
     "user": {
      "displayName": "Omid Daliran",
      "userId": "14293939971066168269"
     },
     "user_tz": -210
    },
    "id": "1M4pR_w-b56v",
    "outputId": "6a2ad51f-f670-410b-d422-79abd037be51"
   },
   "outputs": [],
   "source": [
    "# Example usage\n",
    "query_sentence = \"The movie was terrible and a i hated it\"\n",
    "predicted_sentiment = predict_sentiment(nn_model, query_sentence, w2v)\n",
    "print(f\"Sentiment of the query '{query_sentence}': {predicted_sentiment}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1727355982308,
     "user": {
      "displayName": "Omid Daliran",
      "userId": "14293939971066168269"
     },
     "user_tz": -210
    },
    "id": "hE-QhjNXb7Ra",
    "outputId": "599e6ffc-38f9-47de-c656-010848fb22d4"
   },
   "outputs": [],
   "source": [
    "# Example usage\n",
    "query_sentence = \"The movie was amazing and i loved it\"\n",
    "predicted_sentiment = predict_sentiment(nn_model, query_sentence, w2v)\n",
    "print(f\"Sentiment of the query '{query_sentence}': {predicted_sentiment}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPDNvIJDMFdDJ9FHHgt1zLD",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
